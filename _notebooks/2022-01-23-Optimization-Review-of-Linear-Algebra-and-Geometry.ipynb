{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c415992",
   "metadata": {},
   "source": [
    "# Optimization - Review of Linear Algebra and Geometry\n",
    "\n",
    "> Preliminary mathematical concepts in the study of optimization - Eigenpairs, Fundamental Subspaces, Symmetry, Spectral Decomposition, Convexity, etc.\n",
    "\n",
    "- hide: false\n",
    "- toc: true\n",
    "- badges: true\n",
    "- comments: false\n",
    "- categories: ['Optimization','Applied Mathematics','Proofs']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0eae137",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The study of optimization can be summed up as the attempt to find those parameter(s) that optimize some objective function, if such exist. The objective function can be almost anything — cost, profit, nodes in a wireless network, distance to a destination, similarity to a target image, etc. If the objective function describes cost we may wish to minimize it, whereas if, on the other hand, it describes profit then a natural goal would be to maximize it. \n",
    "\n",
    "The problems of minimization and maximization, summed up as *optimization* in one word, are the same problem up to a reflection with respect to the domain of the parameter(s). \n",
    "\n",
    "Formally, let the objective function be $f: \\mathbb{R^n} \\to \\mathbb{R}$, and let it have minimizer $x^* \\in \\mathbb{R^n}$. Then, by definition of minimizer, $f(x^*) \\leq f(x) \\ \\ \\forall x \\in \\mathbb{R^n}$. It follows that $-f(x^*) \\geq -f(x) \\ \\ \\forall x \\in \\mathbb{R^n}$, so $x^*$ is the maximizer for $-f$.\n",
    "\n",
    "## Model of a Convex Optimization Problem\n",
    "\n",
    "This series of posts will cover the ways in which we can solve an optimization problem of the form\n",
    "\n",
    "$\n",
    "\\textrm{minimize}: f(x)\n",
    "\\\\\n",
    "\\textrm{subject to}: x \\in \\mathcal{X}\n",
    "$\n",
    "\n",
    "where the *objective function* $f$ is a *convex function*, and the *constraint set* $\\mathcal{X}$ is a *convex set*. Importantly, we will *not* cover the ways in which we can model a real-world problem as a convex optimization problem of the above form.\n",
    "\n",
    "## Why Convex Optimization?\n",
    "\n",
    "First, let's define the *size* of an optimization problem as the dimensionality of the parameter $x$ added to the number of the problem constraints.\n",
    "\n",
    "Convex optimization problems are a class of *easy* optimization problems — problems whose time and/or space complexity grows slowly with respect to problem size.\n",
    "\n",
    "These problems are general enough to capture many scenarios of interest, even some that do not fall strictly into the convex category, but specific enough to be solvable through generic algorithms and numerical methods.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8c257c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
