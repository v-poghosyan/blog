{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c415992",
   "metadata": {},
   "source": [
    "# Optimization - Review of Linear Algebra and Geometry\n",
    "\n",
    "> Preliminary mathematical concepts in the study of optimization - Eigenpairs, Fundamental Subspaces, Symmetry, Spectral Decomposition, Convexity, etc.\n",
    "\n",
    "- hide: false\n",
    "- toc: true\n",
    "- badges: true\n",
    "- comments: false\n",
    "- categories: ['Optimization','Applied Mathematics','Proofs']\n",
    "- image: images/conic-combination.png"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0eae137",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The study of optimization can be summed up as the attempt to find those parameter(s) that optimize some objective function, if such exist. The objective function can be almost anything — cost, profit, nodes in a wireless network, distance to a destination, similarity to a target image, etc. If the objective function describes cost we may wish to minimize it. If, on the other hand, it describes profit then a natural goal would be to maximize it. \n",
    "\n",
    "The problems of minimization and maximization, summed up as *optimization* in one word, are the same problem up to a reflection with respect to the domain of the parameter(s). \n",
    "\n",
    "Formally, let the objective function be $f: \\mathbb{R^n} \\to \\mathbb{R}$, and let it have minimizer $x^* \\in \\mathbb{R^n}$. Then, by definition of minimizer, $f(x^*) \\leq f(x) \\ \\ \\forall x \\in \\mathbb{R^n}$. It follows that $-f(x^*) \\geq -f(x) \\ \\ \\forall x \\in \\mathbb{R^n}$, so $x^*$ is the maximizer for $-f$.\n",
    "\n",
    "## Model of a Convex Optimization Problem\n",
    "\n",
    "This series of posts will cover the ways in which we can solve an optimization problem of the form\n",
    "\n",
    "$\n",
    "\\textrm{minimize}: f(x)\n",
    "\\\\\n",
    "\\textrm{subject to}: x \\in \\mathcal{X}\n",
    "$\n",
    "\n",
    "where the *objective function* $f$ is a *convex function*, and the *constraint set* $\\mathcal{X}$ is a *convex set*. Importantly, we will *not* cover the ways in which we can model a real-world problem as a convex optimization problem of the above form.\n",
    "\n",
    "## Why Convex Optimization?\n",
    "\n",
    "First, let's define the *size* of an optimization problem as the dimensionality of the parameter $x$ added to the number of the problem constraints.\n",
    "\n",
    "Convex optimization problems are a class of *easy* optimization problems — problems whose time and/or space complexity grows slowly with respect to problem size.\n",
    "\n",
    "These problems are general enough to capture many scenarios of interest, even some that do not fall strictly into the convex category, but specific enough to be solvable through generic algorithms and numerical methods.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bbacdd",
   "metadata": {},
   "source": [
    "# Review of Linear Algebra and Geometry\n",
    "\n",
    "We start our exploration of convex optimization with a refresher on convexity and the linear algebra that's in common use in the subject. \n",
    "\n",
    "## Convexity\n",
    "\n",
    "Set convexity is defined as follows:\n",
    "\n",
    "> Definition: &nbsp; A set $C \\subseteq \\mathbb{R^d}$ is **convex** if, for all points $x_1,x_2 \\in C$ and any $\\theta \\in [0,1]$, the point $\\theta x_1 + (1-\\theta) x_2$ (i.e. the parametrized line segment between $x_1$ and $x_2$) is also in $C$.\n",
    "<br>\n",
    "\n",
    "\n",
    "### Some Operations that Preserve Convexity\n",
    "\n",
    "Shifting, scaling, and rotation (i.e. *affine* transformations) preserve convexity. Let the matrix $A$ define such a transformation, and $b$ be a shift vector. Then $C' = \\{Ax + b \\ |  \\ x \\in C \\}$ is convex provided that $C$ was convex.\n",
    "\n",
    "An *intersection* of convex sets is also convex. That is, $C' = \\{ x \\ | \\ x \\in C_1 \\cap x \\in C_2 \\}$ is convex provided that $C_1$ and $C_2$ were convex to begin with. The proof follows directly from the definition of intersection...\n",
    "\n",
    "However, *unions* of convex sets need not be convex...\n",
    "\n",
    "## Examples of Convex Sets\n",
    "\n",
    "The following are some common convex sets we will come across in practice.\n",
    "\n",
    "### Convex Hull of $n$ Points\n",
    "\n",
    "A *convex combination* of points $x_1, ..., x_n$ is a point of the form $x = \\theta_1 x_1 + ... + \\theta_n x_n$ where $\\sum_{i = 1}^{n} \\theta_i = 1$ and $\\theta_i \\geq 0 \\ \\ \\forall i$.\n",
    "\n",
    "Let $x_1,x_2,...,x_n$ be $n$ points in space. Their *convex hull* is the set of all points which can be written as some convex combination of them. Equivalently, by varying the $\\theta_i$'s we generate the convex hull as the set of all convex combinations of these points.\n",
    "\n",
    "The convex hull can be visualized as the closed polygon formed when a rubber band is stretched around the $n$ points. The convex hull of two points is the line segment joining them. That of three points is the polygon (complete with its inner region). In general, for $n$ points, the concept generalizes to an $n$-dimensional polygon.\n",
    "\n",
    "Formally, the convex hull is the set $\\{ \\theta_1 x_1 + ... + \\theta_n x_n \\ | \\ \\theta_1 + ... + \\theta_n = 1 \\ \\ \\textrm{and} \\ \\ \\theta_i \\geq 0 \\ \\ \\forall i \\}$\n",
    "\n",
    "\n",
    "\n",
    "> Note: A handy interactive visualization, along with an efficient algorithm that generates a convex hull of $n$ points on a 2D plane can be found in the following [blog post](https://www.jgibson.id.au/articles/convex-hull/) by Joel Gibson.\n",
    "<br>\n",
    "\n",
    "### Convex Hull of a Set\n",
    "\n",
    "The convex hull of a set can be similarly defined as all the convex combinations of the elements in the set. However, since the set may contain infinite elements, there's a more helpful, equivalent definition...\n",
    "\n",
    "Let $C$ be a non-convex set. The convex hull of $C$ is the intersection of all convex supersets of $C$. That is, it's the intersection of all convex sets containing $C$. The result of such an intersection will be the smallest convex superset of $C$. \n",
    "\n",
    "In fact, this minimal convex superset is unique {% fn 1 %} and can therefore be taken as yet another, equivalent, definition for the convex hull of a set.\n",
    "\n",
    "Visualizing the convex hull of a non-convex set is similar to visualizing that of $n$ points — simply imagine the shape enclosed by a rubber band stretched around the non-convex set.\n",
    "\n",
    "### Affine Combination of $n$ Points\n",
    "\n",
    "An *affine combination* of points $x_1,...,x_n$ is a point of the form $x = \\theta_1 x_1 + ... + \\theta_n x_n$ with $\\sum_{i=1}^{n}\\theta_i = 1$ but where the $\\theta_i$'s need not be non-negative. \n",
    "\n",
    "For two points, the set of all affine combinations is the *line* that passes through them, whereas for three points it's the *plane*. in general, it is the plane in $(n-1)$-dimensions passing through the $n$ points.\n",
    "\n",
    "### Linear Combinations - Hyperplanes and Halfspaces\n",
    "\n",
    "A *linear combination* of $n$ vectors is all vectors of the form $x = \\theta_1 x_1 + ... + \\theta_n x_n$ with the $\\theta_i$'s totally unrestricted. \n",
    "\n",
    "The set of all linear combinations of $n$ vectors (i.e. points) is called their *span*. Formally, it is the set $\\{ \\theta_1 x_1 + ... + \\theta_n x_n \\ \\ | \\ \\ \\forall \\theta_1,...,\\theta_n \\}$.\n",
    "\n",
    "The span of a single vector is the line passing through it. For two vectors the span is the plane passing through them and, in general, the span of $n$ vectors is a plane in $n$-dimensions that contains these vectors.\n",
    "\n",
    "\n",
    "#### Hyperplanes\n",
    "\n",
    "For fixed weights $\\theta_i = a_i \\ \\ \\forall i$, a *hyperplane* is the set of all points $x \\in \\mathbb{R^n}$ whose linear combination equals a fixed constant $b \\in \\mathbb{R}$.\n",
    "\n",
    "Formally, a hyperplane is the set $\\{ x \\ \\ | \\ \\ a_1 x_1 + ... a_n x_n = b\\} = \\{ x \\ \\ | \\ \\ a^T x = b\\}$ \n",
    "\n",
    "There's a geometric interpretation of the parameters $a \\in \\mathbb{R^n}$ and $b \\in \\mathbb{R}$. Since the dot-product between perpendicular vectors is $0$, $\\{ x \\ \\ | \\ \\  a^T x = 0\\}$ is simply the set of all vectors perpendicular to $a$ (whose tail, as with all vectors in linear algebra, is considered to be fixed at the origin), making $a$ the *normal vector* to the hyperplane passing through the origin. To allow for parallel hyperplanes that are translated from the origin, the *offset* $b \\in \\mathbb{R}$ is introduced in the generalization $\\{ x \\ \\ | \\ \\ a^T x = b \\}$. This is now the set of all vectors whose dot-product with $a$ is constant. These vectors are not quite perpendicular to $a$, but they form a parallel hyperplane that's been shifted from the origin by a distance of $\\frac{|b|}{\\|a\\|_2}$.\n",
    "\n",
    "Since the sum $a_1 x_1 + ... a_n x_n = b$ is fixed, the last coordinate, which we'll call $x_k$ for some $k \\in [1,...,n]$, is fixed by the choice of the other $n-1$ coordinates. Therefore, a hyperplane  in $\\mathbb{R^n}$ spans $n-1$ dimensions instead of $n$.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708ebfb8",
   "metadata": {},
   "source": [
    "#### Halfspaces\n",
    "\n",
    "A *halfspace* is either of the two sub-spaces a hyperplane partitions the whole space into. Since the dot-product between vectors which are roughly in the same direction is positive, and vice versa, the two halfspaces associated to a hyperplane $\\{ x \\ \\ | \\ \\ a^T x = b\\}$ are $\\{ x \\ \\ | \\ \\ a^T x \\geq b\\}$ and $\\{ x \\ \\ | \\ \\ a^T x \\leq b\\} $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4427fa71",
   "metadata": {},
   "source": [
    "### Conic Combinations of $n$ Points\n",
    "\n",
    "A *conic combination* of $x_1,...x_n$ is a point $x = \\sum_{i=1}^{n} \\theta_i x_i$ where $\\theta_i \\geq 0 \\ \\ \\forall i$. Note that the absence of the restriction that $\\sum_{i=1}^{n} \\theta_i = 1$ is what distinguishes a conic combination from a convex combination. \n",
    "\n",
    "**A visual example:**\n",
    "\n",
    "![](my_icons/conic-combination.png \"The conic combination of vectors (0,1) and (1,1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc519372",
   "metadata": {},
   "source": [
    "### Ellipses\n",
    "\n",
    "Recall from Euclidean geometry that ellipses are conic sections. In general we define ellipses in $n$-dimensions as the [sub-level sets](https://en.wikipedia.org/wiki/Level_set) of [quadratic forms](https://en.wikipedia.org/wiki/Quadratic_form). That is $\\{ x \\ \\ | \\ \\ (x-c)^T M (x-c) \\leq 1 \\}$ where $M \\succeq 0$ defines the stretch along each principal axis, and $c \\in \\mathbb{R^n}$ is the center. \n",
    "\n",
    "An equivalent definition of an ellipse using the L2-norm is $\\{ x  \\ \\ |  \\ \\ \\|Ax - b\\|_2 \\leq 1 \\}$. That is, for a given $A$ and $b$ in the L2-norm definition, we can find an $M$ and $c$ in the sub-level set definition and vice versa.  \n",
    "\n",
    "> Note: More generally, the ellipse is $\\{ x \\ \\ | \\ \\ (x-c)^T M (x-c) \\leq r \\}$. However, since the scaling factor $r$ is positive, it can simply be absorbed into $Q$ without affecting $Q$'s positive semidefiniteness.\n",
    "<br>\n",
    "\n",
    "To quickly convince ourselves in the equivalence of these definitions, we take the simple case where $b = 0$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  \\|Ax\\|_2 &= ((Ax)^T(Ax))^{1/2} \\\\\n",
    "  &= (x^TA^TAx)^{1/2} \\\\\n",
    "  &= (x^TU D U^Tx)^{1/2} \\\\\n",
    "  &= x^TU D^{1/2} U^Tx \\\\\n",
    "  \\end{aligned}\n",
    "$$\n",
    "\n",
    "Where the third equality is by the [spectral decomposition](https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix#Real_symmetric_matrices) of the real symmetric matrix $A^TA$, in which $D = diag(\\lambda_1,...,\\lambda_n)$ is the diagnonal matrix of eigenvalues and the columns of $U$ are the corresponding eigenvectors. Taking $M= UD^{1/2}U^T$, where $D^{1/2}$ is simply $D^{1/2} = diag(\\sqrt\\lambda_1,...,\\sqrt\\lambda_n)$, we have the equivalent sub-level set definition of the ellipse. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133dd7c1",
   "metadata": {},
   "source": [
    "### Norm Balls\n",
    "\n",
    "Related to ellipses are *Euclidean balls*, which are *norm balls* for the choice of the L2-norm. A Euclidean ball has the form $\\{ x \\ \\ | \\ \\ \\|x\\|_2 \\leq r \\}$, and is clearly convex as it's a generalizations of the sphere in $n$-dimensions. \n",
    "\n",
    "But also, a Euclidean ball is the special ellipse for the choice of $M = rI$, and $c = 0$. \n",
    "\n",
    "In general, norm balls $\\{ x \\ \\ | \\ \\ \\|x\\|_p \\leq r\\}$ where $\\|x\\|_p = (x_1^p + ... + x_n^p)^{1/p}$ are convex for any choice of $p \\geq 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846d8b6d",
   "metadata": {},
   "source": [
    "### Polyhedra\n",
    "\n",
    "Where a halfspace is a set with one linear inequality constraint, a *polyhedron* is a set with many, but finite, such linear inequality constraints. These constraints can be packed into a matrix $A \\in \\mathbb{R^{m \\times n}}$ by vector $b \\in \\mathbb{R^m}$ multiplication form, making the polyhedron into the set $\\{x \\ \\ | \\ \\ Ax \\leq b\\}$.\n",
    "\n",
    "Since polyhedra are simply intersections of halfspaces and hyperplanes, and the latter are both convex, polyhedra are also convex sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0729f0f5",
   "metadata": {},
   "source": [
    "### The Set of All Positive Semidefinite Matrices\n",
    "\n",
    "The set of all PSD matrices $\\{ Q \\ \\ | \\ \\ x^TQx \\geq 0 \\ \\ \\forall x \\in \\mathbb{R^m}\\}$ is convex. We can, of course, use the definition of convexity to show this. But, a more elucidative approach would be the following remark. \n",
    "\n",
    "Note that $Q \\mapsto x^TQx$ is a [linear functional](https://en.wikipedia.org/wiki/Linear_form) that maps the space of all PSD matrices to its field of scalars. This is analogous to how $a \\mapsto x^Ta$ is a linear functional so, just as $\\{ a \\ \\ | \\ \\ x^Ta \\geq 0 \\}$ is a halfspace in the space of vectors, $H_x = \\{ Q \\ \\ | \\ \\ x^TQx \\geq 0 \\}$ for a given choice of $x \\in \\mathbb{R^m}$ is a halfspace in the space of PSD matrices. Halfspaces, as we already know, are convex and $\\{ Q \\ \\ | \\ \\ x^TQx \\geq 0 \\ \\ \\forall x \\in \\mathbb{R^m}\\}$ is nothing but an intersection of halfspaces for each choice of $x$. That is, $\\{ Q \\ \\ | \\ \\ x^TQx \\geq 0 \\ \\ \\forall x \\in \\mathbb{R^m}\\} = \\bigcap_x H_x$, concluding the proof of its convexity. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195b7d84",
   "metadata": {},
   "source": [
    "{{ '**Proof of uniqueness of the minimal, convex superset:** \n",
    "Suppose $C_1$ and $C_2$ are both minimal, convex supersets of $C$. Any convex set $D$ that contains $C$ must clearly contain the minimal, convex superset. Hence, $C_1 \\subseteq C_2$ and $C_2 \\subseteq C_1$, which implies $C_1 = C_2$.' | fndetail: 1 }}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7953a43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
