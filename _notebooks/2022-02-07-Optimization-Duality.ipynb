{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b980523b",
   "metadata": {},
   "source": [
    "# Optimization -  Duality\n",
    "\n",
    "> Lagrangian Duality, Weak and Strong Duality, Optimality Conditions, Farkas' Lemma, and Theorems of the Alternative\n",
    "\n",
    "- hide: false\n",
    "- toc: true\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: ['Optimization','Applied Mathematics','Proofs']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bd09b6",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Every convex optimization problem, designated as the ***primal***, has a related problem called its ***dual*** which can be colloquially thought of as its evil twin. The primal and the dual represent two different perspectives on the same problem. \n",
    "\n",
    "In the most general case, if the primal is a minimization problem, its dual is a maximization problem. In the case of constrained optimization, if the primal is minimization in $n$ variables and $m$ constraints then its dual is a maximization in $m$ variables and $n$ constraints. \n",
    "\n",
    "Furthermore, *any* feasible value of the dual is a lower-bound for *all* feasible values of the primal. In particular, should they both exist, the dual optimum is a lower bound for the primal optimum. This property, called ***weak duality***, is at the core of ***duality theory***. Having a problem that obtains, at the very least, a useful lower-bound for the primal optimum and, possibly, the primal optimum itself is the nascent idea of formulating the dual.\n",
    "\n",
    "In the best case scenario, problems exhibit a property called ***strong duality***, which guarantees the primal and the dual optima agree with each other. Strongly dual problems include, but are not limited to, all linear programs and a category of convex non-linear optimization problems. For such problems, solving the dual guarantees that we've also solved the primal. Furthermore, taking the dual of the dual gives back the primal. So this relationship is true in the converse â€” if we've solved the primal then we've also solved its dual.\n",
    "\n",
    "This is what makes duality theory so useful in practice. Having a related, usually easier, optimization problem gives applied scientists a huge computational advantage. However, even if the dual does not turn out to be any easier to solve or strong duality fails to hold, we still stand to gain structural insights about the primal problem.\n",
    "\n",
    "In this post we will show how the dual of a problem arises, we will examine in detail its relationship with the primal, and list all possible primal-dual outcomes. In doing so, we will look at duality in the general case of constrained optimization problems, in the specific case of linear programs, and in a certain category of unconstrained problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f48ee5",
   "metadata": {},
   "source": [
    "# The Dual of a Constrained Problem\n",
    "\n",
    "First, let's focus on deriving the dual of a constrained optimization problem. We shall see that, in a sense, constraints are what give rise to duality through the [Lagrangian](https://en.wikipedia.org/wiki/Lagrangian_relaxation). Certain types of unconstrained problems also have duals which arise from introducing dummy constraints or directly through the [Fenchel-Legendre Transform](https://en.wikipedia.org/wiki/Convex_conjugate).\n",
    "\n",
    "Take the most general form of a convex, constrained problem with $m$ inequality and $n$ equality constraints. To make the discussion interesting, assume the problem is non-trivial (i.e. its constraint set is non-empty and contains more than one feasible point). Furthermore, so that we may have a solution to speak of, assume the problem is bounded with the finite optimum $f_0(x^*)$ for some optimizer $x^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1876188",
   "metadata": {},
   "source": [
    "<br>\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_x &: f_0(x)\n",
    "\\\\\n",
    "s.t. &: \\begin{aligned} &f_i(x) \\leq 0 \\ \\ i = 1, ...,m\n",
    "\\\\ \n",
    "&h_i(x) = 0 \\ \\ i = 1, ... ,p\n",
    "\\end{aligned}\n",
    "\\end{aligned} \\tag{P}\n",
    "$$\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8442a16b",
   "metadata": {},
   "source": [
    "> Note: The $f_i$'s and the $h_i$'s in the constraints must necessarily be convex in order for their sublevel-sets, and hence the problem itself, to be convex. However, the equality constraints may be given as $Ax = b$ in some other sources. These representations are almost equivalent. The $0$-th level-set of $Ax - b$ is indeed a convex set. However, $h_i$'s in the equality constraints $h_i(x) = 0$ need not be linear for the their $0$-th level-set $\\{ x : h_i(x) = 0 \\}$ to be convex. For example, in $\\mathbb{R}$, $x^2 = 0$ does represent a convex level-set. Note, however, that $x^2 = 0$ can be reduced to $x = 0$ which is, indeed, linear. The notion of [quasi-linearity](https://en.wikipedia.org/wiki/Quasiconvex_function) is what's needed here but, in practice, we simply *define* a general convex problem as having only linear equality constraints. Doing so assists in the analysis of problems and in the development of computational methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5031e9",
   "metadata": {},
   "source": [
    "Since optimizing an unconstrained problem is considerably easier than optimizing a constrained problem, we seek to augment the constrained problem into an equivalent unconstrained problem. \n",
    "\n",
    "The idea is to penalize infeasible $x$ using functions that express our *displeasure* for certain choices. \n",
    "\n",
    "At first we use the *infinitely-hard penalty functions* $\\mathbb{1}_-$ and $\\mathbb{1}_0$ which are defined as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb43e8e",
   "metadata": {},
   "source": [
    "<br>\n",
    "$$\\mathbb{1}_-(u) = \n",
    "\\begin{cases}\n",
    "\\begin{aligned} \n",
    "&0  &\\textrm{if} \\ u \\leq 0\n",
    "\\\\\n",
    "&\\infty  &\\textrm{if} \\ u > 0\n",
    "\\end{aligned}\n",
    "\\end{cases}$$\n",
    "<br>\n",
    "$$\\mathbb{1}_0(u) = \n",
    "\\begin{cases}\n",
    "\\begin{aligned} \n",
    "&0  &\\textrm{if} \\ u = 0\n",
    "\\\\\n",
    "&\\infty  &\\textrm{if} \\ u \\ne 0\n",
    "\\end{aligned}\n",
    "\\end{cases}$$\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9b5169",
   "metadata": {},
   "source": [
    "Then the equivalent unconstrained problem can be stated as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535fe75d",
   "metadata": {},
   "source": [
    "<br>\n",
    "$$\\min_x: \\mathcal{J}(x)$$\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1c9cda",
   "metadata": {},
   "source": [
    "where $\\mathcal{J}(x) = f_0(x) + \\sum_{i=1}^m \\mathbb{1}_-(f_i(x)) + \\sum_{i=1}^p \\mathbb{1}_0(h_i(x))$. \n",
    "\n",
    "Equivalently, we can express the objective $\\mathcal{J}(x)$ as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a344e88e",
   "metadata": {},
   "source": [
    "<br>\n",
    "$$\\mathcal{J}(x) = \\begin{cases}\\begin{aligned} \n",
    "&f_0(x) \\ \\ \\textrm{if $x$ is feasible}\n",
    "\\\\\n",
    "&\\infty \\ \\ \\textrm{otherwise}\n",
    "\\end{aligned}\\end{cases}$$\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ada6b0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "dark"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.set_facecolor('#0a0a0a')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b77e49",
   "metadata": {},
   "source": [
    "Informally, if $\\hat x$ is chosen s.t *one or more* of the constraints are broken then the minimization incurs an infinitely positive penalty. Therefore, such a $\\hat x$ will never be selected over any feasible choice $x$ which gives a finite value $f_0(x)$. Moreover, by optimality of $x^*$ in the original problem, we have $f_0(x) \\leq f_0(x^*) \\ \\ \\forall x$. So, the optimum of $\\mathcal{J}(x)$ will also be $f_0(x^*)$.\n",
    "\n",
    "That is:\n",
    "\n",
    "<br>\n",
    "$$\\min_x \\mathcal{J}(x) = f_0(x^*) \\tag{1}$$\n",
    "\n",
    "Moreover, since the optimizer $x^*$ for the original problem is feasible, $\\mathcal{J}(x^*) = f_0(x^*)$ by definition. It follows from $(1)$ that:\n",
    "\n",
    "<br>\n",
    "$$\\mathcal{J}(x^*) = \\min_x \\mathcal{J}(x) \\tag{2.1}$$ \n",
    "\n",
    "Or, equivalently:\n",
    "\n",
    "<br>\n",
    "$$x^* = \\arg \\min_x \\mathcal{J}(x) \\tag{2.2}$$\n",
    "\n",
    "$(1)$ says that it suffices to minimize the unconstrained objective $\\mathcal{J}(x)$ instead of the original problem since doing so results in $f_0(x^*)$, the optimum of the unconstrained problem. $(2.1)$ and $(2.2)$, on the other hand, say that it suffices to find an optimizer $x^*$ of the unconstrained problem, since such a point will also be an optimizer of the constrained problem.\n",
    "\n",
    "As we know, the local optima of unconstrained problems occur at their *stationary points* which can be easily identified using the *unconstrained optimality condition*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0a9722",
   "metadata": {},
   "source": [
    "> **Unconstrained Optimality Condition** &nbsp; If $x^*$ is an optimizer of the unconstrained objective $f_0(x)$ then $\\nabla f_0(x^*) = 0$. That is $x^*$ is a ***stationary point** of $f_0(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d11bd46",
   "metadata": {},
   "source": [
    "Once the stationary points have been found, a global minimizer can be identified among them simply by evaluating the objective at each stationary point.\n",
    "\n",
    "However, we're immediately beset by a problem. We cannot find the gradient of $\\mathcal{J}(x)$ and set it to zero because the infinitely-hard penalty functions are discontinuous and non-differentiable. That is, $\\nabla \\mathcal{J}(x)$ simply does not exist.\n",
    "\n",
    "To sidestep this difficulty we use linear relaxations instead of $\\mathbb{1}_-$ and $\\mathbb{1}_0$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19550946",
   "metadata": {},
   "source": [
    "## The Lagrangian, Dual Variables, and the Dual Function\n",
    "\n",
    "The ***Lagrangian linear relaxation***, sometimes simply referred to as the ***Lagrangian***, is:\n",
    "\n",
    "<br>\n",
    "$$\\mathcal{L}(x,\\lambda,\\mu) = f_0(x) + \\sum_{i=1}^m \\lambda_i f_i(x) + \\sum_{i=1}^p \\mu_i h_i(x)$$\n",
    "$$\\textrm{where} \\ \\lambda \\geq 0$$ \n",
    "\n",
    "We call the $\\lambda_i$'s the ***Lagrange multipliers*** corresponding to the inequality constraints, and the $\\mu_i$'s those corresponding to the equality constraints. The vectors $\\lambda$ and $\\mu$, composed of these Lagrange multipliers, are called the ***Lagrange multiplier vectors*** or, for reasons that will soon become apparent, the ***dual variables***. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cd2d92",
   "metadata": {},
   "source": [
    "> Note: In some sources, the Lagrangian is simply stated as $\\mathcal{L}(x,\\lambda) = f_0(x) + \\sum_{i=1}^n \\lambda_i f_i(x)$. Indeed, by separating the equality constraints $h_i(x) = 0$ into $h_i(x) \\leq 0$ and $-h_i(x) \\leq 0$, we can transform a problem with equality constraints into one with only inequality constraints. So, this formulation of the Lagrangian is still general enough to account for problems with equality constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab381b8",
   "metadata": {},
   "source": [
    "## A Lagrangian Lower-Bound\n",
    "\n",
    "Not only does the Lagrangian relax the unconstrained augmentation of the constrained problem, it also plays a natural role in the formulation of the ***dual problem*** as promised.\n",
    "\n",
    "The first thing to note about the Lagrangian is that the coordinate-wise $\\lambda \\geq 0$ condition is crucial. This is because, in the event that an inequality constraint is violated, say $f_i(x) > 0$, the corresponding $\\lambda_i$ must be non-negative in order to apply a positive penalty to the minimization. \n",
    "\n",
    "On the other hand, $\\mu$ is free to assume any value since the equality constraints can be violated in either direction and both scenarios must be penalized.\n",
    "\n",
    "The second thing to note about the Lagrangian is that, even though it applies a positive penalty that scales linearly in the severity of the violation, this penalty is, nevertheless, not as severe as the infinite penalty applied in $\\mathcal{J}(x)$. Also, in the Lagrangian, we may actually be *rewarding* feasible choices of $x$ that have margin. That is, in the event that $f_i(x) < 0$, $\\lambda_if_i(x)$ is a non-positive reward for the minimization problem. \n",
    "\n",
    "All of this is to say that the Lagrangian is a point-wise lower-bound of the unconstrained problem. That is, the following inequality holds:\n",
    "\n",
    "<br>\n",
    "$$\\mathcal{L}(x,\\lambda,\\mu) \\leq J(x) \\ \\ \\forall x, \\lambda \\geq 0, \\mu \\tag{3.1}$$ \n",
    "\n",
    "This fact is also true by graphing each of the linear and infinite penalties and noticing that $\\lambda_i f_i(x) \\leq \\mathbb{1}_-(f_i(x))$ and $\\mu_i h_i(x) \\leq \\mathbb{1}_0(h_i(x))$ for all $i$ constraints. \n",
    "\n",
    "\n",
    "Taking $\\inf$ w.r.t. $x$ of the LHS in $(3.1)$, we obtain something of interest.\n",
    "\n",
    "<br>\n",
    "$$\\inf_x \\mathcal{L}(x,\\lambda,\\mu) \\leq J(x) \\ \\ \\forall x, \\lambda \\geq 0, \\mu \\tag{3.2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb43a338",
   "metadata": {},
   "source": [
    "> Note: The Lagrangian may not attain its $\\min$ w.r.t. $x$, in which case the LHS is simply $-\\infty$. We shall see later that the assumption of primal boundedness ensures this does not happen. So, from this point on, we will assume the minimum is attained. That is $\\inf_x \\mathcal{L}(x,\\lambda,\\mu) = \\min_x \\mathcal{L}(x,\\lambda,\\mu)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d648144b",
   "metadata": {},
   "source": [
    "Designating the original problem as the *primal* $(P)$, we call $g(\\lambda, \\mu) := \\min_x \\mathcal{L}(x, \\lambda, \\mu)$ the ***dual function*** because it exhibits the property of weak duality. That is, per $(3.2)$, any feasible value of $g(\\lambda, \\mu)$ is a lower-bound for any feasible value of the primal. \n",
    "\n",
    "Taking min of both sides in $(3.1)$, we have a more specific flavor of weak duality.\n",
    "\n",
    "<br>\n",
    "$$g(\\lambda,\\mu) \\leq \\min_x \\mathcal{J}(x) \\ \\ \\forall \\lambda \\geq 0, \\mu$$\n",
    "\n",
    "And, since $\\mathcal{J}(x^*) = f_0(x^*) = \\min_x \\mathcal{J}(x)$, we have:\n",
    "\n",
    "<br>\n",
    "$$g(\\lambda,\\mu) \\leq f_0(x^*) \\ \\ \\forall \\lambda \\geq 0, \\mu \\tag{3.3}$$\n",
    "\n",
    "That is, any feasible value of the dual is a lower-bound for the primal optimum.\n",
    "\n",
    "Maximizing both sides of $(3.3)$ by noticing that the RHS is a constant, and by assuming the LHS attains its $\\max$ we get an even more specific flavor of weak duality.\n",
    "\n",
    "<br>\n",
    "$$\\max_{\\lambda \\geq 0, \\mu} g(\\lambda,\\mu) \\leq f_0(x^*) \\tag{3.4}$$\n",
    "\n",
    "That is, the dual optimum is a lower-bound for the primal optimum.\n",
    "\n",
    "From here we move, quite naturally, to defining the *dual problem* $(D)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebdafaf",
   "metadata": {},
   "source": [
    "## The Lagrange Dual Problem\n",
    "\n",
    "It's natural, to ask what the *tightest* lower bound on the primal optimal value $f_0(x^*)$ is. This amounts to finding the values $\\lambda^* \\geq 0$, and $\\mu^*$ for which $g(\\lambda^*, \\mu^*)$ is maximized. We call this the ***Lagrange dual problem*** or, simply, the ***dual problem***.\n",
    "\n",
    "It can be stated as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b3abe3",
   "metadata": {},
   "source": [
    "<br>\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\max_{\\lambda, \\mu} &: g(\\lambda, \\mu)\n",
    "\\\\\n",
    "s.t. &: \\lambda \\geq  0\n",
    "\\end{aligned} \\tag{D}\n",
    "$$\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffc21fa",
   "metadata": {},
   "source": [
    "Looking at the above, it becomes immediately clear why we were motivated to call $\\lambda$, and $\\mu$ the *dual variables*. They are the variables of the dual problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5a9359",
   "metadata": {},
   "source": [
    "# Weak Duality and Interpretations\n",
    "\n",
    "We now return to the general setting of constrained optimization.\n",
    "\n",
    "We've already seen weak duality formulated as $(3.2)$, $(3.3)$, and $(3.4)$. But, there's yet another, more symmetric, formulation of weak duality as a [*Max-Min Inequality*](https://en.wikipedia.org/wiki/Max%E2%80%93min_inequality) which allows us to prove weak duality through a non-optimization lens. \n",
    "\n",
    "Suppose $x^*$ and $(\\lambda^*, \\mu^*)$ are primal-dual optimal. Then from $(3.4)$ we have weak duality in terms of the primal and dual optima $g(\\lambda^*, \\mu^*)$ as:\n",
    "\n",
    "<br>\n",
    "$$\n",
    "g(\\lambda^*, \\mu^*) \\leq f_0(x^*) \\tag{3.5}\n",
    "$$\n",
    "\n",
    "But since $g(\\lambda^*, \\mu^*)$ is the solution to the dual $(D)$, and $g(\\lambda, \\mu) = \\min_x \\mathcal{L}(x, \\lambda, \\mu)$:\n",
    "\n",
    "<br>\n",
    "$$g(\\lambda^*, \\mu^*) = \\max_{\\lambda \\geq 0, \\mu} \\left\\{ \\min_x \\mathcal{L}(x, \\lambda, \\mu) \\right\\} \\tag{4.1}$$\n",
    "\n",
    "Similarly, it can be shown that:\n",
    "\n",
    "<br>\n",
    "$$f_0(x^*) = \\min_x \\left\\{ \\max_{\\lambda \\geq 0, mu} \\mathcal{L}(x, \\lambda, \\mu) \\right\\} \\tag{4.2}$$\n",
    "\n",
    "To see this, note that for some $x$ fixed by the outer minimizer, maximizing the Lagrangian over $\\lambda \\geq 0$ and $\\mu$ recovers $\\mathcal{J}(x)$. \n",
    "\n",
    "If all of the inequality constraints are respected, that is $f_i(x) \\leq 0$ $\\forall i$, then, in order to maximize the Lagrangian, the best we can do is set $\\lambda_i = 0$ $\\forall i$. In case *any* inequality constraint is violated, that is $f_i(x) > 0$ for some $i$, the result of maximizing the Lagrangian can be made $\\infty$ by choosing $\\lambda_i \\rightarrow \\infty$ and $\\lambda_j = 0$ $\\forall j \\ne i$. \n",
    "\n",
    "Using similar logic, if all equality constraints are respected then $h_i(x) = 0$ $\\forall i$. In this case $\\mu_i$ can be chosen to be any value. If, on the other hand, some equality constraint is violated then $h_i(x) \\ne 0$ for some $i$. By choosing $\\mu_i \\rightarrow \\pm \\infty$, where the sign depends on the direction of the violation, the result can be made $\\infty$.\n",
    "\n",
    "Thus we have shown that:\n",
    "\n",
    "<br>\n",
    "$$\\begin{aligned}\\max_{\\lambda \\geq 0, \\mu} \\mathcal{L}(x,\\lambda,\\mu) &= \\begin{cases}\\begin{aligned} \n",
    "&f_0(x) \\ \\ \\textrm{if $x$ is feasible}\n",
    "\\\\\n",
    "&\\infty \\ \\ \\textrm{otherwise}\n",
    "\\end{aligned}\\end{cases} \\\\ &= \\mathcal{J}(x)\\end{aligned}$$\n",
    "\n",
    "Now, since $x^*$ is the solution to the primal $(P)$ and $\\min_x J(x) = f_0(x^*)$ we have $(4.2)$ as promised. \n",
    "\n",
    "Then, weak duality can be stated as:\n",
    "\n",
    "<br>\n",
    "$$\\max_{\\lambda \\geq 0, \\mu} \\left\\{ \\min_x \\mathcal{L}(x, \\lambda, \\mu) \\right\\} \\leq \\min_x \\left\\{ \\max_{\\lambda \\geq 0, mu} \\mathcal{L}(x, \\lambda, \\mu) \\right\\} \\tag{3.6}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9157cb84",
   "metadata": {},
   "source": [
    "## The Max-Min Inequality\n",
    "\n",
    "The Max-Min Inequality makes no assumptions about the function. It's simply true for all functions of the form $f: X \\times Y \\rightarrow \\mathbb{R}$, and it states that:\n",
    "\n",
    "<br>\n",
    "$$\n",
    "\\sup_{x\\in X} \\left\\{ \\inf_{y\\in Y} f(x,y) \\right\\} \\leq \\inf_{y\\in Y} \\left\\{ \\sup_{x\\in X} f(x,y) \\right\\}\n",
    "$$\n",
    "\n",
    "Since no assumption is made on $f$, the inequality also holds for the Lagrangian, $\\mathcal{L}$. And, since we're in the special case where the optimal values of the primal $(P)$ and the dual $(D)$ are assumed to exist, the functions attain the respective optima. That is, we can replace $\\sup$ and $\\inf$ in the inequality with $\\max$ and $\\min$ which obtains the promised symmetric formulation of weak duality as $(3.6)$.\n",
    "\n",
    "We can now prove weak duality through a non-optimization lens by proving the Max-Min Inequality.\n",
    "\n",
    "For any $f$, and $x \\in X$, $y \\in Y$ we have:\n",
    "\n",
    "<br>\n",
    "$$f(x,y) \\leq \\sup_y f(x,y) \\ \\ \\forall x$$\n",
    "\n",
    "The right hand side is now only a function of $x$, so minimizing both sides w.r.t. $x$ yields:\n",
    "\n",
    "<br>\n",
    "$$ \\inf_x f(x,y) \\leq \\inf_x \\left\\{ \\sup_y f(x,y) \\right\\} \\ \\ \\forall y$$\n",
    "\n",
    "The right hand side is now a constant, so maximizing both sides w.r.t. $y$ results in the desired conclusion.\n",
    "\n",
    "<br>\n",
    "$$\\sup_y \\left\\{ \\inf_x f(x,y) \\right\\} \\leq \\inf_x \\left\\{ \\sup_y f(x,y) \\right\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c0ae91",
   "metadata": {},
   "source": [
    "> Note: Observe how the Max-Min Inequality proof mirrors the steps taken to obtain $(3.2)$-$(3.4)$ from $(3.1)$. In fact, $(3.1)$ is of form $f(x,y) \\leq \\sup_y f(x,y) \\ \\ \\forall x$, which is the first step of the Max-Min Inequality proof, since $J(x)$ is, as shown earlier, $\\max_{\\lambda \\geq 0, \\mu} L(x, \\lambda, \\mu)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311ebbd2",
   "metadata": {},
   "source": [
    "### Game-Theoretic Interpretation\n",
    "\n",
    "The Max-Min Inequality is perhaps best understood intuitively as the following game between two adversarial optimizers. \n",
    "\n",
    "The LHS of the Max-Min Inequality can be interpreted as the following game. First, the outer maximizer, player $Y$, fixes a $y$. Then, the inner minimizer, player $X$, chooses $x_y = \\arg \\inf_x f(x,y)$ which depends on the outer's choice of $y$. Suppose $y^* = \\arg \\inf_y f(x,y)$ is what player $Y$'s choice would have been were it to act independently of the actions of player $X$. We can imagine a scenario in which the score $f(x_{y^*}, y^*)$ is less than the score $f(x_y, y)$ for some other choice of $y$. So, player $Y$ cannot do as well as it would've done independently, whereas player $X$ is free to do its best. Hence, player $X$, the second player, restricts the choices of player $Y$, the first player. \n",
    "\n",
    "Hence, if the goal is to score low, then player $X$ has the advantage if it goes second. Conversely, if the goal is to score high player $Y$ has the advantage if it goes second. This is exactly what the Max-Min Inequality says."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20b7730",
   "metadata": {},
   "source": [
    "# Strong Duality\n",
    "\n",
    "Strong Duality is the case in which the primal and the dual optimal values agree with strict equality. \n",
    "<br>\n",
    "$$\n",
    "g(\\lambda^*, \\mu^*) = f_0(x^*) \\tag{SD - 1}\n",
    "$$\n",
    "\n",
    "Alternatively, in its Max-Min characterization:\n",
    "<br>\n",
    "$$\\max_{\\lambda \\geq 0, \\mu} \\left\\{ \\min_x \\mathcal{L} (x, \\lambda, \\mu) \\right\\} = \\min_x \\left\\{ \\max_{\\lambda \\geq 0, \\mu} \\mathcal{L} (x, \\lambda, \\mu) \\right\\} \\tag{SD - 2}$$\n",
    "\n",
    "Optimization problems that exhibit this property are called Strongly Dual. \n",
    "\n",
    "In keeping with the game theoretic intuition of Weak Duality, one can imagine a game in which the first player's optimal choice is independent of the second player's actions. In such a game, both players are free to play their best strategies and, consequently, the order of play is not important.\n",
    "\n",
    "As mentioned briefly in the introduction, Strong Duality gives applied scientists the ability to solve a related, possibly easier, optimization problem instead of the original, possibly difficult, one. As we shall see it also obtains powerful optimality conditions. For these reasons and more, knowing in advance whether or not a problem is Strongly Dual will be useful to us.\n",
    "\n",
    "We will see that all linear programs are Strongly Dual by a direct proof, but, when it comes to non-linear optimization, Strong Duality is not a general guarantee. The good news is that sufficient conditions for Strong Duality do exist, and will be provided shortly.\n",
    "\n",
    "\n",
    "## An Easier Dual Problem \n",
    "\n",
    "Let's further qualify what we mean when we say Strong Duality gives us a possibly easier problem to solve. \n",
    "\n",
    "The primal, possibly non-convex, problem is that of finding the primal optimal value \n",
    "<br>\n",
    "$$f_0(x^*) = \\min_x \\left\\{ \\max_{\\lambda \\geq 0, \\mu} \\mathcal{L} (x, \\lambda, \\mu) \\right\\}$$\n",
    "\n",
    "But maximizing the Lagrangian over $\\lambda \\geq 0$, and $\\mu$, recovers $J(x)$ which is a non-differentiable objective. So, we cannot use the unconstrained optimality condition in finding its stationary points which is required in the next step.\n",
    "\n",
    "Meanwhile, the dual problem is that of finding the dual optimal value \n",
    "<br>\n",
    "$$g(\\lambda^*, \\mu^*) = \\max_{\\lambda \\geq 0, \\mu} \\left\\{ \\min_x \\mathcal{L} (x, \\lambda, \\mu) \\right\\}$$\n",
    "\n",
    "Minimizing the Lagrangian over $x$ may still be difficult, but, at least, it lends itself to using the method of unconstrained optimization (i.e. using the unconstrained optimality condition). Moreover, the resulting dual function $g(\\lambda, \\mu) = \\min_x \\mathcal{L}(x, \\lambda, \\mu)$ is a point-wise minimum of linear functions in $\\lambda$ and $\\mu$, so its always concave in those variables. And, the constraint $\\lambda \\geq 0$ is simply a linear constraint.  \n",
    "\n",
    "So, overall, the dual problem is convex maximization which is always easier to solve than a non-convex primal problem. Furthermore, if the primal is convex with more variables than constraints then the dual is also convex with more constraints than variables which makes it an easier problem to solve. \n",
    "\n",
    "In these cases, if Strong Duality holds, the dual optimal value $g(\\lambda^*, \\mu^*)$ we find is guaranteed to be equal to the primal optimal value $f_0(x^*)$. Therefore, we've found an easier approach to the primal problem. If Strong Duality fails to hold then, at least, we've found a useful lower-bound to the primal optimal value. \n",
    "    \n",
    "## Optimality Conditions\n",
    "    \n",
    "Strong Duality obtains powerful optimality conditions known as *Stationarity Condition* and *Complementary Slackness*. These are often collected into the [*Karushâ€“Kuhnâ€“Tucker (KKT) Conditions*](https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions).\n",
    "\n",
    "In the absence of Strong Duality the KKT Conditions are necessary but insufficient for optimality. However, for problems which are Strongly Dual the KKT Conditions become a *certificate of optimality*. That is, they are both necessary and sufficient.\n",
    "    \n",
    "### Stationarity Condition\n",
    "\n",
    "In the section titled [An Easier Dual Problem](https://v-poghosyan.github.io/blog/optimization/applied%20mathematics/proofs/2022/02/07/Optimization-LP-Duality.html#An-Easier-Dual-Problem) we mentioned that the dual problem is that of finding the dual optimal value \n",
    "<br>\n",
    "$$g(\\lambda^*, \\mu^*) = \\max_{\\lambda \\geq 0, \\mu} \\left\\{ \\min_x \\mathcal{L} (x, \\lambda, \\mu) \\right\\}$$\n",
    "\n",
    "We mentioned that deriving the dual function, and then solving the dual problem obtains the primal optimal value $f_0(x^*)$ by Strong Duality. But we said nothing about the minimizer $x_{\\lambda, \\mu} = \\arg \\min_x \\mathcal{L} (x, \\lambda, \\mu)$ itself. \n",
    "\n",
    "Turns out, if Strong Duality holds with $x^*$ and $(\\lambda^*, \\mu^*)$ being primal-dual optimal, then $x^* = x_{\\lambda^*, \\mu^*}$. That is, if the outer maximizer fixes $(\\lambda^*, \\mu^*)$ then the inner minimizer's choice of $x$ is the none other than the primal optimal $x^*$.\n",
    "\n",
    "In other words, the primal optimal $x^*$ is also a stationary point of the Lagrangian at $(\\lambda^*,\\mu^*)$. That is:\n",
    "<br>\n",
    "$$\\arg \\min_x \\mathcal{L} (x, \\lambda^*, \\mu^*) = x^* \\tag{5.1}$$ \n",
    "\n",
    "Or, equivalently:\n",
    "<br>\n",
    "$$\\min_x \\mathcal{L}(x, \\lambda^*, \\mu^*) = \\mathcal{L}(x^*, \\lambda^*, \\mu^*) \\tag{5.2}$$\n",
    "\n",
    "This is what we've been looking for. Recall that the original motivation in defining the unconstrained objective $\\mathcal{J}$ and then relaxing it to $\\mathcal{L}$ was to transform the constrained problem into an unconstrained one that was solvable using the unconstrained optimality condition. $(5.1)$ and $(5.2)$ say we can do just that. We can find the optimizer $x^*$ of the original constrained problem by minimizing the unconstrained objective $\\mathcal{L}(x, \\lambda^*, \\mu^*)$.\n",
    "\n",
    "In practice, however, $(5.1)$ and $(5.2)$ only give us a way to solve for a primal optimal $x^*$ directly if a dual optimal $(\\lambda^*, \\mu^*)$ is already known. That is, any time the dual problem is easier to solve than the primal.\n",
    "\n",
    "More generally, this fact gives us a way to check if a pair $(x^*,(\\lambda^*,\\mu^*))$ is primal-dual optimal â€“ an optimality condition known as *Stationarity Condition* $(SC)$.\n",
    "\n",
    "<br>\n",
    "> **Stationarity Condition (SC):** &nbsp; Suppose $x^*$ and $(\\lambda^*, \\mu^*)$ are primal-dual optimal for a Strongly Dual problem. Then:\n",
    "$$\\nabla_x f_0(x^*) + \\sum_i^m \\lambda^*_i\\nabla_xf_i(x^*) + \\sum_{i=1}^p \\mu^*_i\\nabla_xh_i(x^*) = 0$$\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "We can get $(SC)$ by applying the unconstrained optimality condition to $\\mathcal{L}(x, \\lambda^*, \\mu^*)$. In doing so, we obtain:\n",
    "<br>\n",
    "$$\\nabla_x \\mathcal{L} (x^*, \\lambda^*, \\mu^*) = 0$$ \n",
    "\n",
    "Then, expanding the LHS gives:\n",
    "<br>\n",
    "$$\\nabla_x f_0(x^*) + \\sum_i^m \\lambda^*_i\\nabla_xf_i(x^*) + \\sum_{i=1}^p \\mu^*_i\\nabla_xh_i(x^*) = 0 \\tag{SC}$$ \n",
    "\n",
    "For the sake of completeness, since we stated it without offering a proof, let's prove the equivalent claims $(5.1)$ and $(5.2)$ from which $(SC)$ ultimately follows.\n",
    "\n",
    "#### Proof of Claims (5.1) and (5.2)\n",
    "\n",
    "Suppose $x^*$ and $(\\lambda^*, \\mu^*)$ are primal-dual optimal for a Strongly Dual problem as in the hypothesis. \n",
    "\n",
    "The following point-wise inequality holds in general since the LHS is a minimization over $x$ and the RHS is a maximization over $(\\lambda, \\mu)$ of the Lagrangian.\n",
    "<br>\n",
    "$$g(\\lambda, \\mu) \\leq \\mathcal{L}(x, \\lambda, \\mu) \\leq \\mathcal{J}(x) \\ \\ \\forall x, \\lambda \\geq 0, \\mu \\tag{6.1}$$\n",
    "\n",
    "It is also, in particular, true for the primal-dual optimal pair. That is: \n",
    "<br>\n",
    "$$g(\\lambda^*, \\mu^*) \\leq \\mathcal{L}(x^*, \\lambda^*, \\mu^*) \\leq \\mathcal{J}(x^*) \\tag{6.2}$$\n",
    "\n",
    "However, $\\mathcal{J}(x^*) = f_0(x^*)$ and, by $(SD \\ 1)$, $g(\\lambda^*, \\mu^*) = f_0(x^*)$.\n",
    "\n",
    "Then $(6.2)$ is forced to be: \n",
    "<br>\n",
    "$$\\mathcal{L}(x^*, \\lambda^*, \\mu^*) = g(\\lambda^*, \\mu^*) \\tag{6.3}$$ \n",
    "\n",
    "Substituting, the definition of the dual function for the RHS of $(6.3)$, we get:\n",
    "<br>\n",
    "$$\\mathcal{L}(x^*, \\lambda^*, \\mu^*) = \\min_x \\mathcal{L}(x, \\lambda^*, \\mu^*)$$ \n",
    "\n",
    "Which is exactly $(5.2)$.\n",
    "\n",
    "### Complementary Slackness\n",
    "\n",
    "Strong Duality also gives another optimality condition known as *Complementary Slackness* $(CS)$.\n",
    "\n",
    "<br>\n",
    "> **Complementary Slackness (CS):** &nbsp; Suppose $x^*$ and $(\\lambda^*, \\mu^*)$ are primal-dual optimal for a Strongly Dual problem. Then $\\lambda^*_i f_i(x^*) = 0 \\ \\ \\forall i$.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Informally, if a primal constraint at an optimal $x^*$ is *loose*, that is $f_i(x^*) \\ne 0$, then its corresponding dual variable $\\lambda^*_i$ in the dual optimal $\\lambda^*$ must be zero. Conversely, if the dual variable $\\lambda_i^*$ is positive then the corresponding constraint must be *tight*.\n",
    "\n",
    "Note that if a primal constraint is *tight* at $x^*$, Complementary Slackness tells us nothing about its corresponding dual variable. \n",
    "\n",
    "#### Proof of Complementary Slackness\n",
    "\n",
    "Suppose $x^*$ and $(\\lambda^*, \\mu^*)$ are primal-dual optimal for a Strongly Dual problem as in the hypothesis. \n",
    "\n",
    "Expanding the RHS we obtain:\n",
    "<br>\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f_0(x^*) &= g(\\lambda^*, \\mu^*) \\\\ \n",
    "&= \\min_x \\mathcal{L}(x, \\lambda^*, \\mu^*) \\\\ \n",
    "&= \\mathcal{L}(x^*, \\lambda^*, \\mu^*) \\\\\n",
    "&=  f_0(x^*) + \\sum_{i=1}^m \\lambda_i^* f_i(x) + \\sum_{i=1}^p \\mu_i^* h_i(x^*) \\\\\n",
    "&\\leq f_0(x^*)\n",
    "\\end{aligned} \\tag{7}\n",
    "$$\n",
    "\n",
    "The first equality holds by Strong Duality, particularly $(SD \\ 1)$, the second holds by the definition of the dual function, the third equality holds by $(5.2)$, and the fourth is true by the expansion of $\\mathcal{L}(x^*, \\lambda^*, \\mu^*)$.\n",
    "\n",
    "To see why the last inequality holds, note that $\\sum_{i=1}^p \\mu_i^* h_i(x^*) = 0$ since $h_i(x^*) = 0 \\ \\ \\forall i$ by feasibility of $x^*$. Then again, by feasibility of $x^*$, $\\forall i$ $f_i(x^*) \\leq 0$. And since, by construction of $\\mathcal{L}$, $\\lambda \\geq 0$ we have $\\sum_{i=1}^m \\lambda^*_i f_i(x^*) \\leq 0$. \n",
    "\n",
    "But taken altogether $(7)$ says $f_0(x^*) \\leq f_0(x^*)$ which can *only* hold through strict equality. \n",
    "\n",
    "Then it must be the case that $\\sum_{i=1}^m \\lambda^*_i f_i(x^*) = 0$.\n",
    "\n",
    "Being a sum of non-positive terms, $\\sum_{i=1}^m \\lambda^*_i f_i(x^*) = 0$ *if and only if* $\\lambda^*_i f_i(x^*) = 0 \\ \\ \\forall i$ which is what we wanted to show.\n",
    "\n",
    "\n",
    "### Karush-Kuhn-Tucker (KKT) Conditions \n",
    "\n",
    "As mentioned above, Complementary Slackness and Stationarity Condition are often packaged together into the KKT Conditions. \n",
    "\n",
    "<br>\n",
    "> **KKT Conditions:** &nbsp; $x^*, (\\lambda^*, \\mu^*)$ satisfy the KKT conditions if the following hold:\n",
    "&nbsp;\n",
    "> 1. $\\nabla_x f_0(x^*) + \\sum_{i=1}^m \\lambda^*_i\\nabla_xf_i(x^*) + \\sum_{i=1}^p \\mu^*_i\\nabla_xh_i(x^*) = 0$\n",
    "> 2. $\\lambda^*_if_i(x^*) = 0 \\ \\ \\forall i$\n",
    "> 3. $g_i(x^*) \\leq 0 \\ \\ \\forall i$ \n",
    "> 4. $h_i(x^*) = 0 \\ \\ \\forall i$ \n",
    "> 5. $\\lambda^* \\geq 0$\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "We recognize $(KKT \\ 1)$ as being the Stationarity Condition, and $(KKT \\ 2)$ as being the Complementary Slackness condition. $(KKT \\ 3)$ through $(KKT \\ 5)$ simply ensure primal and dual feasibility.\n",
    "\n",
    "Primal-dual pairs which satisfy the KKT Conditions are called *KKT Pairs*.\n",
    "\n",
    "> Note: These conditions apply only to problems with differentiable objective and constraints. For the case in which one (or more) of the objective or constraints is not differentiable, there is a subdifferential version of the KKT Conditions. However, subdifferentials are beyond the scope of this post.\n",
    "<br>\n",
    "\n",
    "As promised, the KKT Conditions together with Strong Duality give a certificate of optimality.\n",
    "\n",
    "<br>\n",
    "> **Certificate of Optimality** &nbsp; If Strong Duality holds, then $x^*, (\\lambda^*, \\mu^*)$ are primal-dual optimal if and only if they are a KKT Pair. \n",
    "<br> \n",
    "<br>\n",
    "\n",
    "We have already shown one direction of this in the sections on the [Stationarity Condition](https://v-poghosyan.github.io/blog/optimization/applied%20mathematics/proofs/2022/02/07/Optimization-Duality.html#Stationarity-Condition) and [Complementary Slackness](https://v-poghosyan.github.io/blog/optimization/applied%20mathematics/proofs/2022/02/07/Optimization-Duality.html#Complementary-Slackness), where we proved that being a primal-dual optimal pair in a Strongly Convex problem guarantees $(x^*, (\\lambda^*, \\mu^*))$ is also a KKT Pair. \n",
    "\n",
    "In fact, as we shall see through an application of [*Farka's Lemma*](https://en.wikipedia.org/wiki/Farkas%27_lemma), $x^*$ being primal optimal is enough to guarantee the existence of a $(\\lambda, \\mu)$ s.t. $(x^*, (\\lambda, \\mu))$ is a KKT Pair without the need for Strong Duality. If Strong Duality does hold, however, we also have that the obtained $(\\lambda, \\mu)$ is dual optimal. That is, we get the second direction in the certificate of optimality â€” Being a KKT Pair in a Strongly Dual problem guarantees primal-dual optimality. \n",
    "\n",
    "Later on in this post we offer a proof of the above for the simple case of linear programs where we construct a dual variable from a primal optimal $x^*$ by enforcing the KKT Conditions, and then show that it is, indeed, dual optimal.\n",
    "\n",
    "#### Generalization of Unconstrained Optimization\n",
    "\n",
    "The KKT Conditions represent a strict generalization of the unconstrained optimality condition for use in constrained problems. \n",
    "\n",
    "Note that if there are no constraints, the KKT Conditions simply reduce to the familiar unconstrained optimality condition:\n",
    "<br>\n",
    "$$\\nabla_x f_0(x^*) = 0$$\n",
    "\n",
    "In order to discuss optimality in constrained problems, we must first define the concept of a *feasible direction*. \n",
    "\n",
    "<br>\n",
    "> **Feasible Direction:** &nbsp; A unit vector $d$ is called a *feasible direction* at any $x$ if $x + \\epsilon d$ remains feasible for $\\epsilon > 0$ small enough.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Then, we can generalize the unconstrained optimality condition by using Taylor Expansion as follows. \n",
    "\n",
    "For small enough $\\epsilon > 0$, we can estimate $f_0(x^* + \\epsilon d)$, where $x^*$ is optimal, for any feasible $d$ by its linear approximation:\n",
    "<br> \n",
    "$$f_0(x^* + \\epsilon d) = f_0(x^*) + \\epsilon \\nabla f_0(x^*)^Td$$\n",
    "\n",
    "But since $x^*$ is optimal, we have:\n",
    "<br>\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f_0(x^*) &\\leq f_0(x^* + \\epsilon d) \\\\\n",
    "& = f_0(x^*) + \\epsilon \\nabla f_0(x^*)^Td\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Which means $\\nabla f_0(x^*)^Td \\geq 0$. And since $d$ was an arbitrary feasible direction, the result holds for all feasible directions $d$. \n",
    "\n",
    "<br>\n",
    "> **Constrained Optimality Condition:** &nbsp; If $x^*$ is an optimizer of $f_0$ over some constraint set then, for any feasible direction $d$ at $x^*$, $\\nabla f_0(x^*)^Td \\geq 0$.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "In words, the directional derivative of the objective function in *any* feasible direction at the optimizer must be non-negative. This ensures that moving in any allowable direction does not improve the objective.\n",
    "\n",
    "We've already seen that being a primal-dual optimal pair guarantees that $(x^*, (\\lambda^*, \\mu^*))$ is also a KKT Pair. But, in showing that, we did not use the constrained optimality condition at $x^*$. Doing so is worth it, however, because it provides a key geometric insight. So, let's show that if $x^*$ satisfies the constrained optimality condition then its KKT Pair exists. \n",
    "\n",
    "If a particular constraint is loose at $x^*$ then taking a small enough step in any direction from $x^*$ does not violate it. Formally, if $f_i(x^*) < 0$, then $f_i(x^* + \\epsilon d) \\leq 0 \\ \\ \\forall d$. So, loose constraints do not pose any restrictions on the feasible directions.\n",
    "\n",
    "However, if a constraint is tight at $x^*$, that is $f_i(x^*) = 0$, then we must be careful not to violate it. For small enough $\\epsilon > 0$, we can estimate $f_i(x^* + \\epsilon d)$ by its linear Taylor Expansion as:\n",
    "<br> \n",
    "$$f_i(x^* + \\epsilon d) = f_i(x^*) + \\epsilon \\nabla f_i(x^*)^Td$$\n",
    "\n",
    "For feasibility, we want $f_i(x^* \\epsilon d) \\leq 0$. So, we require:\n",
    "<br>\n",
    "$$f_i(x^*) + \\epsilon \\nabla f_i(x^*)^Td \\leq 0$$\n",
    "\n",
    "But since $f_i$ is tight at $x^*$, $f_i(x^*) = 0$, which leaves us with:\n",
    "<br> \n",
    "$$\\nabla f_i(x^*)^Td \\leq 0 \\ \\ \\forall i \\ \\textrm{that are binding at $x^*$}$$\n",
    "\n",
    "Clearly the above is a restriction on $d$. The feasible directions can now be stated as:\n",
    "<br>\n",
    "$$d \\ \\textrm{s.t.} \\ \\nabla f_i(x^*)^Td \\leq 0 \\ \\ \\forall i \\ \\textrm{that are binding at $x^*$} \\tag{8.1}$$\n",
    "\n",
    "Or, equivalently:\n",
    "<br>\n",
    "$$d \\ \\textrm{s.t.} \\ - \\nabla f_i(x^*)^Td \\geq 0 \\ \\ \\forall i \\ \\textrm{that are binding at $x^*$} \\tag{8.2}$$\n",
    "\n",
    "But, since $x^*$ is optimal, by the generalized unconstrained optimality condition:\n",
    "<br>\n",
    "$$\\nabla f_0(x^*)^Td \\geq 0 \\ \\ \\forall \\ \\textrm{feasible} \\ d \\tag{8.3}$$. \n",
    "\n",
    "That is, for all $d$ as in $(8.2)$.\n",
    "\n",
    "But together, $(8.2)$ and $(8.3)$ say that $\\not \\exists \\ d$ which defines a separating hyperplane between $\\nabla f_0(x^*)$ and $-\\nabla f_i(x^*)$ for all binding constraints $i$. This means that the *only other* alternative scenario must be true â€” $\\nabla f_0(x^*)$ must lie in the cone of the $-\\nabla f_i(x^*)$'s. Incidentally, this is what's known as a *theorem of the alternative*, specifically Farka's Lemma, which will soon be covered in detail.\n",
    "\n",
    "Formally, $\\exists \\ \\lambda^* \\geq 0$ s.t.\n",
    "<br>\n",
    "$$\\nabla f_0(x^*) + \\sum_{i \\in I} \\lambda^*_i f_i(x^*) = 0 \\tag{8.4}$$\n",
    "\n",
    "Where $I = \\{i : f_i(x^*) = 0 \\}$ is the set of active inequality constraints.\n",
    "\n",
    "But, upon closer examination, $(8.4)$ is exactly $(KKT \\ 1)$, $(KKT \\ 2)$, and $(KKT \\ 5)$ rolled into one condition. The remaining conditions, $(KKT \\ 3)$ and $(KKT \\ 4)$, of course, follow from the assumed feasibility of $x^*$.\n",
    "\n",
    "We can also show that if $(x^*, (\\lambda^*, \\mu^*))$ is a KKT Pair then $x^*$ is optimal by the constrained optimality condition. Note, however, this says nothing about the dual optimality of $(\\lambda^*, \\mu^*)$ if no assumption of Strong Duality is made. The argument is simply reversed. If $\\nabla f_0(x^*)$ is in the aforementioned cone then going in any feasible direction makes life worse. Which is exactly what the constrained optimality condition says. This, of course, provides a strong geometric interpretation of the Stationary Condition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b065c49",
   "metadata": {},
   "source": [
    "## Slater's Condition - Sufficient Condition for Strong Duality\n",
    "\n",
    "Even though not all Strongly Dual problems are convex and not all convex programs are Strongly Dual, convexity together with *Slater's Condition* is sufficient for Strong Duality. \n",
    "\n",
    "<br>\n",
    "> **Slater's Condition:** &nbsp; $\\exists \\ \\hat x$ s.t. $f_i(\\hat x) < 0$, and $h_i(\\hat x) = 0$ $\\forall i$.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "> Note: The equality constraints $h_i(\\hat x) = 0$ are often stated as linear equality constraints $A \\hat x = b$ in certain sources. This is simply due to the observation that equality constraints are convex constraints if and only if all of the $h_i$'s are linear.\n",
    "<br>\n",
    "\n",
    "Informally, Slater's Condition says that the existence of a feasible point which has margin w.r.t. all the inequality constraints is needed in addition to convexity. In even simpler terms, the feasible region must have an interior point. \n",
    "\n",
    "The sufficient condition for Strong Duality is then\n",
    "\n",
    "<br>\n",
    "> **Sufficient Condition for Strong Duality:** &nbsp; Any convex optimization problem satisfying Slater's Condition has Strong Duality.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "The proof of this is beyond what we're trying to achieve in this post. However we motivate it geometrically in the following paragraph. \n",
    "\n",
    "### Geometric Intuition Behind Slater's Condition\n",
    "\n",
    "PICTURE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1b5ce6",
   "metadata": {},
   "source": [
    "# Duality and Sensitivity - Economic Intuition of Complementary Slackness\n",
    "\n",
    "In order to understand what Complementary Slackness means intuitively, the concept of dual variables as *marginal prices* is useful. The dual variable associated with a primal constraint is called the marginal price of that constraint. It represents how much the objective function *would* improve were the constraint relaxed. \n",
    "\n",
    "That is, suppose our goal is to maximize profit and a particular constraint represents how much of a resource we are allowed to utilize. The question we're asking ourselves is: if we  utilized more of this resource, how much would we gain in profit? \n",
    "\n",
    "Complementary Slackness says: if the marginal price (i.e. the dual variable $\\lambda^*_i$) is positive then the profit could be increased by utilizing more of this resource. Hence, the constraint corresponding to $\\lambda^*_i$ must be tight. Otherwise $x^*$ is not optimal, and a better solution can be found by making the non-binding constraint binding. \n",
    "\n",
    "Note that in non-linear optimization this is only local behavior. That is, $\\lambda^*_i$ predicts the improvement in the objective function *only* for small-enough change in the amount $f_i(x^*)$ of resource utilized.\n",
    "\n",
    "We will see why this is the case when we consider the geometric intuition of Complementary Slackness in the case of linear programs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bad557",
   "metadata": {},
   "source": [
    "## Saddle Point Interpretation\n",
    "\n",
    "In this section, we will give the saddle point interpretation of Strong Duality through the *Saddle Point Theorem* which states the following.\n",
    "\n",
    "<br>\n",
    "> **Saddle Point Theorem:** &nbsp; If $x^*$ and $(\\lambda^*, \\mu^*)$ are primal and dual optimal solutions for a convex problem which satisfies Slater's Condition, they form a saddle point of the associated Lagrangian. Conversely, if $(x^*,\\lambda^*, \\mu^*)$ is a saddle point of a Lagrangian, then $x^*$ is a primal optimal, and $(\\lambda^*, \\mu^*)$ a is dual optimal of the associated Strongly Dual problem.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "> Note: This isn't a necessary condition of Strong Duality since not all Strongly Dual problems satisfy the Sufficient Condition for Strong Duality. That is, it would be incorrect to replace \"a convex problem which satisfies Slater's Condition\" in the above theorem with \"a Strongly Dual problem.\" In fact, not all Strongly Dual problems are convex to begin with, in which case the Lagrangian has no saddle points to speak of. \n",
    "<br>\n",
    "\n",
    "First, let's show that the max-min inequality holds with strict equality at saddle points. This will yield one direction of the Saddle Point Theorem proof.\n",
    "\n",
    "\n",
    "### Max-Min Equality at Saddle Points \n",
    "\n",
    "For certain types of functions, informally speaking those that are saddle-shaped, the max-min inequality holds with strict equality. \n",
    "\n",
    "The proof is as follows.\n",
    "\n",
    "Let $f: X \\times Y \\rightarrow \\mathbb{R}$ be saddle-shaped.\n",
    "\n",
    "We call a point $(\\hat x, \\hat y) \\in X \\times Y$ a *saddle-point* of $f(x,y)$ if $f(\\hat x, y) \\leq f(\\hat x, \\hat y) \\leq f(x, \\hat y)$ for all $x \\in X$ and $y \\in Y$.\n",
    "\n",
    "In other words, $\\hat x$ minimizes $f(x, \\hat y)$ over $X$, and $\\hat y$ maximizes $f(\\hat x,y)$ over $Y$. That is\n",
    "\n",
    "$$f(\\hat x, \\hat y) = \\inf_{x \\in X} f(x, \\hat y) \\ \\ \\textrm{and} \\ \\ f(\\hat x, \\hat y) = \\sup_{y \\in Y} f(\\hat x, y)$$\n",
    "\n",
    "But then\n",
    "\n",
    "$$\\sup_{y \\in Y} \\left\\{ \\inf_{x \\in X} f(x,y) \\right\\} = \\inf_{x \\in X} f(x, \\hat y) = f(\\hat x, \\hat y) \\ \\ \\textrm{and} \\ \\ \\inf_{x \\in X} \\left\\{ \\sup_{y \\in Y} f(x,y) \\right\\} = \\sup_{y \\in Y} f(\\hat x, y) = f(\\hat x, \\hat y)$$\n",
    "\n",
    "So the order of optimization over $X$ and $Y$ does not matter. That is, we're in the strict case of the max-min inequality.\n",
    "\n",
    "$$\\sup_{y \\in Y} \\left\\{ \\inf_{x \\in X} f(x,y) \\right\\} = \\inf_{x \\in X} \\left\\{ \\sup_{y \\in Y} f(x,y) \\right\\}$$\n",
    "\n",
    "\n",
    "### Proof of Saddle Point Theorem\n",
    "\n",
    "**$\\implies$:**\n",
    "\n",
    "Suppose $x^*$ and $(\\lambda^*, \\mu^*)$ are primal and dual optimal solutions for a convex problem which satisfies Slater's Condition. Then the problem is Strongly Convex by the Sufficient Condition for Strong Duality. \n",
    "\n",
    "\n",
    "\n",
    "\\\\\\  NEED KKT CONDITIONS  ///\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9046afd",
   "metadata": {},
   "source": [
    "# The Dual of an Unconstrained Problem\n",
    "\n",
    "As mentioned briefly, in the case of certain types of unconstrained problems, the Fenchel-Legendre (FL) Transform is what gives rise to the dual. \n",
    "\n",
    "First, we define the FL Transform which is also known as a *Convex Conjugate* for reasons that will soon become apparent. \n",
    "\n",
    "<br>\n",
    "> **FL Transform / Convex Conjugate:** &nbsp; The *FL Transform* or *Convex Conjugate* of a function $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ is: \n",
    "$$f^*(y) = \\sup_x \\left\\{y^Tx - f(x)\\right\\}$$\n",
    "<br>\n",
    "\n",
    "We note some key properties of the FL Transform.\n",
    "\n",
    "## FL Transform - a Convex Operation\n",
    "\n",
    "The FL Transform $f^*$ is always convex regardless of the convexity of $f$. \n",
    "\n",
    "That's because, for a fixed $x$, $y^Tx - f(x)$ is a linear function in $y$. So, $f^*$ is a point-wise supremum of linear functions, making it convex. \n",
    "\n",
    "## The Case of Involution\n",
    "\n",
    "The double FL Transform $f^{**}$ does not always recover $f$. To see this, note that, as an FL Transform of the function $f^*$, $f^{**}$ is always convex. Therefore, $f^{**} \\ne f$ if $f$ is non-convex. \n",
    "\n",
    "But convexity alone is not enough to guarantee involution. We need an additional condition on $f$, namely that its sub-level sets must be closed, to ensure $f^{**} = f$.\n",
    "\n",
    "## Inverse Gradients\n",
    "\n",
    "If $f$ has closed sub-level sets and is convex then the gradients of $f$ and $f^*$ are inverses. That is, assuming both $f$ and $f^*$ are differentiable:\n",
    "<br>\n",
    "$$y = \\nabla f(x) \\iff x = \\nabla f^*(y)$$\n",
    "\n",
    "Let's prove the $\\implies$ direction. \n",
    "\n",
    "Suppose $y = \\nabla f(x)$. By $f$'s convexity: \n",
    "<br>\n",
    "$$f(\\hat x) \\geq f(x) + y^T(\\hat x - x) \\ \\ \\forall \\hat x$$\n",
    "\n",
    "And so:\n",
    "<br>\n",
    "$$y^T \\hat x - f(\\hat x) \\leq y^T x - f(x) \\ \\ \\forall \\hat x$$\n",
    "\n",
    "By taking supremum over $x$ and by noting that, since the sub-level sets are closed, the supremum is attained, we obtain:\n",
    "<br>\n",
    "$$f^*(y) = y^T x - f(x)$$\n",
    "\n",
    "The desired result follows by taking the gradient of both sides w.r.t. $y$. That is:\n",
    "<br>\n",
    "$$\\nabla f^*(y) = x$$\n",
    "\n",
    "The $\\impliedby$ direction is similar. We start from the assumption that $x = \\nabla f^*(y)$ and get the desired result by using the involution property $f^{**} = f$. \n",
    "\n",
    "## FL Duality\n",
    "\n",
    "As mentioned, the FL Transform has a natural role in duality.\n",
    "\n",
    "Suppose the unconstrained optimization problem is:\n",
    "<br>\n",
    "$$\\min_x : f(x) + h(Ax)$$\n",
    "\n",
    "Where $f$ and $h$ are convex functions, and $A$ is a matrix representing a bounded linear transformation. \n",
    "\n",
    "We introduce a dummy variable $y$ and form the artificial constraint $y = Ax$. The problem becomes:\n",
    "<br>\n",
    "$$\n",
    "\\begin{aligned} \n",
    "\\min_{x,y} &: f(x) + h(y) \\\\ \n",
    "s.t. &: Ax = y\n",
    "\\end{aligned}$$\n",
    "\n",
    "Forming the Lagrangian gives us:\n",
    "<br>\n",
    "$$\\mathcal{L}(x,y,z) = f(x) + h(y) + z^T(Ax - y)$$\n",
    "\n",
    "Then, the dual function is the following FL Transform:\n",
    "<br>\n",
    "$$\n",
    "\\begin{aligned}\n",
    "g(z) &= \\min_{x,y} \\mathcal{L}(x,y,z) \\\\\n",
    "&= \\min_{x,y} f(x) + h(y) + z^T(Ax - y) \\\\\n",
    "&= \\min_{x,y} (A^Tz)^Tx + f(x) - z^Ty + h(y) \\\\ \n",
    "&= \\min_x \\left\\{ (A^Tz)^Tx + f(x) \\right\\} + \\min_y \\left\\{ -z^Ty + h(y) \\right\\} \\\\\n",
    "&= \\min_x \\left\\{ -\\left((-A^Tz)^Tx - f(x)\\right) \\right\\} + \\min_y \\left\\{ -\\left(z^Ty - h(y)\\right) \\right\\} \\\\\n",
    "&= - \\max_x \\left\\{ (-A^Tz)^Tx - f(x) \\right\\} - \\max_y \\left\\{ z^Ty - h(y) \\right\\} \\\\\n",
    "&= - f^*(-A^Tz) - h^*(z)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "And, consequently, the dual problem is: \n",
    "<br> \n",
    "$$\\max_z: - f^*(-A^Tz) - h^*(z)$$\n",
    "\n",
    "Note that the dual is, indeed, an easy problem since the negative of an FL Transform is always concave regardless of the convexity of $f$ and $h$. So, the dual problem is alwayds an unconstrained maximization of a concave function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a3b0b5",
   "metadata": {},
   "source": [
    "# LINEAR PROGRAMS\n",
    "\n",
    "BEST WAY TO UNDERSTAND INTUITIVELY COMPLEMENTARY SLACKNESS AND OTHER PROPERTIES OF STRING DUALITY IN KKT CONDITIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4466e45a",
   "metadata": {},
   "source": [
    "# Weak Duality in Linear Programs\n",
    "\n",
    "Let's now focus on linear programs.\n",
    "\n",
    "Suppose the primal is an LP of the form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92070308",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{cases}\n",
    "\\min_x: c^Tx\n",
    "\\\\\n",
    "s.t.: \\begin{aligned} &Ax \\geq b\n",
    "\\\\ \n",
    "&x \\geq 0\n",
    "\\end{aligned}\n",
    "\\end{cases}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89c3662",
   "metadata": {},
   "source": [
    "By constructing the Lagrangian and going through the steps above we can show its dual is for the form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec50e9f",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{cases}\n",
    "\\max_p: b^Tp\n",
    "\\\\\n",
    "s.t.: \\begin{aligned} &A^Tp \\leq c\n",
    "\\\\ \n",
    "&p \\geq 0\n",
    "\\end{aligned}\n",
    "\\end{cases}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a116c12",
   "metadata": {},
   "source": [
    "Weak Duality states\n",
    "\n",
    "> **Weak Duality:** &nbsp; For any primal feasible $x$ and for all dual feasible $p$, $c^Tx \\geq b^Tp$.\n",
    "<br>\n",
    "\n",
    "That is, any dual feasible solution $b^Tp$ is a *lower bound* for all primal feasible solutions $c^Tx$. Conversely, any primal feasible solution $c^Tx$ is an *upper bound* for all dual feasible solutions $b^Tp$. \n",
    "\n",
    "##  Proof of Weak Duality\n",
    "\n",
    "Let $(p, x)$ be respectively dual-primal feasible. Then $c^Tx = x^Tc \\geq x^TA^Tp \\geq b^Tp$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d5a98d",
   "metadata": {},
   "source": [
    "# Strong Duality\n",
    "\n",
    "While Weak Duality is a useful result, the real strength of duality theory lies in *Strong Duality*. Strong duality is a re-statement of Von Neumann's [Minimax Theorem](https://en.wikipedia.org/wiki/Minimax_theorem) which lays out the conditions for which the max-min inequality holds with strict equality. Roughly speaking, it holds for functions that are saddle-shaped â€” convex in one variable and concave in the another.\n",
    "\n",
    "Instead of proving the Minimax Theorem in the general case, we will stay topical and prove Strong Duality for LP's. That is, the Minimax Theorem as it pertains to the special case of linear programs... \n",
    "\n",
    "> **Strong Duality:** &nbsp; If the primal is feasible and bounded with optimal $x^*$ then the dual is also feasible and bounded. Furthermore, if the dual has optimum $p^*$ then $c^Tx^* = b^Tp^*$.\n",
    "<br>\n",
    "\n",
    "To prove Strong Duality, we require *Farkas' Lemma*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6871a23",
   "metadata": {},
   "source": [
    "## Farkas' Lemma\n",
    "\n",
    "*Farkas' Lemma* belongs to the class of theorems called *Theorems of the Alternative* â€” these are a theorems stating that exactly one of two statements holds true.\n",
    "\n",
    "The lemma simply states that a given vector $c$ is either a [conic combination](https://v-poghosyan.github.io/blog/optimization/applied%20mathematics/proofs/2022/01/23/Optimization-Review-of-Linear-Algebra-and-Geometry.html#Conic-Combinations-of-$n$-Points) of $a_i$'s for some $i \\in I$, or it's separated from their cone by some hyperplane. \n",
    "\n",
    "We state Farkas' Lemma without offering proof since it has such an obvious geometric interpretation.\n",
    "\n",
    "> **Farkas' Lemma:** &nbsp; For any vector $c$ and $a_i \\ \\ (i \\in I)$ either the first or the second statement holds:  \n",
    "&nbsp;\n",
    "> * $\\exists p \\geq 0$ s.t. $c = \\sum_{i \\in I} a_ip_i$\n",
    "> * $\\exists$ vector $d$ s.t. $d^Ta_i \\geq 0 \\ \\ \\forall i \\in I$ but $d^Tc < 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd4127b",
   "metadata": {},
   "source": [
    "## Proof of Strong Duality in LP's\n",
    "\n",
    "\\\\\\ ANOTHER SUFFICIENT CONDITION FOR STRONG DUALITY IS BEING A LINEAR PROGRAM. THE PROOF BELOW IS INDEPENDENT OF PAST SUFFICIENT CONDITIONS. ///\n",
    "\n",
    "The proof is by construction. \n",
    "\n",
    "Suppose $x^*$ is a primal optimal solution. Let the set $I_{x^*} = \\{ i : a_i^Tx^* = b_i\\}$ be the set of the indices of the active constraints at $x^*$. Our goal is to construct a dual optimal solution $p^*$ s.t. $c^Tx^* = b^Tp^*$. \n",
    "\n",
    "Let $d$ be any vector that satisfies $d^Ta_i \\geq 0 \\ \\ \\forall i \\in I_{x^*}$. That is, $d$ is a feasible direction w.r.t. to all the active constraints.\n",
    "\n",
    "A small, positive $\\epsilon$-step in the direction of $d$ results in point $x^* + \\epsilon d$ that's still feasible. The fact that the step is small is what guarantees no inactive constraints are violated.\n",
    "\n",
    "Let's compare the value of the objective at $x^* + \\epsilon d$ to the value of the objective at $x^*$.\n",
    "\n",
    "By the assumption that $x^*$ is optimal, we have $c^Tx^* \\leq c^T(x^* + \\epsilon d) = c^Tx^* + \\epsilon c^Td$. Thus, $c^Td = d^Tc \\geq 0$\n",
    "\n",
    "> Note: $d^Tc$ is nothing but the *directional derivative* at the minimizer $x^*$. It is a *first-order necessary-condition* that the *directional derivative* in any feasible direction $d$ be non-negative at any minimizer $x^*$. This is analogous to the first-derivative test for scalar-valued functions. So, this result should have been expected...\n",
    "<br>\n",
    "\n",
    "But since $d$ is a vector s.t. $d^Ta_i \\geq 0 \\ \\ \\forall i \\in I_{x^*}$ and $d^Tc \\geq 0$, then $d$ does *not* separate $c$ from the cone of the $a_i$'s. And since $d$ was arbitrary, this puts us in the setting of Farkas' Lemma. Namely, there exist *no* vectors $d$ that separate $c$ from the cone. This means the second statement in Farkas' Lemma is violated and the first must be true â€” $c$ must a conic combination of the $a_i$'s that are active at the minimizer. In other words, $\\exists p \\geq 0$ s.t. $c = \\sum_{i \\in I_{x^*}} p_ia_i$. \n",
    "\n",
    "> Note: $c = \\sum_{i \\in I_{x^*}} p_ia_i$ should remind us of the Lagrange optimality condition in the general case of convex optimization. Recall that the Lagrange condition states that a point $x^*$ is optimal for a convex problem with objective $f(x)$ and constraints $g_i(x)=0$ if and only if $\\exists  \\lambda_i$ for each active constraint s.t. $\\nabla f(x^*) = \\sum_i \\lambda_i \\nabla g_i(x^*)$. In fact, Farka's lemma is what underpins the Lagrange condition through the assumption that the non-linear objective $f$ and non-linear constraints $g_i$ behave linearly in a small neighborhood of $x^*$. \n",
    "<br>\n",
    "\n",
    "But $p$ has dimension equal to only the number of active constraints at $x^*$. To be a dual variable at all, it must have dimension equal to the number of all primal constraints. We extend $p$ to $p^*$ by setting all the entries that do not correspond to the active constraints at $x^*$ to be zero. \n",
    "\n",
    "That is $p^*_i = \\begin{cases} p_i \\ \\ \\textrm{if} \\ \\  i \\in I_{x^*} \\\\ 0   \\ \\ \\textrm{if} \\ \\  i \\notin I_{x^*} \\end{cases}$. \n",
    "\n",
    "Now $A^Tp^*  = \\sum_{i} p^*_ia_i = c$, so any feasibility condition in the dual, whether it be $A^Tp \\leq c$, $A^Tp \\geq c$, or $A^Tp = c$, is satisfied by $p^*$. \n",
    "\n",
    "Furthermore, the dual objective at $p^*$ agrees with the primal objective at $x^*$.\n",
    "\n",
    "$$b^Tp^* = \\sum_{i} b_ip_i^* = \\sum_{i \\in I_{x^*}} b_ip_i^* + \\sum_{i \\notin I_{x^*}} b_ip_i^* = \\sum_{i \\in I_{x^*}} a_i^Tx^*p_i^* = (\\sum_{i \\in I_{x^*}} p_ia_i^T)x^* = c^Tx^* $$\n",
    "\n",
    "However, it still remains to be shown that $p^*$ is dual optimal. \n",
    "\n",
    "Whenever the primal objective and the dual objective agree on a value, the respective solutions must be primal-dual optimal. This is simply true by Weak Duality, which states that $b^Tp \\leq c^Tx^*$ $\\forall p$. So, $c^Tx^*$ is an upper bound for any dual feasible solution. But the dual is a maximization problem, so the dual optimal must be $p^*$ s.t. $b^Tp^* = c^Tx^*$.\n",
    "\n",
    "\n",
    "NOTE THAT WE HAVE CONSTRUCTED THE DUAL OPTIMAL BY EXPLICITY SATISFYING THE KKT CONDITIONS IN THE LINEAR CASE. SO THIS IS THE PROOF THAT KKT PAIR => PRIMAL/DUAL OPTIMAL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb50fa4d",
   "metadata": {},
   "source": [
    "# Theorems of the Alternative\n",
    "\n",
    "As mentioned earlier, these are theorems that describe exclusively disjoint scenarios that together comprise the entire outcome space. Formally, these are theorems of the form $A \\implies \\neg B \\land \\neg A \\implies  B$  where $A$, and $B$ are logical statements.\n",
    "\n",
    "Note that theorems of equivalence (i.e. theorems of the form *'the following are equivalent - TFAE'*) can also be formulated as theorems of the alternative. To say that $A$ and $B$ are equivalent means $ A \\iff B$. But this breaks down as $A \\implies B \\land B \\implies A$. Letting $\\hat B = \\neg B$ we can rewrite the above as $A \\implies \\neg \\hat B \\land B \\implies A$. But, by taking the contrapositive, $B \\implies A$ becomes $\\neg A \\implies \\neg B$, which is to say $\\neg A \\implies \\hat B$. In summary, we have shown that $A \\iff B$ is equivalent to $A \\implies \\neg \\hat B \\land \\neg A \\implies \\hat B$.\n",
    "\n",
    "So, the class of theorems of the alternative is much broader than it appears and includes theorems of equivalence.\n",
    "\n",
    "## Example of a Theorem of the Alternative\n",
    "\n",
    "To see how we can prove a theorem of the alternative, it helps to state one. \n",
    "\n",
    "> **Theorem:** &nbsp; Exactly one of the following two statements most hold for a given matrix A.\n",
    "&nbsp;\n",
    "> 1. $\\exists x \\ne 0$ s.t. $Ax = 0$ and $x \\geq 0$\n",
    "> 2. $\\exists p$ s.t. $p^TA > 0$\n",
    "<br>\n",
    "\n",
    "### Using a Separation Argument\n",
    "\n",
    "At the heart of separation arguments lies this simple fact. \n",
    "\n",
    "> **Separating Hyperplane Theorem:** For any convex set $C$, if a point $\\omega \\notin C$ then there exists a hyperplane separating $\\omega$ and $C$.\n",
    "<br>\n",
    "\n",
    "Farkas' Lemma, for instance, is proved by a separation argument that uses, as its convex set, the conic combination of the $a_i$'s. The conclusion is immediate since in Farkas' Lemma the first statement plainly says that a vector belongs to the convex set, and the second statement plainly says there exists a separating hyperplane between the two. \n",
    "\n",
    "This is the pattern all separation arguments must follow. However, in general, it may take a bit of work to define the problem-specific convex set and also to show that the two statements are *really* talking about belonging to this set, and separation from it. However, once these three things are accomplished the proof is complete. \n",
    "\n",
    "Using this idea, let's give a proof of the above theorem of the alternative using a separation argument.\n",
    "\n",
    "#### Proof\n",
    "\n",
    "First order of business is to come up with a convex set. \n",
    "\n",
    "Let's take $C = \\{ z : z = Ay, \\sum_i y_i = 1, y \\geq 0 \\}$ to be the convex hull of the columns of $A$.\n",
    "\n",
    "The first statement in the theorem was that $\\exists x \\ne 0$ s.t. $Ax = 0$ and $x \\geq 0$.\n",
    "\n",
    "Since $x \\ne 0$ and $x \\geq 0$ we can scale as $x$ as $y = \\alpha x$ until $\\sum_i y_i = 1$.\n",
    "\n",
    "So, the first statement is equivalent to saying the origin belongs to the convex hull $C$ (i.e. $0 \\in C$)\n",
    "\n",
    "The second statement was that $\\exists p$ s.t. $p^TA > 0$. This is equivalent to saying that all the columns of $A$ lie to one side of the separating hyperplane introduced by $p$.\n",
    "\n",
    "But all $z \\in C$ are convex combinations of $A$'s columns. In particular since they're a convex combination they're also a conic combination, so all $z \\in C$ also lie on the same side of the hyperplane. That is $p^Tz > 0 \\ \\ \\forall z \\in C$. \n",
    "\n",
    "But, of course, $p^T0 = 0$ (not $> 0$). So, according to the second statement, the origin is separated from $C$. \n",
    "\n",
    "This concludes the proof since the two statements must be mutually exclusive. \n",
    "\n",
    "### Using Strong Duality\n",
    "\n",
    "Strong duality isn't just a tool for applied science, it has important theoretical uses. For instance, now that we've proven it we can use Strong Duality, instead of a separation argument, to prove theorems of the alternative. \n",
    "\n",
    "Since it gives us feasibility of two different constraint sets, it makes sense to use duality to prove theorems of existence. \n",
    "\n",
    "Let's take the aforementioned theorem of the alternative for example...\n",
    "\n",
    "#### Proof\n",
    "\n",
    "To prove the theorem we need to show two things. First, we need to show $1 \\implies \\neg 2$, then we need to show $\\neg 1 \\implies 2$.\n",
    "\n",
    "The $1 \\implies \\neg 2$ direction is simple. \n",
    "\n",
    "Suppose $\\exists x \\ne 0$ s.t. $Ax = 0$ and $x \\geq 0$. \n",
    "\n",
    "Then $\\forall p \\ \\ (p^TA)x = p^T(Ax) = p^T0 = 0$ (not $> 0$).\n",
    "\n",
    "We tackle the $\\neg 1 \\implies 2$ direction using duality.\n",
    "\n",
    "The strategy is to construct a linear program based on $\\neg 1$ such that the feasibility of its dual implies $2$.\n",
    "\n",
    "We can express $\\neg 1$ as '$\\forall x \\ne 0$, either $Ax \\ne 0$ or $x < 0$.' Equivalently, '$x \\ne 0 \\implies Ax \\ne 0$ or $x < 0$.' Taking the contrapositive, statement $1$ becomes '$Ax = 0$ and  $x \\geq 0 \\implies x = 0$.' \n",
    "\n",
    "So, let's form the LP "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c3e1fb",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{cases}\n",
    "\\max_x: \\textbf{1}^Tx\n",
    "\\\\\n",
    "s.t.: \\begin{aligned} &Ax = 0\n",
    "\\\\ \n",
    "&x \\geq 0\n",
    "\\end{aligned}\n",
    "\\end{cases}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2a5452",
   "metadata": {},
   "source": [
    "Note that $x = 0$ is a feasible solution to the LP. Furthermore, assuming statement $1$ guarantees that $x = 0$ is the only feasible solution. Thus, the LP is feasible and bounded. \n",
    "\n",
    "By Strong Duality, its dual must also be feasible and bounded. \n",
    "\n",
    "The dual is..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5685c7d",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{cases}\n",
    "\\min_p: \\textbf{0}^Tp\n",
    "\\\\\n",
    "s.t.: p^TA \\geq \\textbf{1}\n",
    "\\end{cases}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48256e50",
   "metadata": {},
   "source": [
    "... and since it's feasible, $\\exists p$ s.t. $p^TA \\geq 1 > 0$ which demonstrates the truth of statement $2$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a19837",
   "metadata": {},
   "source": [
    "# Complementary Slackness\n",
    "\n",
    "*Complementary Slackness* is a fundamental property that exists between any primal optimal solution and any dual optimal solution. \n",
    "\n",
    "In the preceding section on Strong Duality we constructed a dual optimal by setting those of its variables that corresponded to the inactive constraints of the primal optimal to be zero. \n",
    "\n",
    "This is true in general, for all primal-dual optimal pairs. \n",
    "\n",
    "If a primal's constraint is loose at a some primal optimal, then the corresponding variable in the dual optimal is zero, and vice versa. \n",
    "\n",
    "Formally, this can be stated as\n",
    "\n",
    "> **Complementary Slackness:** if $x$ is primal feasible and $p$ is dual feasible, then $x$ and $p$ are respectively optimal iff:\n",
    "&nbsp;\n",
    "> 1. $(b_i - \\sum_{j} a_{ij}x_j)p_i = 0 \\ \\ \\forall i$\n",
    "> 2. $(\\sum_{i} a_{ij}p_i - c_j)x_j = 0  \\ \\ \\forall j$\n",
    "<br>\n",
    "\n",
    "If we recall, in the proof of Strong Duality we constructed a dual optimal by setting those of its variables that corresponded to the primal's slack constraints to be zero. In other words, we constructed a dual optimal in such a way as to satisfy the Complementary Slackness theorem. So, the fact that this generalizes to all primal-dual optima shouldn't surprise us. \n",
    "\n",
    "However, the above does not constitute a proof of Complementary Slackness, so let's offer one.\n",
    "\n",
    "Take as a starting point the primal-dual pair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c6b65e",
   "metadata": {},
   "source": [
    "$\n",
    "\\textrm{P} \\ \\ \n",
    "\\begin{cases}\n",
    "\\min_x: c^Tx\n",
    "\\\\\n",
    "s.t.: \\begin{aligned} &Ax \\geq b\n",
    "\\\\ \n",
    "&x \\geq 0\n",
    "\\end{aligned}\n",
    "\\end{cases}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2a090d",
   "metadata": {},
   "source": [
    "$\n",
    "\\textrm{D} \\ \\ \n",
    "\\begin{cases}\n",
    "\\max_p: b^Tp\n",
    "\\\\\n",
    "s.t.: \\begin{aligned} &A^Tp \\leq c\n",
    "\\\\ \n",
    "&p \\geq 0\n",
    "\\end{aligned}\n",
    "\\end{cases}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19e7b52",
   "metadata": {},
   "source": [
    "## Proof of Complementary Slackness\n",
    "\n",
    "**Sufficiency $\\impliedby$:**\n",
    "\n",
    "Suppose both equalities hold.\n",
    "\n",
    "Summing each over all $i$'s and $j$'s respectively and adding the results we get\n",
    "\n",
    "$$\\sum_i \\left(b_i - \\sum_j a_{ij}x_j \\right)p_i + \\sum_j \\left( \\sum_i a_{ij}p_i - c_j \\right)x_j = 0$$\n",
    "\n",
    "Which simplifies to \n",
    "\n",
    "$$\\sum_i b_ip_i - \\sum_i \\sum_j a_{ij}x_jy_i + \\sum_j \\sum_i a_{i,j}y_ix_j - \\sum_j c_jx_j = 0$$\n",
    "\n",
    "Or, in matrix-vector form\n",
    "\n",
    "$$b^Tp - p^TAx + p^TAx - c^Tx = 0$$\n",
    "\n",
    "The middle two terms cancel, and we get $b^Tp = c^Tp$. \n",
    "\n",
    "By Weak Duality, $x$ and $p$ are primal-dual optimal.\n",
    "\n",
    "**Necessity $\\implies$:**\n",
    "\n",
    "Suppose $x$ and $p$ are primal-dual optimal. \n",
    "\n",
    "By Strong Duality $b^Tp = c^Tx$. \n",
    "\n",
    "In other words, $b^Tp - c^Tx = 0$. Adding and subtracting the terms canceled in the first part, we can bring the sum to the form\n",
    "\n",
    "$$b^Tp - p^TAx + p^TAx - c^Tx = 0$$\n",
    "\n",
    "Which is, once again, the same as\n",
    "\n",
    "$$\\sum_i \\left(b_i - \\sum_j a_{ij}x_j \\right)p_i + \\sum_j \\left( \\sum_i a_{ij}p_i - c_j \\right)x_j = 0$$\n",
    "\n",
    "But $p$ is dual feasible, so $p_i \\geq 0 \\ \\ \\forall i$. And since $x$ is primal feasible, $Ax \\geq b$ implies $(b_i - \\sum_j a_{ij}x_j) \\leq 0 \\ \\ \\forall i$. \n",
    "\n",
    "Similarly, $x_j \\geq 0 \\ \\ \\forall j$ and $( \\sum_i a_{ij}p_i - c_j) \\geq 0 \\ \\ \\forall j$. \n",
    "\n",
    "So the above expression is a sum of all non-positive terms that adds up to zero. This can only happen if each term is equal to zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa65bc7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abe53a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
