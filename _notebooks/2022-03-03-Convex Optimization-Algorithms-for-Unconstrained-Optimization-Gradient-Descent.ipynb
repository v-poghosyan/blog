{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98902f2a",
   "metadata": {},
   "source": [
    "# Convex Optimization - Algorithms for Unconstrained Optimization, Gradient-Descent\n",
    "\n",
    "> GD, Smoothness, Strict Convexity, Line Search\n",
    "\n",
    "- hide: true\n",
    "- toc: true\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: ['Optimization','Applied Mathematics','Proofs']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135f170a",
   "metadata": {},
   "source": [
    "# PLAN\n",
    "\n",
    "1. Unconstrained algorithms \n",
    "2. Oracle Access Model of order 1\n",
    "3. Develop GD from two perspectives - linear and quadratic\n",
    "4. Run GD on model problems x^2 and |x|\n",
    "5. Develop the notion of M-smooth and m-strongly convex based on step 4\n",
    "6. Analyze the performance of GD\n",
    "\n",
    "8. Develop Accelerated GD\n",
    "7. Develop Subgradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39109ccd",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "***Gradient descent*** is a powerful, yet incredibly simple, algorithm for unconstrained, convex optimization. We can think of it as the analog of a ***greedy algorithm*** in the setting of convex, continuous optimization. That is, it is our attempt to do best locally given limited information about the objective $f$, and limited computational power.\n",
    "\n",
    "# The Gradient Descent Algorithm\n",
    "\n",
    "As an iterative algorithm, it relies on ***initialization*** and an ***update step***.\n",
    "\n",
    "Let $x$ be the initial iterate, and let the update be given by $x^+ = x + \\eta d$ for some directional unit-vector $d$ and ***step-size*** parameter $\\eta > 0$.\n",
    "\n",
    "We base the algorithm on the assumption that the linear approximation of the objective at a the next iterate $x^+$ is a good-enough local approximation of its true value at $x^+$. That is:\n",
    "\n",
    "<br>\n",
    "$$f(x + \\eta d) \\approx f(x) + \\eta \\nabla f(x)^T d \\tag{1.1}$$\n",
    "<br>\n",
    "\n",
    "Since we wish to minimize $f$, it would be wise to insist that the objective at $x^+$ improves or, at the very least, does not worsen. This is the locally optimal choice that presents itself to us. We insist: \n",
    "\n",
    "<br>\n",
    "$$f(x + \\eta d) \\approx f(x) + \\eta \\nabla f(x)^T d \\leq f(x) \\tag{1.2}$$\n",
    "<br>\n",
    "\n",
    "Since $f(x)$ is fixed and $\\eta > 0$, this amounts to minimizing the scaled inner-product $\\nabla f(x)^Td$. So, we choose $d$ opposite and parallel to the gradient, i.e. $d = - \\frac{\\nabla f(x)}{||\\nabla f(x)||_2}$.\n",
    "\n",
    "The update step becomes:\n",
    "\n",
    "<br>\n",
    "$$x^+ = x - \\eta \\frac{\\nabla f(x)}{||\\nabla f(x)||_2}$$\n",
    "<br>\n",
    "\n",
    "By re-labeling, $\\eta$ absorbs the normalization constant obtaining the final gradient descent update step: \n",
    "\n",
    "<br>\n",
    "$$x^+ = x - \\eta \\nabla f(x) \\tag{1.3}$$\n",
    "<br>\n",
    "\n",
    "This makes intuitive sense because the negative gradient direction is the direction of steepest decrease. So, it's only natural that the update should take us in this most enticing direction given that we've specified our goal is to minimize the objective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59f97a6",
   "metadata": {},
   "source": [
    "# Important Questions\n",
    "\n",
    "Given the ease with which we came up with the algorithm, we should ask ourselves the following questions. \n",
    "\n",
    "1. Is gradient descent guaranteed to converge for all step-sizes? \n",
    "2. How should we choose a step-size that guarantees convergence? \n",
    "3. What's the rate of convergence of gradient descent? Does the rate depend on step-size? Does it depend on properties of the objective function?\n",
    "4. How should we choose a step-size that maximizes convergence rate?\n",
    "\n",
    "We will explore each of this questions shortly but, before doing that, it's worth taking a high-level look at convex optimization itself. Perhaps the most important question to ask is this: does gradient descent's convergence rate, for an optimally chosen step-size, give a taxonomy of easier-to-harder problems within the scope of convex optimization? The answer, as it turns out, is *yes*.\n",
    "\n",
    "Let's start by running gradient descent on two quintessential convex problems in $\\mathbb{R}$, $f(x) = x^2$ and $h(x) = |x|$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be9be13",
   "metadata": {},
   "source": [
    "If $f$ is $M$-smooth, then we have the tightest possible point-wise quadratic upper bound as:\n",
    "\n",
    "<br>\n",
    "$$f(y) \\leq f(x) + \\nabla f(x)^T(y-x) + \\frac{M}{2} ||y-x||_2^2 \\ \\ \\forall y$$\n",
    "<br>\n",
    "\n",
    "How does this help us choose step size in a better way?\n",
    "\n",
    "Plug the step $x^+ = x - \\eta \\nabla f(x)$ into the upper bound:\n",
    "\n",
    "<br>\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f(x^+) &\\leq f(x) + \\nabla f(x)^T(\\eta \\nabla f(x)) + \\frac{M}{2} ||\\eta \\nabla f(x)||_2^2 \\\\\n",
    "\\iff f(x^+) &\\leq f(x) +  \\eta || \\nabla f(x) ||_2^2 + \\frac{M \\eta^2}{2} ||\\nabla f(x)||_2^2\n",
    "\\end{aligned}\n",
    "$$\n",
    "<br>\n",
    "\n",
    "But the RHS is a quadratic function in $\\eta$, the step-size. So, the upper bound on the next iterate $f(x^+)$, and consequently also the next iterate itself, can be minimized w.r.t. $\\eta$. Essentially, we trust the upper bound is a good quadratic approximation and choose its minimizer as the next iterate. Just like in NM we trust the quadratic estimate, and set its minimizer as the next iterate. But here, we require no second-order information about the function.\n",
    "\n",
    "The non-NM view is this. We have a fixed $x$ and the quadratic upper bound at that $x$. Any next iterate will be smaller than the qudratic UB *at* that iterate. So we choose a step size that minimizes this UB. This guarantees $f(x^+) \\leq$ min of UB which is the tightest guarantee we can get on $f(x^+)$ with the information at hand. So we, once again, do the locally optimal thing and choose that.\n",
    "\n",
    "The best choice turns out to be $\\eta = \\frac{1}{M}$\n",
    "\n",
    "\n",
    "FOR FUNCTIONS THAT ARE M SMOOTH M WORKS FOR EVERY POINT. WHEREAS WE ARE SHIT OUT OF LUCK IF SUCH AN M DOESN'T EXIST. THEN WE DON'T HAVE THIS UB. IT IS THEN THAT WE USE EITHER NM OR QUASI-NM. \n",
    "\n",
    "||\\||"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bece59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
