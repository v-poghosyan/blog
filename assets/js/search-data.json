{
  
    
        "post0": {
            "title": "Machine Learning - Decision Trees",
            "content": "Introduction . Classification vs. Regression . We start our discussion of decision trees with a definition of classification and classifier. . Definition:&nbsp; Classification is the process of grouping data into discrete categories (i.e. class labels). . We may contrast this definition with regression which is the process of predicting a continous (i.e. real or complex-valued) output. . A common example of a classification problem is the sorting of emails into the binary categories of &#39;spam&#39; and &#39;not spam&#39;. However, the labels in a classification problem need not be binary ‚Äî they may be any discrete set. Whereas a common example of regression is learning a linear (or a non-linear) function that best fits a given dataset. . Note: The line between classification and regression is sometimes blurred. For instance, logistic regression is a regression algorithm which outputs a prediction in the continous probability range $[0,1]$. It&#8217;s commonly used with a decision rule which casts its output into discrete classes. Thus, even though it&#8217;s a regression algorithm, it can easily be converted into a classification algorithm and is often used for classification problems in practice. . This leads us to the expected definition of a classifier, which is: . Definition:&nbsp; A classifier is any algorithm that performs classification. . Decision Trees . Decision trees are one type of classifier among many. . The nodes of a decision tree correspond to the features of the dataset and its leaves correspond to the class labels. The paths in a decision tree corrspond to the conjunction of features that lead to the class labels at its leaves. . To understand this, let&#39;s look at an example of a non-binary decision tree that&#39;s nonetheless very easy to understand because of the historical context of the data it&#39;s attempting to learn. . Example: . The above decision tree has identified three features that best predict the chances of a given passanger of the Titanic to survive. These three features, in order of their effect on the accuracy of the prediction, are gender, age, and sibsp (which is the number of siblings or spouses). . As we can infer from the tree, were you a passanger on the Titanic, you would&#39;ve likely survived if you were either female or a male child (below the age of 9.5) with less than 3 siblings (a conjunction of features). . TEST . Setup . Simplifying Assumptions . In the rest of this article, for simplicity, we will assume binary input and binary output for decision trees. That is, the training set is ${S = {(x^1,y^1), ... ,(x^k, y^k) }}$ with ${x^i in {0,1 }^n}$ and ${y^i in {0,1 } forall i}$. This means that the decision tree itself is simply a binary function which also receives binary input. . The task is to learn this function. . Potential Function .",
            "url": "https://v-poghosyan.github.io/blog/machine%20learning/decision%20trees/random%20forests/2021/12/29/Machine-Learning-Decision-Trees.html",
            "relUrl": "/machine%20learning/decision%20trees/random%20forests/2021/12/29/Machine-Learning-Decision-Trees.html",
            "date": " ‚Ä¢ Dec 29, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Python for Machine Learning - Pandas",
            "content": "Importing Pandas . Pandas is a library that contains pre-written code to help wrangle with data. We can think of it as Python&#39;s equivalent of Excel. . We import Pandas into our development environment as we import any other library ‚Äî using the import command. . import pandas . It&#39;s standard to import Pandas with the shorthand pd in order to avoid typing pandas all the time. . import pandas as pd . This gives us access to a vast array of pre-built objects, functions, and methods which are detailed in the API reference. . Two Underlying Data Types . Series . The basic unit of Pandas is the pandas.Series object which, in keeping with the Excel analogy, can be thought of as a column in an Excel table. It&#39;s a one-dimensional data structure that&#39;s derived from a NumPy array. However, unlike a NumPy array, the indices of a Series object aren&#39;t limited to the integer values $0,1,...n$ ‚Äî they can also be descriptive labels. . Let&#39;s create a Series object representing the populations of the G-7 countries in units of millions. . import pandas as pd g7_pop = pd.Series([35,63,80,60,127,64,318]) . As we can see, creating a series is a matter of passing a Python list (or a Numpy array) into the Series constructor. . Indexing . Indexing a Series object is similar to indexing a Python list. For instance, let&#39;s print the first element in the above series. . g7_pop[0] . 35 . Let&#39;s now swap out the integer-based indices with descriptive labels. Each Series object has an index argument that can be overwritten. . g7_pop.index = [ &#39;Canada&#39;, &#39;France&#39;, &#39;Germany&#39;, &#39;Italy&#39;, &#39;Japan&#39;, &#39;UK&#39;, &#39;US&#39; ] . Now, we can print the first element of the series using its descriptive label. . g7_pop[&#39;Canada&#39;] . 35 . We may notice a similarity between a standard Python dictionary and a labeled Series object. Namely, indexing a series with a label and keying into a Python dictionary have similar syntax. In fact, it&#39;s possible to create a labeled Series object directly from a Python dictionary. . g7_pop = pd.Series({ &#39;Canada&#39; : 35, &#39;France&#39; : 63, &#39;Germany&#39; : 80, &#39;Italy&#39; : 60, &#39;Japan&#39; : 127, &#39;UK&#39; : 64, &#39;US&#39; : 318 }) g7_pop . Canada 35 France 63 Germany 80 Italy 60 Japan 127 UK 64 US 318 dtype: int64 . In the event of overwritting the integer-based indices, it&#39;s still possible to access the elements of a Series sequentially using the iloc method (short for &quot;integer location&quot;). To retrieve the population of Canada, we can do as follows: . g7_pop.iloc[0] . 35 . It&#39;s also possible to use a range when indexing. For instance, suppose we&#39;d like to retrieve the populations of the first three countries from g7_pop. We can simply write: . g7_pop[&#39;Canada&#39;:&#39;Germany&#39;] . Canada 35 France 63 Germany 80 dtype: int64 . Or equivalently: . g7_pop.iloc[0:3] . Canada 35 France 63 Germany 80 dtype: int64 . Since the Series object is based on a Numpy array, it also supports multi-indexing through passing a list of indices or a Boolean mask. . For instance, to print the populations of Canada and Germany at the same time, we can pass in the list [&#39;Canada&#39;,&#39;Germany&#39;] or the Boolean mask [True, False, True, False, False, False, False]. . g7_pop[[&#39;Canada&#39;,&#39;Germany&#39;]] . Canada 35 Germany 80 dtype: int64 . g7_pop[[True, False, True, False, False, False, False]] . Canada 35 Germany 80 dtype: int64 . Broadcasted and Vectorized Operations . Since it&#39;s based on a NumPy array, a Series object also supports vectorization and broadcasted operations. . As a quick reminder, vectorization is the process by which NumPy optimizes looping in Python. It stores the array internally in a contiguous block of memory and restricts its contents to only one data type. Letting Python know this data type in advance, NumPy can then skip the per-iteration type checking that Python normally does in order to speed up our code. In fact, NumPy delegates most of the operations on such optimized arrays to pre-written C code under the hood. . Broadcasting, on the other hand, is the optimized process by which NumPy performs arithmetic and Boolean operations on arrays of unequal dimensions. . For instance, suppose the projected population growth of each G-7 country is 10 mln by the year 2030. Instead of looping through the Series object and adding 10 to each row or using a list comprehension, we can simply use broadcasted addition. . g7_2030_pop = g7_pop + 10 g7_2030_pop . Canada 45 France 73 Germany 90 Italy 70 Japan 137 UK 74 US 328 dtype: int64 . Filtering . Thanks to broadcasted Boolean operations and multi-indexing with a Boolean mask, it&#39;s possible to write concise and readable filtering expressions on Series. . For instance, let&#39;s return the list of countries with a population over 70 mln. . g7_pop[g7_pop &gt;= 70] . Germany 80 Japan 127 US 318 dtype: int64 . The expression g7_pop &gt;= 70 is a broadcasted Boolean operation on the Series object g7_pop which returns a Boolean array [False, False, True, False, True, False, True]. Then g7_pop is multi-indexed using this Boolean mask. . As another example of readable filtering expressions, we can return the list of countries whose populations exceed the mean population. . g7_pop[g7_pop &gt;= g7_pop.mean()] . Japan 127 US 318 dtype: int64 . DataFrame . Each DataFrame is composed of one or more Series. Whereas a Series is analogous to a column of an Excel table, a DataFrame is analogous to the table itself. . The DataFrame constructor accepts a variety of input types, among them an ndarray and a dictionary. . If we&#39;re passing an ndarray, it becomes necessary to specify the column labels separately. Additionally, we may overwrite the integer-based indexing as we did with the Series object. . data = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) df = pd.DataFrame(data, index = [&#39;R1&#39;,&#39;R2&#39;,&#39;R3&#39;], columns = [&#39;C1&#39;, &#39;C2&#39;, &#39;C3&#39;]) df . C1 C2 C3 . R1 1 | 2 | 3 | . R2 4 | 5 | 6 | . R3 7 | 8 | 9 | . We may bypass specifying columns manually by passing in a dictionary instead: . data = { &#39;C1&#39; : [1, 2, 3], &#39;C2&#39; : [4, 5, 6], &#39;C3&#39; : [7, 8, 9] } df = pd.DataFrame(data, index = [&#39;R1&#39;,&#39;R2&#39;,&#39;R3&#39;]) df . C1 C2 C3 . R1 1 | 4 | 7 | . R2 2 | 5 | 8 | . R3 3 | 6 | 9 | . . Note: Whereas in a Series the keys of the input dictionary were the row labels, in a DataFrame they&#8217;re the column labels. . In practice we often create a DataFrame from a CSV file using the pandas.read_csv() method like so: . csv_path = &#39;file.csv&#39; #Stores the path to a CSV df = pd.read_csv(csv_path) . . Tip: We may optionally pass header = None as an argument to read_csv() after csv_path if the first row of the CSV file itself is a data point, and not a header. . Tip: Pandas also supports reading an Excel file into a DataFrame using the pandas.read_excel() method. . Common Methods . Here are the common DataFrame methods that we should keep in our toolbox. These methods give us an overview of our data, and help us clean it up. . df.head() shows, by default, the first 5 rows of the dataset. Accepts an integer argument for the number of rows to display. | df.tail() shows the last 5 rows, and also accepts an integer argument. | df.info() gives a bird&#39;s eye overview of the dataset by showing the total rows/columns, the number of non-null datapoints, and the data types. | df.describe() returns statistically significant values for each column such as, the mean, standard deviation, minimum, and maximum values. | df.shape - returns the dimension of the dataset as an $(m,n)$ tuple. | . Indexing . Let&#39;s add to the dataset of G-7 countries the columns &#39;GDP&#39; and &#39;Surface Area&#39;. . g7_df = pd.DataFrame({ &#39;Population&#39; : g7_pop, &#39;GDP&#39; : [1.7, 2.8, 3.8, 2.1, 4.6, 2.9, 1.7], &#39;Surface Area&#39; : [9.0, 0.6, 0.3, 0.3, 0.3, 0.2, 9.0] }) g7_df . Population GDP Surface Area . Canada 35 | 1.7 | 9.0 | . France 63 | 2.8 | 0.6 | . Germany 80 | 3.8 | 0.3 | . Italy 60 | 2.1 | 0.3 | . Japan 127 | 4.6 | 0.3 | . UK 64 | 2.9 | 0.2 | . US 318 | 1.7 | 9.0 | . Whereas in a Series, the primary axis of indexing are the rows, in a DataFrame the primary axis are the columns. Thus, indexing a column works as expected. . g7_df[&#39;GDP&#39;] . Canada 1.7 France 2.8 Germany 3.8 Italy 2.1 Japan 4.6 UK 2.9 US 1.7 Name: GDP, dtype: float64 . Multi-indexing also works in the familiar way: . g7_df[[&#39;Population&#39;,&#39;GDP&#39;]] . Population GDP . Canada 35 | 1.7 | . France 63 | 2.8 | . Germany 80 | 3.8 | . Italy 60 | 2.1 | . Japan 127 | 4.6 | . UK 64 | 2.9 | . US 318 | 1.7 | . If we want to index rows, however, we must use the loc or iloc methods. . For instance, say we are interested in the population, GDP, and surface area of only the first three countries. We could query the dataset like so: . g7_df.loc[&#39;Canada&#39;:&#39;Germany&#39;] . Population GDP Surface Area . Canada 35 | 1.7 | 9.0 | . France 63 | 2.8 | 0.6 | . Germany 80 | 3.8 | 0.3 | . If we&#39;re only interested in the population and GDP of the first three countries, then we could instead do the following: . g7_df[[&#39;Population&#39;, &#39;GDP&#39;]].loc[&#39;Canada&#39;:&#39;Germany&#39;] . Population GDP . Canada 35 | 1.7 | . France 63 | 2.8 | . Germany 80 | 3.8 | . Noting that loc accepts two inputs, one for the selection of rows and one for columns, we can achieve the above more concisely as follows: . g7_df.loc[&#39;Canada&#39;:&#39;Germany&#39;, [&#39;Population&#39;, &#39;GDP&#39;]] . Population GDP . Canada 35 | 1.7 | . France 63 | 2.8 | . Germany 80 | 3.8 | . Filtering . Since the loc method also accepts a Boolean mask as input, we use it to filter the DataFrame by row. For instance, suppose we want the GDP of countries with a population over 70 mln. We query the dataset as follows: . g7_df.loc[g7_df[&#39;Population&#39;] &gt;= 70, &#39;GDP&#39;] . Germany 3.8 Japan 4.6 US 1.7 Name: GDP, dtype: float64 . As additional exercise, suppose we are only interested in the population and GDP of countries that are smaller than 1.0 mln square kilometers. . g7_df.loc[g7_df[&#39;Surface Area&#39;] &lt;= 1.0, [&#39;Population&#39;,&#39;GDP&#39;]] . Population GDP . France 63 | 2.8 | . Germany 80 | 3.8 | . Italy 60 | 2.1 | . Japan 127 | 4.6 | . UK 64 | 2.9 | . Adding, Dropping, and Renaming Columns and Rows . Let&#39;s add a &#39;Languages&#39; column to g7_df. We simply follow the same syntax as adding a key to a dictionary... . languages = pd.Series({ &#39;Canada&#39; : &#39;French, English&#39;, &#39;France&#39; : &#39;French&#39;, &#39;Germany&#39; : &#39;German&#39;, &#39;Italy&#39; : &#39;Italian&#39;, &#39;Japan&#39; : &#39;Japanese&#39;, &#39;UK&#39; : &#39;English&#39;, &#39;US&#39; : &#39;English&#39; }) # Next, we add the series as a column to the DataFrame g7_df[&#39;Languages&#39;] = languages g7_df . Population GDP Surface Area Languages . Canada 35 | 1.7 | 9.0 | French, English | . France 63 | 2.8 | 0.6 | French | . Germany 80 | 3.8 | 0.3 | German | . Italy 60 | 2.1 | 0.3 | Italian | . Japan 127 | 4.6 | 0.3 | Japanese | . UK 64 | 2.9 | 0.2 | English | . US 318 | 1.7 | 9.0 | English | . We can also drop a column or a row using the drop() method. . Let&#39;s drop the newly created &#39;Languages&#39; column. To drop a coulumn, we specify a columns argument in the drop() method like so: . g7_df.drop(columns = &#39;Languages&#39;) . Population GDP Surface Area . Canada 35 | 1.7 | 9.0 | . France 63 | 2.8 | 0.6 | . Germany 80 | 3.8 | 0.3 | . Italy 60 | 2.1 | 0.3 | . Japan 127 | 4.6 | 0.3 | . UK 64 | 2.9 | 0.2 | . US 318 | 1.7 | 9.0 | . The method drop() returns a new DataFrame which does not contain the unneeded column, but the original g7_df still contains this column. To prove this, let&#39;s print it: . g7_df . Population GDP Surface Area Languages . Canada 35 | 1.7 | 9.0 | French, English | . France 63 | 2.8 | 0.6 | French | . Germany 80 | 3.8 | 0.3 | German | . Italy 60 | 2.1 | 0.3 | Italian | . Japan 127 | 4.6 | 0.3 | Japanese | . UK 64 | 2.9 | 0.2 | English | . US 318 | 1.7 | 9.0 | English | . As we can see the &#39;Languages&#39; column is still there. The solution is to specify an inplace = True argument so that the column is dropped in-place. . g7_df.drop(columns = &#39;Languages&#39;, inplace = True) g7_df . Population GDP Surface Area . Canada 35 | 1.7 | 9.0 | . France 63 | 2.8 | 0.6 | . Germany 80 | 3.8 | 0.3 | . Italy 60 | 2.1 | 0.3 | . Japan 127 | 4.6 | 0.3 | . UK 64 | 2.9 | 0.2 | . US 318 | 1.7 | 9.0 | . In order to drop rows, we specify the index argument instead. Suppose we want to remove Canada and Italy from the dataset. . g7_df.drop(index = [&#39;Canada&#39;, &#39;Italy&#39;]) . Population GDP Surface Area . France 63 | 2.8 | 0.6 | . Germany 80 | 3.8 | 0.3 | . Japan 127 | 4.6 | 0.3 | . UK 64 | 2.9 | 0.2 | . US 318 | 1.7 | 9.0 | . It is also possible to rename a column or a row using the rename() method. . Suppose we&#39;d like to rename the columns to include the units of measurement, and suppose we&#39;d also like to expand the UK and US to their full names. . g7_df.rename( columns = { &#39;Population&#39; : &#39;Population (mln)&#39;, &#39;GDP&#39; : &#39;GDP (USD)&#39;, &#39;Surface Area&#39; : &#39;Surface Area (mln sq. km)&#39; }, index = { &#39;UK&#39; : &#39;United Kingdom&#39;, &#39;US&#39; : &#39;United States&#39; } ) . Population (mln) GDP (USD) Surface Area (mln sq. km) . Canada 35 | 1.7 | 9.0 | . France 63 | 2.8 | 0.6 | . Germany 80 | 3.8 | 0.3 | . Italy 60 | 2.1 | 0.3 | . Japan 127 | 4.6 | 0.3 | . United Kingdom 64 | 2.9 | 0.2 | . United States 318 | 1.7 | 9.0 | . Manipulating Columns . Those of us familiar with Excel know the functions feature which allows users to select specific cells or combinations of cells, perform algebraic or logical operations with their contents, and store the results in new cells. The way to do that in Pandas is, once again, through broadcasted operations. . For instance, suppose we&#39;d like to add a new &#39;GDP Per Capita&#39; column to the g7_df dataset. This is simply a matter of dividing the GDP of each country by its population. . Using broadcasted division, the code is simply: . g7_df[&#39;GDP Per Capita&#39;] = g7_df[&#39;GDP&#39;] / g7_df[&#39;Population&#39;] g7_df . Population GDP Surface Area GDP Per Capita . Canada 35 | 1.7 | 9.0 | 0.048571 | . France 63 | 2.8 | 0.6 | 0.044444 | . Germany 80 | 3.8 | 0.3 | 0.047500 | . Italy 60 | 2.1 | 0.3 | 0.035000 | . Japan 127 | 4.6 | 0.3 | 0.036220 | . UK 64 | 2.9 | 0.2 | 0.045312 | . US 318 | 1.7 | 9.0 | 0.005346 | . Worked Example - Bitcoin Price Timeseries: Cleaning and Reindexing . Sometimes we may wish to use a certain column to index a DataFrame. For instance, if we&#39;re working with a dataset of Bitcoin prices, it would be wise to use the &#39;Time&#39; column as the index so that we can easily access the value of Bitcoin at a given time. . Note: Data that&#8217;s indexed by time is called a timeseries... . For this example, we will retrieve the actual daily Bitcoin price history from CoinCap API 2.0. Feel free to check out the code that fetches the data as JSON and converts it into a Pandas DataFrame in the collapsable code below. . import requests import json # Specifying request URL, payload, and headers url = &#39;https://api.coincap.io/v2/assets/bitcoin/history?interval=d1&#39; payload = {} headers = {} # Making the request and parsing it as JSON response = requests.request(&#39;GET&#39;, url, headers = headers, data = payload) json_data = json.loads(response.text)[&#39;data&#39;] # Converting the result into a DataFrame bitcoin_df = pd.json_normalize(json_data) # Cleanup bitcoin_df.rename( columns = { &#39;priceUsd&#39; : &#39;priceInUSD&#39;, &#39;time&#39; : &#39;Time&#39;, &#39;date&#39; : &#39;Date&#39; }, inplace = True ) . . The result of this is the following DataFrame: . bitcoin_df.head() . priceInUSD Time Date . 0 37480.8939504110899664 | 1610668800000 | 2021-01-15T00:00:00.000Z | . 1 36853.8623471143090244 | 1610755200000 | 2021-01-16T00:00:00.000Z | . 2 35670.6623897365179078 | 1610841600000 | 2021-01-17T00:00:00.000Z | . 3 36061.4760792230247237 | 1610928000000 | 2021-01-18T00:00:00.000Z | . 4 36868.3293610208260669 | 1611014400000 | 2021-01-19T00:00:00.000Z | . Now that we have the dataset as a cleaned-up Pandas DataFrame called bitcoin_df, we can get to work. . First order of business is to reindex the dataset based on the &#39;Time&#39; column. We can set a column as index using the set_index() method in the following way: . bitcoin_df.set_index(&#39;Time&#39;, inplace=True) bitcoin_df.index.name = None # The index column shouldn&#39;t have a name ‚Äî this removes the name &#39;Time&#39; bitcoin_df.head() . priceInUSD Date . 1610668800000 37480.8939504110899664 | 2021-01-15T00:00:00.000Z | . 1610755200000 36853.8623471143090244 | 2021-01-16T00:00:00.000Z | . 1610841600000 35670.6623897365179078 | 2021-01-17T00:00:00.000Z | . 1610928000000 36061.4760792230247237 | 2021-01-18T00:00:00.000Z | . 1611014400000 36868.3293610208260669 | 2021-01-19T00:00:00.000Z | . As we can see the column that was previously named &#39;Time&#39; now acts as index. . Next, we should convert the entries of the index from a Unix timestamp into a Python datetime for more clarity. . import datetime bitcoin_df.index = pd.to_datetime(bitcoin_df.index, unit=&#39;us&#39;) bitcoin_df.head() . priceInUSD Date . 2021-01-15 37480.8939504110899664 | 2021-01-15T00:00:00.000Z | . 2021-01-16 36853.8623471143090244 | 2021-01-16T00:00:00.000Z | . 2021-01-17 35670.6623897365179078 | 2021-01-17T00:00:00.000Z | . 2021-01-18 36061.4760792230247237 | 2021-01-18T00:00:00.000Z | . 2021-01-19 36868.3293610208260669 | 2021-01-19T00:00:00.000Z | . Now we can comfortably access the price of Bitcoin on any given day. Suppose we&#39;d like to know its price on 2021-12-28, the day of writing this post... We can simply do: . bitcoin_df.loc[&#39;2021-12-28&#39;, &#39;priceInUSD&#39;] . &#39;48995.0145281203441155&#39; .",
            "url": "https://v-poghosyan.github.io/blog/python%20for%20ml/pandas/machine%20learning/2021/12/28/Python-for-ML-Pandas.html",
            "relUrl": "/python%20for%20ml/pandas/machine%20learning/2021/12/28/Python-for-ML-Pandas.html",
            "date": " ‚Ä¢ Dec 28, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "LeetCode 11 - Container with Most Water",
            "content": "Introduction . Problem Statement . You are given an integer array height of length n. There are n vertical lines drawn such that the two endpoints of the ith line are (i, 0) and (i, height[i]). . Find two lines that together with the x-axis form a container, such that the container contains the most water. . Return the maximum amount of water a container can store. . Example: . . Input: height = [1,8,6,2,5,4,8,3,7] Output: 49 Explanation: The above vertical lines are represented by array [1,8,6,2,5,4,8,3,7]. In this case, the max area of water (blue section) the container can contain is 49. . Foreword . The brute force solution to this problem consists of checking each pair of vertical lines. Since order of any given pair does not matter, this solution has time complexity ${O({n choose 2}) = O(n^2)}$ where $n$ is the length of the height array. . The non-brute-force solution to this problem (i.e. the &#39;two pointer solution&#39;) is pretty intuitive - the difficulty lies in its proof of correctness. Therefore, I will give away the procedure in this foreword and then proceed to prove its correctness. . The Procedure . The following is the overall procedure in words: . Initialize left and right pointers at 1 and n respectively (assuming indices start at 1). | While the pointers do not intersect: . Fix the pointer whose corresponding vertical line is longer. | Advance the pointer whose corresponding vertical line is shorter towards the fixed one. | . | . The Code . #hide-output class Solution: def maxArea(self, height: List[int]) -&gt; int: i, j = 0, len(height) - 1 water = 0 while i &lt; j: water = max(water, (j - i) * min(height[i], height[j])) if height[i] &lt; height[j]: i += 1 else: j -= 1 return water . . Proof of Correctness . Optimal Substructure . The procedure is inspired by the following recurisve optimal substructure of the problem: . Let $h(i)$ denote the height of the $i$-th vertical line. . | Let $a(i,j)$ denote the area of the container formed by the pair of vertical lines $(i,j)$. . | Let $maxArea([i:j])$ denote the maximum area formed by the lines ${i,...,j}$ ‚Äì that is the output of the procedure on the subarray ${[i:j]}$. . | . Claim . The problem has optimal substructure: $$maxArea([1:n]) = max {a(1,n), maxArea([2:n]) }$$ . Proof of Claim . For the initial pair $(1,n)$ where, WLOG, ${h(1) &lt; h(n)}$ we have ${a(1,n) &gt; a(1,k) forall k}$. This is because we&#39;re starting out from the widest container formed by ${(1,n)}$ and considering containers of decreasing width formed by the pairs ${(1, n-1), (1, n-2), ..., (1,2)}$. . There are two cases: . In case ${h(k) &gt; h(1)}$ for some ${1 &lt; k leq n}$ the area of the container formed by ${(1,k)}$ is still determined by ${h(1)}$, except now it&#39;s less wide. Whereas if ${h(k) &lt; h(1)}$ the area of the container decreases not only in width but also in height. . In both cases we have ${a(1,n) &gt; a(1,k)}$ which means in general ${a(1,n) &gt; a(1, k) forall k}$. . Therefore, we may omit the first vertical line from consideration and consider the subproblem on the indices ${2,...,n}$. The overall optimal solution will then be $$maxArea([1:n]) = max {a(1,n), maxArea([2:n]) }$$ as was the claim. . Inductive Proof . As with all problems that have a recursive optimal substructure, an inductive proof of correctness is immediately what springs to mind. . Base Case . For the case when $n = 2$, ${maxArea([1:2]) = max {a(1,2), maxArea([2:2]) } = a(1,2)}$ since ${maxArea[2:2] = 0}$. This is obviously correct. . Inductive Step . Suppose for an array of length $m$, the procedure $maxArea$ is correct. We would like to show that for an array of length $m+1$ it is still correct. . Assume, WLOG, ${h(1) &lt; h(m+1)}$. Note that by the optimal substructure shown above, ${maxArea([1:m+1]) = max {a(1,m+1), maxArea([2:m+1]) }}$. In omiting the first element of the input array, the only pairs we remove from consideration are ${(1,m), (1,m-1),..., (1,2)}$ which we have already shown to be suboptimal to ${(1,m+1)}$ in the proof of the optimal substructure. And since by assumption the procedure $maxArea$ on the $m$-element subarray ${[2:m+1]}$ is correct, we are done! . Conclusion . With some problems, it is the case that figuring out why the solution works gives more insight into the problem than simply solving it based on raw intuition... .",
            "url": "https://v-poghosyan.github.io/blog/leetcode/toy%20problems/proofs/2021/12/28/LeetCode-11.html",
            "relUrl": "/leetcode/toy%20problems/proofs/2021/12/28/LeetCode-11.html",
            "date": " ‚Ä¢ Dec 28, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Thanks for dropping by! I‚Äôm Vahram, a software developer and graduate student of Computer Science @ UT Austin. . üéì Education . After graduating from community college in 2018, I transfered to UCLA where I pursued my undergraduate studies in Applied Mathematics. In 2020, having graduated from UCLA with honors, I enrolled in the M.S. in Computer Science program @ UT Austin. . üíª Work . While still at community college I got my first job as a Software Engineer Intern @ Omron Automation. There, I was part of the team that developed embedded systems for the motion controllers and control panels, as well as the team that built and maintained the company‚Äôs proprietary Visual Studio based IDE. . In the Summer of 2022 I will be joining the team over @ Capital One as a Software Engineer Intern. . üôã‚Äç‚ôÇÔ∏è Social Media . For a detailed rundown of my work history, check out my LinkedIn. . Feel free to also email me at: vahram@utexas.edu .",
          "url": "https://v-poghosyan.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://v-poghosyan.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}