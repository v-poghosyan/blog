{
  
    
        "post0": {
            "title": "Machine Learning - Decision Trees",
            "content": "Introduction . Classification vs. Regression . We start our discussion of decision trees with a definition of classification and classifier. . Definition:&nbsp; Classification is the process of grouping data into discrete categories (i.e. class labels). . We may contrast this definition with regression which is the process of predicting a continous (i.e. real or complex-valued) output. . A common example of a classification problem is the sorting of emails into the binary categories of &#39;spam&#39; and &#39;not spam&#39;. However, the labels in a classification problem need not be binary ‚Äî they may be any discrete set. Whereas a common example of regression is learning a linear (or a non-linear) function that best fits a given dataset. . Note: The line between classification and regression is sometimes blurred. For instance, logistic regression is a regression algorithm which outputs a prediction in the continous probability range $[0,1]$. It&#8217;s commonly used with a decision rule which casts its output into discrete classes. Thus, even though it&#8217;s a regression algorithm, it can easily be converted into a classification algorithm and is often used for classification problems in practice. . This leads us to the expected definition of a classifier, which is: . Definition:&nbsp; A classifier is any algorithm that performs classification. . Decision Trees . Decision trees are one type of classifier among many. . The nodes of a decision tree correspond to the features of the dataset and its leaves correspond to the class labels. The paths in a decision tree corrspond to the conjunction of features that lead to the class labels at its leaves. . To understand this, let&#39;s look at an example of a non-binary decision tree that&#39;s nonetheless very easy to understand because of the historical context of the data it&#39;s attempting to learn. . Example: . The above decision tree has identified three features that best predict the chances of a given passanger of the Titanic to survive. These three features, in order of their effect on the accuracy of the prediction, are gender, age, and sibsp (which is the number of siblings or spouses). . As we can infer from the tree, were you a passanger on the Titanic, you would&#39;ve likely survived if you were either female or a male child (below the age of 9.5) with less than 3 siblings (a conjunction of features). . Setup . Simplifying Assumptions . In the rest of this article, for simplicity, we will assume binary input and binary output for decision trees. That is, the training set is ${S = {(x^1,y^1), ... ,(x^k, y^k) }}$ with ${x^i in {0,1 }^n}$ and ${y^i in {0,1 } forall i}$. This means that the decision tree itself is simply a binary function which also receives binary input. . The task is to learn this function. . Potential Function .",
            "url": "https://v-poghosyan.github.io/blog/machine%20learning/decision%20trees/random%20forests/2021/12/29/Machine-Learning-Decision-Trees.html",
            "relUrl": "/machine%20learning/decision%20trees/random%20forests/2021/12/29/Machine-Learning-Decision-Trees.html",
            "date": " ‚Ä¢ Dec 29, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "LeetCode 11 - Container with Most Water",
            "content": "Introduction . Problem Statement . You are given an integer array height of length n. There are n vertical lines drawn such that the two endpoints of the ith line are (i, 0) and (i, height[i]). . Find two lines that together with the x-axis form a container, such that the container contains the most water. . Return the maximum amount of water a container can store. . Example: . . Input: height = [1,8,6,2,5,4,8,3,7] Output: 49 Explanation: The above vertical lines are represented by array [1,8,6,2,5,4,8,3,7]. In this case, the max area of water (blue section) the container can contain is 49. . Foreword . The brute force solution to this problem consists of checking each pair of vertical lines. Since order of any given pair does not matter, this solution has time complexity ${O({n choose 2}) = O(n^2)}$ where $n$ is the length of the height array. . The non-brute-force solution to this problem (i.e. the &#39;two pointer solution&#39;) is pretty intuitive - the difficulty lies in its proof of correctness. Therefore, I will give away the procedure in this foreword and then proceed to prove its correctness. . The Procedure . The following is the overall procedure in words: . Initialize left and right pointers at 1 and n respectively (assuming indices start at 1). | While the pointers do not intersect: . Fix the pointer whose corresponding vertical line is longer. | Advance the pointer whose corresponding vertical line is shorter towards the fixed one. | . | . The Code . #hide-output class Solution: def maxArea(self, height: List[int]) -&gt; int: i, j = 0, len(height) - 1 water = 0 while i &lt; j: water = max(water, (j - i) * min(height[i], height[j])) if height[i] &lt; height[j]: i += 1 else: j -= 1 return water . . Proof of Correctness . Optimal Substructure . The procedure is inspired by the following recurisve optimal substructure of the problem: . Let $h(i)$ denote the height of the $i$-th vertical line. . | Let $a(i,j)$ denote the area of the container formed by the pair of vertical lines $(i,j)$. . | Let $maxArea([i:j])$ denote the maximum area formed by the lines ${i,...,j}$ ‚Äì that is the output of the procedure on the subarray ${[i:j]}$. . | . Claim . The problem has optimal substructure: $$maxArea([1:n]) = max {a(1,n), maxArea([2:n]) }$$ . Proof of Claim . For the initial pair $(1,n)$ where, WLOG, ${h(1) &lt; h(n)}$ we have ${a(1,n) &gt; a(1,k) forall k}$. This is because we&#39;re starting out from the widest container formed by ${(1,n)}$ and considering containers of decreasing width formed by the pairs ${(1, n-1), (1, n-2), ..., (1,2)}$. . There are two cases: . In case ${h(k) &gt; h(1)}$ for some ${1 &lt; k leq n}$ the area of the container formed by ${(1,k)}$ is still determined by ${h(1)}$, except now it&#39;s less wide. Whereas if ${h(k) &lt; h(1)}$ the area of the container decreases not only in width but also in height. . In both cases we have ${a(1,n) &gt; a(1,k)}$ which means in general ${a(1,n) &gt; a(1, k) forall k}$. . Therefore, we may omit the first vertical line from consideration and consider the subproblem on the indices ${2,...,n}$. The overall optimal solution will then be $$maxArea([1:n]) = max {a(1,n), maxArea([2:n]) }$$ as was the claim. . Inductive Proof . As with all problems that have a recursive optimal substructure, an inductive proof of correctness is immediately what springs to mind. . Base Case . For the case when $n = 2$, ${maxArea([1:2]) = max {a(1,2), maxArea([2:2]) } = a(1,2)}$ since ${maxArea[2:2] = 0}$. This is obviously correct. . Inductive Step . Suppose for an array of length $m$, the procedure $maxArea$ is correct. We would like to show that for an array of length $m+1$ it is still correct. . Assume, WLOG, ${h(1) &lt; h(m+1)}$. Note that by the optimal substructure shown above, ${maxArea([1:m+1]) = max {a(1,m+1), maxArea([2:m+1]) }}$. In omiting the first element of the input array, the only pairs we remove from consideration are ${(1,m), (1,m-1),..., (1,2)}$ which we have already shown to be suboptimal to ${(1,m+1)}$ in the proof of the optimal substructure. And since by assumption the procedure $maxArea$ on the $m$-element subarray ${[2:m+1]}$ is correct, we are done! . Conclusion . With some problems, it is the case that figuring out why the solution works gives more insight into the problem than simply solving it based on raw intuition... .",
            "url": "https://v-poghosyan.github.io/blog/leetcode/toy%20problems/proofs/2021/12/28/LeetCode-11.html",
            "relUrl": "/leetcode/toy%20problems/proofs/2021/12/28/LeetCode-11.html",
            "date": " ‚Ä¢ Dec 28, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Thanks for dropping by! I‚Äôm Vahram, a software developer and graduate student of Computer Science @ UT Austin. . üéì Education . After graduating from community college in 2018, I transfered to UCLA where I pursued my undergraduate studies in Applied Mathematics. In 2020, having graduated from UCLA with honors, I enrolled in the M.S. in Computer Science program @ UT Austin. . üíª Work . While still at community college I got my first job as a Software Engineer Intern @ Omron Automation. There, I was part of the team that developed embedded systems for the motion controllers and control panels, as well as the team that built and maintained the company‚Äôs proprietary Visual Studio based IDE. . In the Summer of 2022 I will be joining the team over @ Capital One as a Software Engineer Intern. . üôã‚Äç‚ôÇÔ∏è Social Media . For a detailed rundown of my work history, check out my LinkedIn. . Feel free to also email me at: vahram@utexas.edu .",
          "url": "https://v-poghosyan.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://v-poghosyan.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}