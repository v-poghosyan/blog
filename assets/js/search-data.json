{
  
    
        "post0": {
            "title": "Optimization - Robust LP's - Modelling Discrete Failures",
            "content": "Model . We are faced with the task of modeling a scenario in which at most $k$ of the total $n$ workers, machines, sensors, or other system components can fail. The task is to minimize the amount of system components needed, thereby minimizing cost, subject to certain known and robust constraints. . We will assume the linear cost function to be $c^Tx$, and the known constraints to be $Ax leq b$. The robust constraint is $a_R^Tx leq b_R, forall a_R in D_k$ where $b_R$ is a known vector, and $D_k$ is the following interval uncertainty set with an additional combinatorial constraint: . $D_k = { a : a_i in [ hat a_i - delta_i, hat a_i + delta_i] wedge textrm{at least $n-k$ of the $a_i$&#39;s exactly equal $ hat a_i$} }$ . In $D_k$, we can think of each $ hat a_i$ as the spec at which the $i$-th component should operate, and the $ delta_i$&#39;s as the $i$-th component&#39;s deviation from this spec. Thus, $D_k$ models the discrete failures scenario exactly... . Formulating as an Optimization Problem . Let&#39;s attempt to formulate this problem as a robust LP. . So far we have . $ begin{cases} min: c^Tx s.t.: begin{aligned} &amp;Ax leq b &amp;a_R^Tx leq b_R forall a_r in D_k end{aligned} end{cases} $ . Which is, of course, not in standard form. . For a fixed $x$, $a_R^Tx leq b_R forall a_r implies a_{R max}^Tx leq b_R$. But since we don&#39;t know the value of $a_{R max}$, we can formulate an inner optimization problem for which it is the optimal value. . So we can state the following equivalent optimization problem . $ begin{cases} min: c^Tx s.t.: Ax leq b begin{cases} max_{a_R}: &amp;a_R^Tx s.t.: &amp;a_R in D_k end{cases} leq b_R end{cases} $ . Formulating the Inner Problem as a Linear Program . Let&#39;s focus on the inner problem . $ begin{cases} max_{a_R}: &amp;a_R^Tx s.t.: &amp;a_R in D_k end{cases} leq b_R $ . Our strategy now is to expand the constraint set $D_k$. . To that end, let&#39;s introduce slack variables $-1 leq z_i leq 1 forall i$, which represent the direction of each component&#39;s deviation from its spec. We can now rewrite the objective as: . $ begin{aligned} a_R^Tx &amp; = sum a_ix_i &amp; = sum ( hat a_i + z_i delta_i)x_i &amp; = sum hat a_ix_i + sum delta_iz_ix_i end{aligned} $ . So the optimization problem, which is now in the variables $z_i$, becomes . $ begin{cases} max_{z}: sum hat a_ix_i + sum delta_iz_ix_i s.t.: begin{aligned} &amp;-1 leq z_i leq 1 forall i &amp; textrm{at most $k$ of the $z_i ne 0$} end{aligned} end{cases} $ . We still have the combinatorial constraint &#39;at most $k$ of the $z_i ne 0$,&#39; which makes this into a mixed optimization problem... . We need to massage this problem more to bring it to standard form. . Note that we&#39;re dealing with a problem of maximization. In the objective, $ sum hat a_ix_i$ is fixed by virtue of the $ hat a_i$&#39;s being fixed by the given $D_k$ and the $x_i$&#39;s being fixed by the outer optimization problem. Note that the $ delta_i$&#39;s are also fixed by $D_k$. Therefore, what would maximize the objective is each term of $ sum delta_i z_i x_i$ contributing positively to the sum. . This happens when $z_i$ and $x_i$ have the same sign $ forall i$. That is, their product $z_ix_i$ takes values in $[0, |x_i|]$. . The remaining cases can be disposed of without changing the optimal value of the optimization problem. . Rewriting the problem, we have . $ begin{cases} max_{z}: sum hat a_ix_i + sum delta_iz_i|x_i| s.t.: begin{aligned} &amp;0 leq z_i leq 1 forall i &amp; textrm{at most $k$ of the $z_i ne 0$} end{aligned} end{cases} dagger $ . . Note: that $|x_i|$ does not make the objective non-linear because the $x_i$&#8217;s are fixed values, and not the decision variables. . Relaxing the Combinatorial Constraint with a Continuous Constraint . We will relax the combinatorial constraint &#39;at most $k$ of the $z_i ne 0$&#39; by replacing it with $ sum z_i leq k$. . Although this is a relaxation of the constraint, we will show that it makes no difference within the context of preserving the optimization problem. That is, it does not affect the optimal value of the problem. . First and foremost, it&#39;s easy to see that &#39;at most $k$ of the $z_i ne 0$&#39; $ implies$ $ sum z_i leq k$ since each $z_i in [0,1]$. . We claim the converse is true as well, given that we restrict our attention to the optimal solution to the above LP $ dagger$. That is &#39;at most $k$ of the $z_i ne 0$&#39; $ impliedby$ $ sum z_i leq k$. . This claim is true by the geometry of linear programs. An optimal solution to the LP can only occur at an extreme point, and those are defined exactly by $n$ independent active constraints. . In the above LP, $0 leq z_i leq 1 forall i$ represent a set of $2n$ independent constraints, and $ sum z_i leq k$ is just one additional constraint. . If all of the $n$ active constraints come from $0 leq z_i leq 1 forall i$, then since a given $z_i$ cannot simultaneously be $0$ and $1$ the $z_i$&#39;s of the optimal solution must take integral values (that is, either $0$ or $1$ and nothing in between). . In the general case, at least $n-1$ constraints must come from $0 leq z_i leq 1 forall i$, which implies at least $n-1$ of the $z_i$&#39;s take integral values and the remaining active constraint is $ sum z_i = k$. But $n$ numbers, of which $n-1$ are integers, cannot add up to an integer value $k$ unless the remaining number is also an integer. So, once again we have that all the $z_i$&#39;s are integral valued. . Then $ sum z_i leq k$ $ implies$ at most $k$ of the $z_i = 1$ $ implies$ &#39;at most $k$ of the $z_i ne 0$&#39; as was the claim. . This leaves us with the inner optimization problem . $ begin{cases} max_{z}: sum hat a_ix_i + sum delta_iz_i|x_i| s.t.: begin{aligned} &amp;0 leq z_i leq 1 forall i &amp; sum z_i leq k end{aligned} end{cases} $ . which is finally a linear program. . Putting the Inner and Outer Problems Together . The combined optimization problem becomes . $ begin{cases} min: c^Tx s.t.: Ax leq b begin{cases} max_{z}: sum hat a_ix_i + sum delta_iz_i|x_i| s.t.: begin{aligned} &amp;0 leq z_i leq 1 forall i &amp; sum z_i leq k end{aligned} end{cases} leq b_R end{cases} $ . This is, of course, still not a linear program. Firstly, it&#39;s a mixture between minimization and maximization. Secondly, since the variables are $x_i$, and $z_i$, the term $ sum delta_iz_ix_i$ is not linear in the decision variables. Thirdly, $|x_i|$ is not linear in $x_i$. . We can address these issues one by one... . Taking the Dual of the Inner . We can turn the inner maximization problem to an inner minimization problem by taking its dual. As we know, by LP-duality (otherwise known as strong duality) this does not affect the optimal value of the problem. . The overall problem becomes . $ begin{cases} min: c^Tx s.t.: Ax leq b begin{cases} min_{ lambda}: sum hat a_ix_i + sum lambda_i + lambda_0k s.t.: begin{aligned} &amp; lambda_0 + lambda_i geq delta_i|x_i| forall i &amp; lambda geq 0 end{aligned} end{cases} leq b_R end{cases} $ . Flattening the problems, since both are now minimization, we arrive at the following . $ begin{cases} min_{x, lambda}: c^Tx s.t.: begin{aligned} &amp;Ax leq b &amp; sum hat a_ix_i + sum lambda_i + lambda_0k leq b_R &amp; lambda_0 + lambda_i geq delta_i|x_i| forall i &amp; lambda geq 0 end{aligned} end{cases} $ . Linearizing the Absolute Value Constraints . This is almost a linear program, except for the fact that $|x_i|$&#39;s are nonlinear terms in the constraint. The last step is to split these constraints into corresponding pairs of linear constraints. . For each $i$, . $ begin{aligned} lambda_0 + lambda_i geq delta_i|x_i| &amp; implies - lambda_0 - lambda_i leq delta_ix_i leq lambda_0 + lambda_i &amp; implies begin{aligned} lambda_0 + lambda_i &amp; geq delta_ix_i &amp; textrm{and} lambda_0 + lambda_i &amp; geq - delta_ix_i end{aligned} end{aligned}$ . So, the final problem, which is a linear program in every right, is . $ begin{cases} min_{x, lambda}: c^Tx s.t.: begin{aligned} &amp;Ax leq b &amp; sum hat a_ix_i + sum lambda_i + lambda_0k leq b_R &amp; lambda_0 + lambda_i geq delta_ix_i forall i &amp; lambda_0 + lambda_i geq - delta_ix_i forall i &amp; lambda geq 0 end{aligned} end{cases} $ . Conclusion . Using the geometry of linear programs and LP duality, we were able to sidestep the complexity of a robust problem with a combinatorial constraint by formulating it as a linear program which can be solved by a number of fast algorithms such as the Simplex Algorithm or the Interior Point Method. . This shows the versatility of linear programs in addressing a variety of interesting mixed optimization problems. .",
            "url": "https://v-poghosyan.github.io/blog/optimization/combinatorics/applied%20mathematics/2022/02/09/Optimization-Robust-Linear-Programs-Modelling-Discrete-Failures.html",
            "relUrl": "/optimization/combinatorics/applied%20mathematics/2022/02/09/Optimization-Robust-Linear-Programs-Modelling-Discrete-Failures.html",
            "date": " ‚Ä¢ Feb 9, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Optimization - Review of Linear Algebra and Geometry",
            "content": "Introduction . The study of optimization can be summed up as the attempt to find those parameter(s) that optimize some objective function, if such exist. The objective function can be almost anything ‚Äî cost, profit, nodes in a wireless network, distance to a destination, similarity to a target image, etc. If the objective function describes cost we may wish to minimize it. If, on the other hand, it describes profit then a natural goal would be to maximize it. . The problems of minimization and maximization, summed up as optimization in one word, are the same problem up to a reflection with respect to the domain of the parameter(s). . Formally, let the objective function be $f: mathbb{R^n} to mathbb{R}$, and let it have minimizer $x^* in mathbb{R^n}$. Then, by definition of minimizer, $f(x^*) leq f(x) forall x in mathbb{R^n}$. It follows that $-f(x^*) geq -f(x) forall x in mathbb{R^n}$, so $x^*$ is the maximizer for $-f$. . Model of a Convex Optimization Problem . This series of posts will cover the ways in which we can solve an optimization problem of the form . $ textrm{minimize}: f(x) textrm{subject to}: x in mathcal{X} $ . where the objective function $f$ is a convex function, and the constraint set $ mathcal{X}$ is a convex set. Importantly, we will not cover the ways in which we can model a real-world problem as a convex optimization problem of the above form. . Why Convex Optimization? . First, let&#39;s define the size of an optimization problem as the dimensionality of the parameter $x$ added to the number of the problem constraints. . Convex optimization problems are a class of easy optimization problems ‚Äî problems whose time and/or space complexity grows slowly with respect to problem size. . These problems are general enough to capture many scenarios of interest, even some that do not fall strictly into the convex category, but specific enough to be solvable through generic algorithms and numerical methods. . Review of Linear Algebra and Geometry . We start our exploration of convex optimization with a refresher on convexity and the linear algebra that&#39;s in common use in the subject. . Convexity . Set convexity is defined as follows: . Definition:&nbsp; A set $C subseteq mathbb{R^d}$ is convex if, for all points $x_1,x_2 in C$ and any $ theta in [0,1]$, the point $ theta x_1 + (1- theta) x_2$ (i.e. the parametrized line segment between $x_1$ and $x_2$) is also in $C$. . Some Operations that Preserve Convexity . Shifting, scaling, and rotation (i.e. affine transformations) preserve convexity. Let the matrix $A$ define such a transformation, and $b$ be a shift vector. Then $C&#39; = {Ax + b | x in C }$ is convex provided that $C$ was convex. . An intersection of convex sets is also convex. That is, $C&#39; = { x | x in C_1 cap x in C_2 }$ is convex provided that $C_1$ and $C_2$ were convex to begin with. The proof follows directly from the definition of intersection... . However, unions of convex sets need not be convex... . Examples of Convex Sets . The following are some common convex sets we will come across in practice. . Convex Hull of $n$ Points . A convex combination of points $x_1, ..., x_n$ is a point of the form $x = theta_1 x_1 + ... + theta_n x_n$ where $ sum_{i = 1}^{n} theta_i = 1$ and $ theta_i geq 0 forall i$. . Let $x_1,x_2,...,x_n$ be $n$ points in space. Their convex hull is the set of all points which can be written as some convex combination of them. Equivalently, by varying the $ theta_i$&#39;s we generate the convex hull as the set of all convex combinations of these points. . The convex hull can be visualized as the closed polygon formed when a rubber band is stretched around the $n$ points. The convex hull of two points is the line segment joining them. That of three points is the polygon (complete with its inner region). In general, for $n$ points, the concept generalizes to an $n$-dimensional polygon. . Formally, the convex hull is the set $ { theta_1 x_1 + ... + theta_n x_n | theta_1 + ... + theta_n = 1 textrm{and} theta_i geq 0 forall i }$ . Note: A handy interactive visualization, along with an efficient algorithm that generates a convex hull of $n$ points on a 2D plane can be found in the following blog post by Joel Gibson. . Convex Hull of a Set . The convex hull of a set can be similarly defined as all the convex combinations of the elements in the set. However, since the set may contain infinite elements, there&#39;s a more helpful, equivalent definition... . Let $C$ be a non-convex set. The convex hull of $C$ is the intersection of all convex supersets of $C$. That is, it&#39;s the intersection of all convex sets containing $C$. The result of such an intersection will be the smallest convex superset of $C$. . In fact, this minimal convex superset is unique 1 and can therefore be taken as yet another, equivalent, definition for the convex hull of a set. . Visualizing the convex hull of a non-convex set is similar to visualizing that of $n$ points ‚Äî simply imagine the shape enclosed by a rubber band stretched around the non-convex set. . Affine Combination of $n$ Points . An affine combination of points $x_1,...,x_n$ is a point of the form $x = theta_1 x_1 + ... + theta_n x_n$ with $ sum_{i=1}^{n} theta_i = 1$ but where the $ theta_i$&#39;s need not be non-negative. . For two points, the set of all affine combinations is the line that passes through them, whereas for three points it&#39;s the plane. in general, it is the plane in $(n-1)$-dimensions passing through the $n$ points. . Linear Combinations - Hyperplanes and Halfspaces . A linear combination of $n$ vectors is all vectors of the form $x = theta_1 x_1 + ... + theta_n x_n$ with the $ theta_i$&#39;s totally unrestricted. . The set of all linear combinations of $n$ vectors (i.e. points) is called their span. Formally, it is the set $ { theta_1 x_1 + ... + theta_n x_n | forall theta_1,..., theta_n }$. . The span of a single vector is the line passing through it. For two vectors the span is the plane passing through them and, in general, the span of $n$ vectors is a plane in $n$-dimensions that contains these vectors. . Hyperplanes . For fixed weights $ theta_i = a_i forall i$, a hyperplane is the set of all points $x in mathbb{R^n}$ whose linear combination equals a fixed constant $b in mathbb{R}$. . Formally, a hyperplane is the set $ { x | a_1 x_1 + ... a_n x_n = b } = { x | a^T x = b }$ . There&#39;s a geometric interpretation of the parameters $a in mathbb{R^n}$ and $b in mathbb{R}$. Since the dot-product between perpendicular vectors is $0$, $ { x | a^T x = 0 }$ is simply the set of all vectors perpendicular to $a$ (whose tail, as with all vectors in linear algebra, is considered to be fixed at the origin), making $a$ the normal vector to the hyperplane passing through the origin. To allow for parallel hyperplanes that are translated from the origin, the offset $b in mathbb{R}$ is introduced in the generalization $ { x | a^T x = b }$. This is now the set of all vectors whose dot-product with $a$ is constant. These vectors are not quite perpendicular to $a$, but they form a parallel hyperplane that&#39;s been shifted from the origin by a distance of $ frac{|b|}{ |a |_2}$. . Since the sum $a_1 x_1 + ... a_n x_n = b$ is fixed, the last coordinate, which we&#39;ll call $x_k$ for some $k in [1,...,n]$, is fixed by the choice of the other $n-1$ coordinates. Therefore, a hyperplane in $ mathbb{R^n}$ spans $n-1$ dimensions instead of $n$. . Halfspaces . A halfspace is either of the two sub-spaces a hyperplane partitions the whole space into. Since the dot-product between vectors which are roughly in the same direction is positive, and vice versa, the two halfspaces associated to a hyperplane $ { x | a^T x = b }$ are $ { x | a^T x geq b }$ and $ { x | a^T x leq b } $. . Conic Combinations of $n$ Points . A conic combination of $x_1,...x_n$ is a point $x = sum_{i=1}^{n} theta_i x_i$ where $ theta_i geq 0 forall i$. Note that the absence of the restriction that $ sum_{i=1}^{n} theta_i = 1$ is what distinguishes a conic combination from a convex combination. . A visual example: . . Ellipses . Recall from Euclidean geometry that ellipses are conic sections. In general we define ellipses in $n$-dimensions as the sub-level sets of quadratic forms. That is $ { x | (x-c)^T M (x-c) leq 1 }$ where $M succeq 0$ defines the stretch along each principal axis, and $c in mathbb{R^n}$ is the center. . An equivalent definition of an ellipse using the L2-norm is $ { x | |Ax - b |_2 leq 1 }$. That is, for a given $A$ and $b$ in the L2-norm definition, we can find an $M$ and $c$ in the sub-level set definition and vice versa. . Note: More generally, the ellipse is $ { x | (x-c)^T M (x-c) leq r }$. However, since the scaling factor $r$ is positive, it can simply be absorbed into $Q$ without affecting $Q$&#8217;s positive semidefiniteness. . To quickly convince ourselves in the equivalence of these definitions, we take the simple case where $b = 0$. . $$ begin{aligned} |Ax |_2 &amp;= ((Ax)^T(Ax))^{1/2} &amp;= (x^TA^TAx)^{1/2} &amp;= (x^TU D U^Tx)^{1/2} &amp;= x^TU D^{1/2} U^Tx end{aligned} $$Where the third equality is by the spectral decomposition of the real symmetric matrix $A^TA$, in which $D = diag( lambda_1,..., lambda_n)$ is the diagnonal matrix of eigenvalues and the columns of $U$ are the corresponding eigenvectors. Taking $M= UD^{1/2}U^T$, where $D^{1/2}$ is simply $D^{1/2} = diag( sqrt lambda_1,..., sqrt lambda_n)$, we have the equivalent sub-level set definition of the ellipse. . Norm Balls . Related to ellipses are Euclidean balls, which are norm balls for the choice of the L2-norm. A Euclidean ball has the form $ { x | |x |_2 leq r }$, and is clearly convex as it&#39;s a generalizations of the sphere in $n$-dimensions. . But also, a Euclidean ball is the special ellipse for the choice of $M = rI$, and $c = 0$. . In general, norm balls $ { x | |x |_p leq r }$ where $ |x |_p = (x_1^p + ... + x_n^p)^{1/p}$ are convex for any choice of $p geq 1$. . Polyhedra . Where a halfspace is a set with one linear inequality constraint, a polyhedron is a set with many, but finite, such linear inequality constraints. These constraints can be packed into a matrix $A in mathbb{R^{m times n}}$ by vector $b in mathbb{R^m}$ multiplication form, making the polyhedron into the set $ {x | Ax leq b }$. . Since polyhedra are simply intersections of halfspaces and hyperplanes, and the latter are both convex, polyhedra are also convex sets. . The Set of All Positive Semidefinite Matrices . The set of all PSD matrices $ { Q | x^TQx geq 0 forall x in mathbb{R^m} }$ is convex. We can, of course, use the definition of convexity to show this. But, a more elucidative approach would be the following remark. . Note that $Q mapsto x^TQx$ is a linear functional that maps the space of all PSD matrices to its field of scalars. This is analogous to how $a mapsto x^Ta$ is a linear functional so, just as $ { a | x^Ta geq 0 }$ is a halfspace in the space of vectors, $H_x = { Q | x^TQx geq 0 }$ for a given choice of $x in mathbb{R^m}$ is a halfspace in the space of PSD matrices. Halfspaces, as we already know, are convex and $ { Q | x^TQx geq 0 forall x in mathbb{R^m} }$ is nothing but an intersection of halfspaces for each choice of $x$. That is, $ { Q | x^TQx geq 0 forall x in mathbb{R^m} } = bigcap_x H_x$, concluding the proof of its convexity. . 1. Proof of uniqueness of the minimal, convex superset: Suppose $C_1$ and $C_2$ are both minimal, convex supersets of $C$. Any convex set $D$ that contains $C$ must clearly contain the minimal, convex superset. Hence, $C_1 subseteq C_2$ and $C_2 subseteq C_1$, which implies $C_1 = C_2$.‚Ü© .",
            "url": "https://v-poghosyan.github.io/blog/optimization/applied%20mathematics/proofs/2022/01/23/Optimization-Review-of-Linear-Algebra-and-Geometry.html",
            "relUrl": "/optimization/applied%20mathematics/proofs/2022/01/23/Optimization-Review-of-Linear-Algebra-and-Geometry.html",
            "date": " ‚Ä¢ Jan 23, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Thanks for dropping by! I‚Äôm Vahram, a software developer and graduate student of Computer Science @ UT Austin. . üéì Education . After graduating from community college in 2018, I transfered to UCLA where I pursued my undergraduate studies in Applied Mathematics. In 2020, having graduated from UCLA with honors, I enrolled in the M.S. in Computer Science program @ UT Austin. . üíª Work . While still at community college I got my first job as a Software Engineer Intern @ Omron Automation. There, I was part of the team that developed embedded systems for the motion controllers and control panels, as well as the team that built and maintained the company‚Äôs proprietary Visual Studio based IDE. . In the Summer of 2022 I will be joining the team over @ Capital One as a Software Engineer Intern. . üôã‚Äç‚ôÇÔ∏è Social Media . For a detailed rundown of my work history, check out my LinkedIn. . Feel free to also email me at: vahram@utexas.edu .",
          "url": "https://v-poghosyan.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://v-poghosyan.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}