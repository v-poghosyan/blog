{
  
    
        "post0": {
            "title": "Python for Machine Learning - NumPy",
            "content": "Importing NumPy . NumPy is a scientific computing library for Python. It&#39;s an extensive collection of pre-written code that optimizes and extends, among other things, the Python array (i.e. list) object into an n-dimensional NumPy array called ndarray. It comes with a variety of tools, such as matrix operations and common mathematical functions, that enable Python to perform complex mathematical tasks such as solve linear algebraic problems, generate pseudo-random numbers, perform Fourier analysis, etc. . We import NumPy, as we import any other library, using the import keyword (with or without a shorthand). . import numpy . Or, alternatively: . import numpy as np . Optimizations . As we&#39;ve briefly discussed in the &quot;Python for Machine Learning - Pandas&quot; post, NumPy works by delegating tasks to well-optimized C code under the hood. In this way it exploits the flexibility of Python while bypassing its speed limitations as an interpreted language and, instead, exploiting the speed advantages of a compiled language. . Scalable Memory Representation . One of the things NumPy optimizes is data storage. In contrast to Python 3.x&#39;s scalable memory representation of numeric values, such as integers, which can grow to accomodate a given number, NumPy stores numeric types in fixed-sized blocks of memory (e.g. int32 or int64). This means NumPy is able to take advantage of the low-level CPU instructions of modern processors that are designed for fixed-sized numeric types. Another advantage of fixed-sized storage is that consecutive blocks of memory can be allocated, which enables the libraries upon which NumPy relies to do extremely performant computations. This enforcement of fixed-sized data types is part of the optimization strategy NumPy uses called vectorization. . Vectorization . As already discussed in the aforementioned post, vectorization is the process by which NumPy stores an array internally in a contiguous block of memory, and restricts its contents to only one data type. Letting Python know this data type in advance, NumPy can then skip the per-iteration type checking that Python normally does as its iterating through a loop in order to speed up our code. Optimizing the array data structure in such a way enables NumPy to delegate most of the operations on such arrays to pre-written C code under the hood. In effect, this simply means that looping occurs in C instead of Python. . Broadcasting . The term broadcasting describes the process by which NumPy performs arithmetic operations on arrays of different dimensions. The process is usually as follows: the smaller array is “broadcast” across the larger array so that the two arrays have compatible dimensions. Broadcasting provides a means of vectorizing array operations. . Comparing Runtime . To demonstrate the performance optimizations of NumPy, let&#39;s compare squaring every element of a 1,000,000-element array and summing the results. . Using a Python List . First, we will use a Python list: . unoptimized_list = list(range(1000000)) . Squaring each element and summing: . import numpy as np %timeit np.sum([i**2 for i in unoptimized_list]) . 320 ms ± 2.81 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) . . Note: Even though we&#8217;re using NumPy&#8217;s sum() method, since the input we&#8217;re passing to it is a regular Python list, NumPy optimizations are not applied. . As we can see the whole thing took about 314 ms. . Using a NumPy Array . Now let&#39;s do the same with a NumPy array, which also gives us the opportunity to intruduce the syntax for defining one using a range. . optimized_array = np.arange(1000000) . Let&#39;s check the type of optimized_array to convince ourselves that it is, indeed, a NumPy ndarray. . type(optimized_array) . numpy.ndarray . Now, finally, let&#39;s square each element and sum the results: . %timeit np.sum(optimized_array**2) . 1.61 ms ± 19.7 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each) . Remarkably, the run-time was cut from 314 ms to only around 1.61ms! . NumPy Basics . Let&#39;s exlpore some of the ways in which we can represent arrays and matrices in NumPy. . Creating Arrays . We&#39;ve already seen how we can create a 1-dimensional NumPy array of consecutive integers $0,1,...,n-1$ using the arrange() method. . The standard way of creating a NumPy array is passing a Python list to the constructor array() like so: . a = np.array([1,2,3]) a . array([1, 2, 3]) . We can also create some sommon arrays, such as an array of consecutive integers, with some special methods such as arange(), which takes an integer $n$ as input and creates a sequential array from $0,...,n-1$. . np.arange(5) # array([0, 1, 2, 3, 4]) . array([0, 1, 2, 3, 4]) . Representing Matrices . Let&#39;s represent a $2 times 3$ matrix $ A = begin{bmatrix} 1 &amp; 2 &amp; 3 4 &amp; 5 &amp; 6 end{bmatrix} $ using NumPy: . A = np.array([[1,2,3], [4,5,6]]) A . array([[1, 2, 3], [4, 5, 6]]) . There are also ways to quickly create some common matrices using special methods. . For example, ones() accepts a shape pair, and creates a matrix of $1$s with of the given shape. . np.ones((2,3)) . array([[1., 1., 1.], [1., 1., 1.]]) . The method, zeros() works the same way as ones(): . np.zeros((2,3)) . array([[0., 0., 0.], [0., 0., 0.]]) . Meanwhile, identity() accepts an integer $n$ as input and creates a square $n times n$ identity matrix. . np.identity(3) . array([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]]) . Creating a matrix with identical elements in general uses the full() method which takes a shape attribute, a value attribute and on optional dtype attribute as follows: . np.full((2,3), 7, dtype = int) . array([[7, 7, 7], [7, 7, 7]]) . Indexing . Indexing a 1-dimensional NumPy array is done as expected, through the use of the trusty brackets []. Indexing an n-dimensional matrix in NumPy still uses [] but it introduces a new, improved, syntax. . Suppose we&#39;d like to access the element in the first row, and last column of A. The standard way would be: . A[0][2] . 3 . As we can see, that still works. But the recommended and, subjectively speaking, prettier way is: . A[0,2] . 3 . Of course, slicing still works as expected. . For example, let&#39;s print the entire first row of A: . A[0,:] . array([1, 2, 3]) . The entire first column: . A[:,0] . array([1, 4]) . Finally, let&#39;s print the submatrix $ begin{bmatrix} 2 &amp; 3 end{bmatrix} $: . A[0,1:] . array([2, 3]) . Properties and Methods of NumPy Arrays . A few of the useful properties and methods of ndarray are highlighted in this section. . shape - returns the shape of the matrix as an $(m,n)$ pair . A.shape # (2,3) . | . ndim - returns the dimension of a matrix as a single digit . A.ndim # 2 . . Note: The output of the ndim property should not be understood in a linear algebraic sense as the dimension of either the domain or range of the corresponding transformation, nor the dimension of either of its four fundamental subspaces. It is to only be understood in the data structure sense as the level of nestedness of the array. . | . size - returns the total number of elements in the matrix . A.size # 6 . | . dtype - returns the data type of the elements in the matrix. . A.dtype # dtype(&#39;int32&#39;) . . Note: If the ndarray does not represent a matrix, such as B = np.array([[1,2,3],[4,5]]) then dtype outputs O signifying that the entries are general Python objects. In such a case, the array loses its optimizations. . | . Statistical and Mathematical Methods . There is also a vast selection of statstical, and more generally, mathematical methods that ndarrays come with. Here are a few of the common ones: . sum() - returns the sum of all the entries . A.sum() # 21 . It also accepts an axis attribute where axis = 0 refers to the sum along the columns, and axis = 1 refers to the sum along the rows. . A.sum(axis = 0) # [5,7,9] . A.sum(axis = 1) # [6,15] . | mean() - returns the empirical mean of all the entries . A.mean() # 3.5 . | var() - returns the variance of the entries . A.var() # 2.9166666666666665 . | std() - returns the standard deviation of the entries . A.std() # 1.707825127659933 . | . Multi-Indexing, Filtering, and Broadcasted Operations . Recall from the Pandas article the ways in which we were able to multi-index and filter, and how we eliminated the need for using Python loops and list comprehensions using broadcasted operators instead. Since both a Pandas Series and a DataFrame are extensions of NumPy&#39;s ndarray, all of these apply here as well. . As a refresher on broadcasted operations, here are a few filtering examples. . Let&#39;s obtain those elements of A that are greater than 3: . A[A &gt; 3] . array([4, 5, 6]) . Now let&#39;s obtain those elements of A that are greater than the empirical mean: . A[A &gt; A.mean()] . array([4, 5, 6]) . What about those elements of A that are less than or equal to the empirical mean? . A[~(A &gt; A.mean())] . array([1, 2, 3]) . Which is equivalent to: . A[A &lt;= A.mean()] . array([1, 2, 3]) . Matrix Operations . One of NumPy&#39;s key selling points is that it makes matrix operations in Python easy. It offers simple syntax to add, multiply, transpose, invert, flatten, etc. matrices. . Addition . Addition of matrices is, by defualt, per-element (as are all NumPy opertions). There&#39;s no special syntax, it&#39;s done through the + operator. . For example: . A = np.ones((2,3)) B = np.ones((2,3)) A + B . array([[2., 2., 2.], [2., 2., 2.]]) . Multiplication . The operator * performs per-element multiplication. . A = np.full((2,3), 2, dtype = int) B = np.full((2,3), 3, dtype = int) A * B . array([[6, 6, 6], [6, 6, 6]]) . But this, as we know, isn&#39;t matrix multiplication as it&#39;s commonly defined in mathematics — the dot product of corresponding rows and columns. For instance, if we try to multiply an $m times n$ matrix with an $n times p$ matrix, NumPy will throw the following error: . A = np.full((2,3), 2, dtype = int) B = np.full((3,4), 3, dtype = int) A * B . ValueError: operands could not be broadcast together with shapes (2,3) (3,4) . This is because per-element operations require the shapes of the operands to be the same or compatible up to broadcasting. Here, since the shapes are different, NumPy attempts to broadcast one operand to match the shape of the other. But broadcasting is impossible between matrices of shapes $2 times 3$ and $3 times 4$ per the broadcasting rules. . There&#39;s a workaround that lets us use *. Since NumPy overloads the * operator, it works as it should for numpy.matrix types. . A = np.matrix([[2,2,2], [2,2,2]]) B = np.matrix([[3,3,3,3], [3,3,3,3], [3,3,3,3]]) A * B . matrix([[18, 18, 18, 18], [18, 18, 18, 18]]) . However, this is not the recommended way to do matrix multiplication in NumPy. Overloaded operators can produce convoluted code. For instance, we may have many different matrix and ndarray data structures and be unable to anticipate the result of a given * operation. . Instead, the recommended way to do matrix multiplication is through the @ operator. . When we use @ NumPy internally uses its matmul() method. So, the following are equivalent and both produce the matrix product of A and B. . A = np.full((2,3), 2, dtype = int) B = np.full((3,4), 3, dtype = int) A @ B . array([[18, 18, 18, 18], [18, 18, 18, 18]]) . A = np.full((2,3), 2, dtype = int) B = np.full((3,4), 3, dtype = int) np.matmul(A,B) . array([[18, 18, 18, 18], [18, 18, 18, 18]]) . NumPy also offers, dot() which is equivalent to matmul() for 1D and 2D matrices. So, the following is yet another way we can multiply matrices: . A = np.full((2,3), 2, dtype = int) B = np.full((3,4), 3, dtype = int) A.dot(B) . array([[18, 18, 18, 18], [18, 18, 18, 18]]) . However, matmul() is preferred over dot() because of the clarity of its name, and because the dot product has a distinct mathematical meaning separate from matrix multiplication. .",
            "url": "https://v-poghosyan.github.io/blog/python%20for%20ml/numpy/machine%20learning/2022/02/14/Python-for-ML-NumPy.html",
            "relUrl": "/python%20for%20ml/numpy/machine%20learning/2022/02/14/Python-for-ML-NumPy.html",
            "date": " • Feb 14, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Optimization - Linear Programs and LP Geometry",
            "content": "Introduction . A linear program is a convex optimization problem in $n$-dimensions with a linear objective and a polytope constraint. That is, its constraint set is an intersection of $n$-dimentsional linear inequalities (halfspaces) and linear equalities (hyperplanes). . In matrix form, it may be stated as . $ begin{cases} min_x: c^Tx s.t.: begin{aligned} &amp;A_1x leq b_1 &amp;A_2x geq b_2 &amp;A_3x = b_3 end{aligned} end{cases} $ . where $c in mathbb{R}^n$ is the cost vector of the objective function, $x in mathbb{R^n}$ is the decision variable, $A_1 in mathbb{R}^{m times n}$, $b_1 in mathbb{R}^m$ and $A_2 in mathbb{R}^{p times n}$, $b_2 in mathbb{R}^p$ together define the collection of linear inequality constraints, and $A_3 in mathbb{R}^{q times n}$ and $b_3 in mathbb{R}^q$ define the collection of linear equality constraints. . As we will shortly prove, an LP in any form such as the one above can be converted into its standard form . $ begin{cases} min_x: c^Tx s.t.: begin{aligned} &amp;Ax = b &amp;x geq 0 end{aligned} end{cases} dagger $ . Applications . Linear programs are only a small subset of convex optimization problems but they&#39;re robust enough to model many real-life scenarios. For instance, even though they are continuous optimization problems, due to their geometry — namely the fact that optimal solutions to an LP may occur only at the extreme points of the constraint set — they have a strong combinatorial flavor. This is why LP&#39;s are highly successful at modeling problems that are inherently combinatorial — problems of scheduling, finding the shortest path, modeling a discrete failures scenario, etc. . The reason LP&#39;s are of special interest in the study of optimization is due to the availability of fast algorithms that solve them. So, if a convex optimization problem happens to also be an LP, we can solve it much faster. . Feasibility and Boundedness . There are two ways in which LP&#39;s may fail to have an optimal solution, by either being infeasible or unbounded. Both of these cases are typically uninteresting in practice however they give important theoretical results. It&#39;s also useful to check whether or not a given LP is feasible and bounded before attempting to optimize. . Infeasible LP&#39;s are those LP&#39;s that have an empty constraint set, whereas unbounded LP&#39;s have open constraint sets. However, it&#39;s important to understand that an LP may have an open constraint set without being unbounded. . Consider the two examples below . $$ begin{cases} min_{x_1}: x_1 s.t.: x_1 leq 4 end{cases} tag{1} $$ $$ begin{cases} min_{x_1}: x_1 s.t.: x_1 geq 4 end{cases} tag{2} $$ The first problem is unbounded, since $x_1$ can be taken arbitrarily small. However, the second problem is bounded despite having an open constraint set. The optimal value of $(2)$ is $x_1 = 4$. . Thus, an LP is said to be unbounded not when its constraint polytope is open, but when it&#39;s feasible and has no optimal solution. . Converting to Standard Form . Given any starting point, an LP can be written in standard form $ dagger$. This is useful for standardization of the problem from an algorithmic perspective, and it&#39;s what the Simplex algorithm relies on to solve LP&#39;s. . In the most general case an LP can be stated with inequality constraints going in both directions and equality constraints as follows . $ begin{cases} min_x: c^Tx s.t.: begin{aligned} &amp;A_1x leq b_1 &amp;A_2x geq b_2 &amp;A_3x = b_3 end{aligned} end{cases} $ . But the inequality constraints can always be combined into $Ax leq b$ for $A = [A_1, A_2]$ and $b = [b_1, -b_2]^T$ and relabeled as $A_1$ and $b_1$. . So there&#39;s no qualitative difference between having two types of inequalities versus just one. Simply concatenate and relabel the matrices to get . $ begin{cases} min_x: c^Tx s.t.: begin{aligned} &amp;A_1x leq b_1 &amp;A_2x = b_2 end{aligned} end{cases} $ . The real challenge lies in converting the inequality constraints $A_1x leq b_1$ into equality constraints $A_1x = b_1$. . Introducing Slack Variables . The inequality constraint $A_1x leq b_1$ has slack. Formally, we can define vector $s geq 0$ (component-wise), that bridges the gap between $A_1x$ and $b_1$, that is s.t. $A_1x + s = b_1$. . Since this introduces new variables, we have to represent those in the objective and the equality constraints in a way that doesn&#39;t affect the optimization outcome. . The LP becomes . $ begin{cases} min_x: c^Tx + mathbf{0}^Ts s.t.: begin{aligned} &amp;A_1x + s = b_1 &amp;A_2x + 0s = b_2 &amp;s geq 0 end{aligned} end{cases} $ . This LP is equivalent to the one before. Namely, if the previous optimizer was $x^*$, the optimizer in the new LP is the concatenation $[x^*, b_1 - A_1x]$ which gives the same optimal value in the objective function. . This is almost in standard form, an LP with only equality constraints, and non-negativity constraints. However, the decision variable of this LP is the concatenation $[x,s]^T$, whereas the non-negativity applies to $s$ alone. . The next step is to decompose $x$ as $x = x^+ - x^-$ where $x^+,x^- geq 0$ respectively contain only the positive and only the negative entries of $x$. That is, $x^+$, and $x^-$ have entries $x_i^+ = max {0, x_i }$ and $x_i^- = -min {0, x_i }$. . With this substitution we get . $ begin{cases} min_x: c^Tx^+ - c^Tx^- + mathbf{0}^Ts s.t.: begin{aligned} &amp;A_1x^+ - A_1x^- + s = b_1 &amp;A_2x^+ - A_2x^- + 0s = b_2 &amp;x^+, x^-, s geq 0 end{aligned} end{cases} $ . Which is an LP in standard form $ dagger$. . Geometry of Linear Programs . Take a simple feasible, bounded LP in two dimensions that has a unique solution, draw its polygonal constraint set. Then draw the level sets of the objective function noting that the direction of steepest change (the positive or negative gradient) is perpendicular to the level sets. The conclusion is almost immediate — the unique optimal solution occurs at a vertex (i.e. an extreme point) of the polygonal constraint set. . To formalize this, we need to introduce a few definitions and prove a theorem called The Extreme Point Theorem which can be found towards the end of this post. . Extreme Points - Geometric Definitions . First, let&#39;s give a couple of geometric definitions of an extreme point. . Definition 1:&nbsp; A point $x$ is an extreme point of a polytope $P$ if it is not the convex combination of any other two points in the polytope. . That is, if $ exists y,z in P$ and $ lambda in [0,1]$ s.t. $x = lambda y + (1- lambda)z$ then $x$ is not an extreme point of $P$. . Definition 2:&nbsp; A point $x$ is an extreme point of a polytope $P$ if it is the unique optimum for some cost vector $c$. . That is, if $ exists c in mathbb{R}^n$ s.t. $c^Tx &lt; c^Ty forall y in P$ then $x$ is an extreme point. . Extreme Points - Algebraic Definition . It&#39;s useful to define an extreme point algebraically. To that end, let&#39;s define the concept of a basic feasible solution (BFS). . Suppose we have the polytope $ {x : Ax leq b, Dx = f }$. . Definition:&nbsp; An active constraint at $x$ is a constraint that&#39;s satisfied through strict equality. . That is, the $i$-th constraint is said to be active at x if $a_i^Tx = b_i$. . This can be thought of as $x$ being on the edge of the halfspace defined by $a_i^Tx leq b_i$. . We can also define the active set at $x$ as the set of all active constraints at $x$. . So, the active set at $x$ is $ mathcal{A}_x = { a_i : a_i^Tx = b_i } cup { d_i : d_i^Tx = f_i }$, where $ { d_i : d_i^Tx = f_i }$ is included for completion. . Basic Feasible Solution . We are now ready to define what it means for a point $x$ to be a basic feasible solution of a linear program. . Definition:&nbsp; The point $x$ is a basic feasible solution (BFS) of the linear program if its active set $ mathcal{A}_x$ contains exactly $n$ linearly independent vectors where $n$ is the dimension of $x$. . Let&#39;s ponder the BFS definition for a minute. . Imagine a closed polytope in 2D. Each of its vertices are defined by, at least, two intersecting lines. It&#39;s possible that a vertex is the result of the intersection of three or more lines, but deleting all but two of those lines will still retain the vertex. In other words, two linearly independent (i.e. non-parallel) constraints define an extreme point in 2D. . The BFS definition is simply a generalization of this insight to $n$-dimensions. . As we will prove shortly, BFS and extreme point are synonymous. In fact, the following are equivalent: . $x$ is an extreme point by Definition 1. | $x$ is an extreme point by Definition 2. | $x$ is a basic feasible solution. | . Matrix-Vector Formulation of Basic Feasible Solutions . Taking as our starting point an LP in standard form $ dagger$ we can characterize basic feasible solutions in matrix-vector form. . Take the standard constraint set $ Omega = { Ax = b, x geq 0 }$ and let&#39;s make a few simplifying assumptions. . $A$ is $m times n$ with $m leq n$. | $A$ is full-rank | $b geq 0$ | $A$ has form $A = [B,D]$ where $B$ is an $m times m$ full-rank matrix and $D$ is the rest of $A$. | Some of these assumptions impose restrictions on $ Omega$, whereas others are without loss of generality. . Assumption 1 is simply there to make the problem interesting. Were $n &gt; m$, the system of equalities would be over-determined and the constraint set would either be empty or contain a single point, which itself would be the optimum. It is, therefore an assumption which is not done without loss of generality. . Assumption 2 is equivalent to saying $rank(A) = m$. That is to say, all $m$ rows of $A$, as well as $m$ of the $n$ columns of $A$, are linearly independent. This assumption is also not done without loss of generality. Having less linearly independent rows corresponds to having less non-redundant constraints which clearly affects the constraint set $ Omega$. . Assumption 3 is done W.L.O.G. since the signs of $A$&#39;s row entries can always be flipped. . Assumption 4 is also done W.L.O.G. because if $A$ contains a full-rank $m times m$ submatrix per Assumption 2, then $A = [B,D]$ is a re-ordering of $A$ which adds no further restrictions on $ Omega$. . For $ Omega$ that satisfies Assumptions 1-4, the basic feasible solutions can be reformulated as follows. . Definition:&nbsp; Let $x_B$ be such that. $Bx_B = b$. Then the concatenation $x = [x_B, 0]^T$ is a solution to $Ax = b$. Such solutions are called feasible solutions. Furthermore, if $x_B geq 0$, such solutions are called basic ferasible solutions. . Note that, for the case we&#39;re in, this is consistent with the earlier definition of a BFS. . Let $x$ be a BFS according to this definition. $Ax = b$ poses a set of $m$ linearly independent constraints since $rank(A) = m$, whereas $x geq 0$ poses a set of $n$. But $x = [x_B, 0]^T$ is a vector at which all $m$ of the equality constraints $Ax = b$ are active, and $n-m$ of the inequality constraints $x geq 0$ are also active. So in total $n$ linearly independent constraints are active at a BFS, which is consistent with the earlier definition. . Basic Feasible Solutions and Extreme Points are Equivalent . To formally prove that basic feasible solutions are extreme points in the geometric sense, consider the following theorem and its proof. . Theorem:&nbsp; The point $x$ is an extreme point of $ Omega = { Ax =b, x geq 0 }$ if and only if it is a basic feasible solution. . Proof . Sufficiency: $ implies$ . Let $x$ be an extreme point of $ Omega$. Since it&#39;s in $ Omega$, $x geq 0$ and $Ax = b$. . Equivalently, $ sum_{i=1}^n x_ia_i = b$ where the $a_i$&#39;s are the column vectors of $A$. . Note that $x$ must contain zero entries, since it takes $n$ linearly independent active constraints to be an extreme point and only $m$ come from the equality constraints $Ax = b$. . By Assumption 1, $m$ of $A$&#39;s columns are linearly independent. We&#39;d like to claim that these $m$ are exactly those corresponding to the non-zero $x_i$ entries. . If this claim turns out to be true, then the full-rank $m times m$ submatrix $B$ will contain exactly those $m$ columns. And $x = [x_B, 0]^T$, where $x_B$ are the non-zero entries of $x$, would be a BFS. $ ast$ . So, let&#39;s prove the linear independence claim using a contradiction argument. . Without loss of generality, through rearrangement, let the first $m$ elements be the nonzero entries. That is, $x_1, ..., x_m &gt; 0$, and $x_{m+1}, ... ,x_n = 0$. . Then $ sum_{i=1}^n x_ia_i = sum_{i=1}^m x_ia_i = b$. . Towards contradiction, assume $a_1, ..., a_m$ are linearly dependent. Then $ exists y_1, ..., y_m in mathbb{R}$ not all zero s.t. $y_1a_1 + ... + y_ma_m = 0$ . Take $ epsilon &gt; 0$ to be very small. Small enough so that $x_i pm epsilon y_i &gt; 0 forall i = 1,...,m$. . Define two points as . $z^1 = [x_1 - epsilon y_1, ..., x_p - epsilon y_p, 0, ..., 0]^T$ and, $z^2 = [x_1 + epsilon y_1, ..., x_p + epsilon y_p, 0, ..., 0]^T$. . These points clearly satisfy $z_1,z_2 geq 0$, so they they satisfy one of $ Omega$&#39;s constraints. . Furthermore, . $Az^1 = sum_{i=1}^m z^1_ia_i = sum_{i=1}^m x_ia_i - epsilon sum_{i=1}^m y_ia_i = b$ since $ sum_{i=1}^m y_ia_i = 0$. . and similarly $Az^2 = b$. . So, $z^1$, and $z^2$ are indeed in $ Omega$. . But note that $x = frac{z^1 + z^2}{2}$ is a convex combination of two points in $ Omega$, which contradicts the assumption that it&#39;s an extreme point. . Hence, $a_1,...,a_m$ must be linearly independent. This concludes the proof by $ ast$. . Necessity: $ impliedby$ . Suppose $x$ is a BFS and assume, towards contradiction, that it&#39;s not and extreme point of $ Omega$. . Then $ exists y,z in Omega$ with $y ne z$ s.t. $x = alpha y + (1- alpha) z$ for some $ alpha in (0,1)$. . But since $y,z in Omega$ they satisfy $Ay = Az = b$, so $Ay - Az = A(y - z) 0$. . That is $(y_1 - z_1)a_1 + ... + (y_m - z_m)a_m = 0$. . But since $y ne z$, not all $(y_i - z_i) = 0$. So, $a_1,..., a_m$ are linearly dependent. This contradicts the assumption that $x$ was a BFS. . The Extreme Point Theorem . Why devote so much time defining extreme points geometrically, and then again algebraically? As hinted earlier and as shall be proved shortly, optima of linear programs can only occur at extreme points. This is the reason LP&#39;s are a class of easy convex optimization problems — the search space for their optima can be reduced to a finite number of extreme points. . The Extreme Point Theorem:&nbsp; If a linear program&gt; . has a finite optimum, and | its constraint polytope has at least one extreme point, | . then there is an extreme point which is optimal. . So, if we want to solve linear programs we need only consider the extreme points. . Let&#39;s prove the theorem through induction on the dimension. . Proof . Take the following general LP and assume it has a finite optimum. Assume also that its constraint polytope has, at least, one extreme point. . $ begin{cases} min_{x}: c^Tx s.t.: x in P end{cases} $ . Assume the theorem is true for this LP with an $(n-1)$-dimensional constraint polytope. The objective is to show that it&#39;s also true for the same LP with an $n$-dimensional constraint polytope. . Let $v$ be the optimal value of the LP. . Let $Q = P cap { x : c^Tx = v }$ be the intersection of the constraint polytope with the level set of the objective function at the optimal value. . Since $Q$ is the intersection of an $n$-dimensional polytope $P$ with an additional linear constraint (a hyperplane), it is $(n-1)$-dimensional. . By the inductive hypothesis, there is an extreme point $x^* in Q$ that&#39;s optimal for the LP. . By a contradiction argument, $x^*$ is also an extreme point in $P$. . Suppose it is not an extreme point in $P$. Then by Definition 1 of extreme point, $x^*$ is a convex combination of two points in $P$. That is, $ exists y,z in P$ s.t. $ lambda y + (1- lambda)z = x^*$ for some $ lambda in [0,1]$. . But then $ lambda c^Ty + (1- lambda)c^Tz = c^Tx^* = v$, since $x^*$ is optimal. But the right hand side is a convex combination of scalars, so $c^Ty = c^Tz = v$. This means $y,z in Q$, which contradicts the fact that $x^*$ is an extreme point in $Q$. . Hence, $x^*$ must be an extreme point in $P$. . Conclusion . Linear programs are a sub-class of convex optimization problems for which the search space can be reduced to a finite set of basic feasible solutions or extreme points. This lends LP&#39;s to be solvable using a number of fast, iterative algorithms. .",
            "url": "https://v-poghosyan.github.io/blog/optimization/applied%20mathematics/proofs/2022/02/14/Optimization-Geometry-of-Linear-Programs.html",
            "relUrl": "/optimization/applied%20mathematics/proofs/2022/02/14/Optimization-Geometry-of-Linear-Programs.html",
            "date": " • Feb 14, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Optimization - Robust LP's - Modelling Discrete Failures",
            "content": "Model . We are faced with the task of modeling a scenario in which at most $k$ of the total $n$ workers, machines, sensors, or other system components can fail. The task is to minimize the amount of system components needed, thereby minimizing cost, subject to certain known and robust constraints. . We will assume the linear cost function to be $c^Tx$, and the known constraints to be $Ax leq b$. The robust constraint is $a_R^Tx leq b_R, forall a_R in D_k$ where $b_R$ is a known vector, and $D_k$ is the following interval uncertainty set with an additional combinatorial constraint: . $D_k = { a : a_i in [ hat a_i - delta_i, hat a_i + delta_i] wedge textrm{at least $n-k$ of the $a_i$&#39;s exactly equal $ hat a_i$} }$ . In $D_k$, we can think of each $ hat a_i$ as the spec at which the $i$-th component should operate, and the $ delta_i$&#39;s as the $i$-th component&#39;s deviation from this spec. Thus, $D_k$ models the discrete failures scenario exactly... . Formulating as an Optimization Problem . Let&#39;s attempt to formulate this problem as a robust LP. . So far we have . $ begin{cases} min: c^Tx s.t.: begin{aligned} &amp;Ax leq b &amp;a_R^Tx leq b_R forall a_r in D_k end{aligned} end{cases} $ . Which is, of course, not in standard form. . For a fixed $x$, $a_R^Tx leq b_R forall a_r implies a_{R max}^Tx leq b_R$. But since we don&#39;t know the value of $a_{R max}$, we can formulate an inner optimization problem for which it is the optimal value. . So we can state the following equivalent optimization problem . $ begin{cases} min: c^Tx s.t.: Ax leq b begin{cases} max_{a_R}: &amp;a_R^Tx s.t.: &amp;a_R in D_k end{cases} leq b_R end{cases} $ . Formulating the Inner Problem as a Linear Program . Let&#39;s focus on the inner problem . $ begin{cases} max_{a_R}: &amp;a_R^Tx s.t.: &amp;a_R in D_k end{cases} leq b_R $ . Our strategy now is to expand the constraint set $D_k$. . To that end, let&#39;s introduce slack variables $-1 leq z_i leq 1 forall i$, which represent the direction of each component&#39;s deviation from its spec. We can now rewrite the objective as: . $ begin{aligned} a_R^Tx &amp; = sum a_ix_i &amp; = sum ( hat a_i + z_i delta_i)x_i &amp; = sum hat a_ix_i + sum delta_iz_ix_i end{aligned} $ . So the optimization problem, which is now in the variables $z_i$, becomes . $ begin{cases} max_{z}: sum hat a_ix_i + sum delta_iz_ix_i s.t.: begin{aligned} &amp;-1 leq z_i leq 1 forall i &amp; textrm{at most $k$ of the $z_i ne 0$} end{aligned} end{cases} $ . We still have the combinatorial constraint &#39;at most $k$ of the $z_i ne 0$,&#39; which makes this into a mixed optimization problem... . We need to massage this problem more to bring it to standard form. . Note that we&#39;re dealing with a problem of maximization. In the objective, $ sum hat a_ix_i$ is fixed by virtue of the $ hat a_i$&#39;s being fixed by the given $D_k$ and the $x_i$&#39;s being fixed by the outer optimization problem. Note that the $ delta_i$&#39;s are also fixed by $D_k$. Therefore, what would maximize the objective is each term of $ sum delta_i z_i x_i$ contributing positively to the sum. . This happens when $z_i$ and $x_i$ have the same sign $ forall i$. That is, their product $z_ix_i$ takes values in $[0, |x_i|]$. . The remaining cases can be disposed of without changing the optimal value of the optimization problem. . Rewriting the problem, we have . $ begin{cases} max_{z}: sum hat a_ix_i + sum delta_iz_i|x_i| s.t.: begin{aligned} &amp;0 leq z_i leq 1 forall i &amp; textrm{at most $k$ of the $z_i ne 0$} end{aligned} end{cases} dagger $ . . Note: that $|x_i|$ does not make the objective non-linear because the $x_i$&#8217;s are fixed values, and not the decision variables. . Relaxing the Combinatorial Constraint with a Continuous Constraint . We will relax the combinatorial constraint &#39;at most $k$ of the $z_i ne 0$&#39; by replacing it with $ sum z_i leq k$. . Although this is a relaxation of the constraint, we will show that it makes no difference within the context of preserving the optimization problem. That is, it does not affect the optimal value of the problem. . First and foremost, it&#39;s easy to see that &#39;at most $k$ of the $z_i ne 0$&#39; $ implies$ $ sum z_i leq k$ since each $z_i in [0,1]$. . We claim the converse is true as well, given that we restrict our attention to the optimal solution to the above LP $ dagger$. That is &#39;at most $k$ of the $z_i ne 0$&#39; $ impliedby$ $ sum z_i leq k$. . This claim is true by the geometry of linear programs. An optimal solution to the LP can only occur at an extreme point, and those are defined exactly by $n$ independent active constraints. . In the above LP, $0 leq z_i leq 1 forall i$ represent a set of $2n$ independent constraints, and $ sum z_i leq k$ is just one additional constraint. . If all of the $n$ active constraints come from $0 leq z_i leq 1 forall i$, then since a given $z_i$ cannot simultaneously be $0$ and $1$ the $z_i$&#39;s of the optimal solution must take integral values (that is, either $0$ or $1$ and nothing in between). . In the general case, at least $n-1$ constraints must come from $0 leq z_i leq 1 forall i$, which implies at least $n-1$ of the $z_i$&#39;s take integral values and the remaining active constraint is $ sum z_i = k$. But $n$ numbers, of which $n-1$ are integers, cannot add up to an integer value $k$ unless the remaining number is also an integer. So, once again we have that all the $z_i$&#39;s are integral valued. . Then $ sum z_i leq k$ $ implies$ at most $k$ of the $z_i = 1$ $ implies$ &#39;at most $k$ of the $z_i ne 0$&#39; as was the claim. . This leaves us with the inner optimization problem . $ begin{cases} max_{z}: sum hat a_ix_i + sum delta_iz_i|x_i| s.t.: begin{aligned} &amp;0 leq z_i leq 1 forall i &amp; sum z_i leq k end{aligned} end{cases} $ . which is finally a linear program. . Putting the Inner and Outer Problems Together . The combined optimization problem becomes . $ begin{cases} min: c^Tx s.t.: Ax leq b begin{cases} max_{z}: sum hat a_ix_i + sum delta_iz_i|x_i| s.t.: begin{aligned} &amp;0 leq z_i leq 1 forall i &amp; sum z_i leq k end{aligned} end{cases} leq b_R end{cases} $ . This is, of course, still not a linear program. Firstly, it&#39;s a mixture between minimization and maximization. Secondly, since the variables are $x_i$, and $z_i$, the term $ sum delta_iz_ix_i$ is not linear in the decision variables. Thirdly, $|x_i|$ is not linear in $x_i$. . We can address these issues one by one... . Taking the Dual of the Inner . We can turn the inner maximization problem to an inner minimization problem by taking its dual. As we know, by LP-duality (otherwise known as strong duality) this does not affect the optimal value of the problem. . The overall problem becomes . $ begin{cases} min: c^Tx s.t.: Ax leq b begin{cases} min_{ lambda}: sum hat a_ix_i + sum lambda_i + lambda_0k s.t.: begin{aligned} &amp; lambda_0 + lambda_i geq delta_i|x_i| forall i &amp; lambda geq 0 end{aligned} end{cases} leq b_R end{cases} $ . Flattening the problems, since both are now minimization, we arrive at the following . $ begin{cases} min_{x, lambda}: c^Tx s.t.: begin{aligned} &amp;Ax leq b &amp; sum hat a_ix_i + sum lambda_i + lambda_0k leq b_R &amp; lambda_0 + lambda_i geq delta_i|x_i| forall i &amp; lambda geq 0 end{aligned} end{cases} $ . Linearizing the Absolute Value Constraints . This is almost a linear program, except for the fact that $|x_i|$&#39;s are nonlinear terms in the constraint. The last step is to split these constraints into corresponding pairs of linear constraints. . For each $i$, . $ begin{aligned} lambda_0 + lambda_i geq delta_i|x_i| &amp; implies - lambda_0 - lambda_i leq delta_ix_i leq lambda_0 + lambda_i &amp; implies begin{aligned} lambda_0 + lambda_i &amp; geq delta_ix_i &amp; textrm{and} lambda_0 + lambda_i &amp; geq - delta_ix_i end{aligned} end{aligned}$. . So, the final problem, which is a linear program in every right, is . $ begin{cases} min_{x, lambda}: c^Tx s.t.: begin{aligned} &amp;Ax leq b &amp; sum hat a_ix_i + sum lambda_i + lambda_0k leq b_R &amp; lambda_0 + lambda_i geq delta_ix_i forall i &amp; lambda_0 + lambda_i geq - delta_ix_i forall i &amp; lambda geq 0 end{aligned} end{cases} $ . Conclusion . Using the geometry of linear programs and LP duality, we were able to sidestep the complexity of a robust problem with a combinatorial constraint by formulating it as a linear program which can be solved by a number of fast algorithms such as the Simplex Algorithm or the Interior Point Method. . This shows the versatility of linear programs in addressing a variety of interesting mixed optimization problems. .",
            "url": "https://v-poghosyan.github.io/blog/optimization/combinatorics/applied%20mathematics/2022/02/09/Optimization-Robust-Linear-Programs-Modelling-Discrete-Failures.html",
            "relUrl": "/optimization/combinatorics/applied%20mathematics/2022/02/09/Optimization-Robust-Linear-Programs-Modelling-Discrete-Failures.html",
            "date": " • Feb 9, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Optimization - Review of Linear Algebra and Geometry",
            "content": "Introduction . The study of optimization can be summed up as the attempt to find those parameter(s) that optimize some objective function, if such exist. The objective function can be almost anything — cost, profit, nodes in a wireless network, distance to a destination, similarity to a target image, etc. If the objective function describes cost we may wish to minimize it. If, on the other hand, it describes profit then a natural goal would be to maximize it. . The problems of minimization and maximization, summed up as optimization in one word, are the same problem up to a reflection with respect to the domain of the parameter(s). . Formally, let the objective function be $f: mathbb{R^n} to mathbb{R}$, and let it have minimizer $x^* in mathbb{R^n}$. Then, by definition of minimizer, $f(x^*) leq f(x) forall x in mathbb{R^n}$. It follows that $-f(x^*) geq -f(x) forall x in mathbb{R^n}$, so $x^*$ is the maximizer for $-f$. . Model of a Convex Optimization Problem . This series of posts will cover the ways in which we can solve an optimization problem of the form . $ textrm{minimize}: f(x) textrm{subject to}: x in mathcal{X} $ . where the objective function $f$ is a convex function, and the constraint set $ mathcal{X}$ is a convex set. Importantly, we will not cover the ways in which we can model a real-world problem as a convex optimization problem of the above form. . Why Convex Optimization? . First, let&#39;s define the size of an optimization problem as the dimensionality of the parameter $x$ added to the number of the problem constraints. . Convex optimization problems are a class of easy optimization problems — problems whose time and/or space complexity grows slowly with respect to problem size. . These problems are general enough to capture many scenarios of interest, even some that do not fall strictly into the convex category, but specific enough to be solvable through generic algorithms and numerical methods. . Review of Linear Algebra and Geometry . We start our exploration of convex optimization with a refresher on convexity and the linear algebra that&#39;s in common use in the subject. . Convexity . Set convexity is defined as follows: . Definition:&nbsp; A set $C subseteq mathbb{R^d}$ is convex if, for all points $x_1,x_2 in C$ and any $ theta in [0,1]$, the point $ theta x_1 + (1- theta) x_2$ (i.e. the parametrized line segment between $x_1$ and $x_2$) is also in $C$. . Some Operations that Preserve Convexity . Shifting, scaling, and rotation (i.e. affine transformations) preserve convexity. Let the matrix $A$ define such a transformation, and $b$ be a shift vector. Then $C&#39; = {Ax + b | x in C }$ is convex provided that $C$ was convex. . An intersection of convex sets is also convex. That is, $C&#39; = { x | x in C_1 cap x in C_2 }$ is convex provided that $C_1$ and $C_2$ were convex to begin with. The proof follows directly from the definition of intersection... . However, unions of convex sets need not be convex... . Examples of Convex Sets . The following are some common convex sets we will come across in practice. . Convex Hull of $n$ Points . A convex combination of points $x_1, ..., x_n$ is a point of the form $x = theta_1 x_1 + ... + theta_n x_n$ where $ sum_{i = 1}^{n} theta_i = 1$ and $ theta_i geq 0 forall i$. . Let $x_1,x_2,...,x_n$ be $n$ points in space. Their convex hull is the set of all points which can be written as some convex combination of them. Equivalently, by varying the $ theta_i$&#39;s we generate the convex hull as the set of all convex combinations of these points. . The convex hull can be visualized as the closed polygon formed when a rubber band is stretched around the $n$ points. The convex hull of two points is the line segment joining them. That of three points is the polygon (complete with its inner region). In general, for $n$ points, the concept generalizes to an $n$-dimensional polygon. . Formally, the convex hull is the set $ { theta_1 x_1 + ... + theta_n x_n | theta_1 + ... + theta_n = 1 textrm{and} theta_i geq 0 forall i }$ . Note: A handy interactive visualization, along with an efficient algorithm that generates a convex hull of $n$ points on a 2D plane can be found in the following blog post by Joel Gibson. . Convex Hull of a Set . The convex hull of a set can be similarly defined as all the convex combinations of the elements in the set. However, since the set may contain infinite elements, there&#39;s a more helpful, equivalent definition... . Let $C$ be a non-convex set. The convex hull of $C$ is the intersection of all convex supersets of $C$. That is, it&#39;s the intersection of all convex sets containing $C$. The result of such an intersection will be the smallest convex superset of $C$. . In fact, this minimal convex superset is unique 1 and can therefore be taken as yet another, equivalent, definition for the convex hull of a set. . Visualizing the convex hull of a non-convex set is similar to visualizing that of $n$ points — simply imagine the shape enclosed by a rubber band stretched around the non-convex set. . Affine Combination of $n$ Points . An affine combination of points $x_1,...,x_n$ is a point of the form $x = theta_1 x_1 + ... + theta_n x_n$ with $ sum_{i=1}^{n} theta_i = 1$ but where the $ theta_i$&#39;s need not be non-negative. . For two points, the set of all affine combinations is the line that passes through them, whereas for three points it&#39;s the plane. in general, it is the plane in $(n-1)$-dimensions passing through the $n$ points. . Linear Combinations - Hyperplanes and Halfspaces . A linear combination of $n$ vectors is all vectors of the form $x = theta_1 x_1 + ... + theta_n x_n$ with the $ theta_i$&#39;s totally unrestricted. . The set of all linear combinations of $n$ vectors (i.e. points) is called their span. Formally, it is the set $ { theta_1 x_1 + ... + theta_n x_n | forall theta_1,..., theta_n }$. . The span of a single vector is the line passing through it. For two vectors the span is the plane passing through them and, in general, the span of $n$ vectors is a plane in $n$-dimensions that contains these vectors. . Hyperplanes . For fixed weights $ theta_i = a_i forall i$, a hyperplane is the set of all points $x in mathbb{R^n}$ whose linear combination equals a fixed constant $b in mathbb{R}$. . Formally, a hyperplane is the set $ { x | a_1 x_1 + ... a_n x_n = b } = { x | a^T x = b }$ . There&#39;s a geometric interpretation of the parameters $a in mathbb{R^n}$ and $b in mathbb{R}$. Since the dot-product between perpendicular vectors is $0$, $ { x | a^T x = 0 }$ is simply the set of all vectors perpendicular to $a$ (whose tail, as with all vectors in linear algebra, is considered to be fixed at the origin), making $a$ the normal vector to the hyperplane passing through the origin. To allow for parallel hyperplanes that are translated from the origin, the offset $b in mathbb{R}$ is introduced in the generalization $ { x | a^T x = b }$. This is now the set of all vectors whose dot-product with $a$ is constant. These vectors are not quite perpendicular to $a$, but they form a parallel hyperplane that&#39;s been shifted from the origin by a distance of $ frac{|b|}{ |a |_2}$. . Since the sum $a_1 x_1 + ... a_n x_n = b$ is fixed, the last coordinate, which we&#39;ll call $x_k$ for some $k in [1,...,n]$, is fixed by the choice of the other $n-1$ coordinates. Therefore, a hyperplane in $ mathbb{R^n}$ spans $n-1$ dimensions instead of $n$. . Halfspaces . A halfspace is either of the two sub-spaces a hyperplane partitions the whole space into. Since the dot-product between vectors which are roughly in the same direction is positive, and vice versa, the two halfspaces associated to a hyperplane $ { x | a^T x = b }$ are $ { x | a^T x geq b }$ and $ { x | a^T x leq b } $. . Conic Combinations of $n$ Points . A conic combination of $x_1,...x_n$ is a point $x = sum_{i=1}^{n} theta_i x_i$ where $ theta_i geq 0 forall i$. Note that the absence of the restriction that $ sum_{i=1}^{n} theta_i = 1$ is what distinguishes a conic combination from a convex combination. . A visual example: . . Ellipses . Recall from Euclidean geometry that ellipses are conic sections. In general we define ellipses in $n$-dimensions as the sub-level sets of quadratic forms. That is $ { x | (x-c)^T M (x-c) leq 1 }$ where $M succeq 0$ defines the stretch along each principal axis, and $c in mathbb{R^n}$ is the center. . An equivalent definition of an ellipse using the L2-norm is $ { x | |Ax - b |_2 leq 1 }$. That is, for a given $A$ and $b$ in the L2-norm definition, we can find an $M$ and $c$ in the sub-level set definition and vice versa. . Note: More generally, the ellipse is $ { x | (x-c)^T M (x-c) leq r }$. However, since the scaling factor $r$ is positive, it can simply be absorbed into $Q$ without affecting $Q$&#8217;s positive semidefiniteness. . To quickly convince ourselves in the equivalence of these definitions, we take the simple case where $b = 0$. . $$ begin{aligned} |Ax |_2 &amp;= ((Ax)^T(Ax))^{1/2} &amp;= (x^TA^TAx)^{1/2} &amp;= (x^TU D U^Tx)^{1/2} &amp;= x^TU D^{1/2} U^Tx end{aligned} $$Where the third equality is by the spectral decomposition of the real symmetric matrix $A^TA$, in which $D = diag( lambda_1,..., lambda_n)$ is the diagnonal matrix of eigenvalues and the columns of $U$ are the corresponding eigenvectors. Taking $M= UD^{1/2}U^T$, where $D^{1/2}$ is simply $D^{1/2} = diag( sqrt lambda_1,..., sqrt lambda_n)$, we have the equivalent sub-level set definition of the ellipse. . Norm Balls . Related to ellipses are Euclidean balls, which are norm balls for the choice of the L2-norm. A Euclidean ball has the form $ { x | |x |_2 leq r }$, and is clearly convex as it&#39;s a generalizations of the sphere in $n$-dimensions. . But also, a Euclidean ball is the special ellipse for the choice of $M = rI$, and $c = 0$. . In general, norm balls $ { x | |x |_p leq r }$ where $ |x |_p = (x_1^p + ... + x_n^p)^{1/p}$ are convex for any choice of $p geq 1$. . Polyhedra . Where a halfspace is a set with one linear inequality constraint, a polyhedron is a set with many, but finite, such linear inequality constraints. These constraints can be packed into a matrix $A in mathbb{R^{m times n}}$ by vector $b in mathbb{R^m}$ multiplication form, making the polyhedron into the set $ {x | Ax leq b }$. . Since polyhedra are simply intersections of halfspaces and hyperplanes, and the latter are both convex, polyhedra are also convex sets. . The Set of All Positive Semidefinite Matrices . The set of all PSD matrices $ { Q | x^TQx geq 0 forall x in mathbb{R^m} }$ is convex. We can, of course, use the definition of convexity to show this. But, a more elucidative approach would be the following remark. . Note that $Q mapsto x^TQx$ is a linear functional that maps the space of all PSD matrices to its field of scalars. This is analogous to how $a mapsto x^Ta$ is a linear functional so, just as $ { a | x^Ta geq 0 }$ is a halfspace in the space of vectors, $H_x = { Q | x^TQx geq 0 }$ for a given choice of $x in mathbb{R^m}$ is a halfspace in the space of PSD matrices. Halfspaces, as we already know, are convex and $ { Q | x^TQx geq 0 forall x in mathbb{R^m} }$ is nothing but an intersection of halfspaces for each choice of $x$. That is, $ { Q | x^TQx geq 0 forall x in mathbb{R^m} } = bigcap_x H_x$, concluding the proof of its convexity. . 1. Proof of uniqueness of the minimal, convex superset: Suppose $C_1$ and $C_2$ are both minimal, convex supersets of $C$. Any convex set $D$ that contains $C$ must clearly contain the minimal, convex superset. Hence, $C_1 subseteq C_2$ and $C_2 subseteq C_1$, which implies $C_1 = C_2$.↩ .",
            "url": "https://v-poghosyan.github.io/blog/optimization/applied%20mathematics/proofs/2022/01/23/Optimization-Review-of-Linear-Algebra-and-Geometry.html",
            "relUrl": "/optimization/applied%20mathematics/proofs/2022/01/23/Optimization-Review-of-Linear-Algebra-and-Geometry.html",
            "date": " • Jan 23, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Machine Learning - Decision Trees",
            "content": "Introduction . Classification vs. Regression . We start our discussion of decision trees with a definition of classification and classifier. . Definition:&nbsp; Classification is the process of grouping data into discrete categories (i.e. class labels). . We may contrast this definition with regression which is the process of predicting a continuous (i.e. real or complex-valued) output. . A common example of a classification problem is the sorting of emails into the binary categories of &#39;spam&#39; and &#39;not spam&#39;. However, the labels in a classification problem need not be binary — they may be any discrete set. Whereas a common example of regression is learning a linear (or a non-linear) function that best fits a given dataset. . Note: The line between classification and regression is sometimes blurred. For instance, logistic regression is a regression algorithm which outputs a prediction in the continous probability range $[0,1]$. It&#8217;s commonly used with a decision rule which casts its output into discrete classes. Thus, even though it&#8217;s a regression algorithm, it can easily be converted into a classification algorithm and is often used for classification problems in practice. . This leads us to the expected definition of a classifier, which is: . Definition:&nbsp; A classifier is any algorithm that performs classification. . Decision Trees . Decision trees are one type of powerful classifier among many. . The nodes of a decision tree correspond to the features of the dataset and its leaves correspond to the class labels. The paths in a decision tree correspond to the conjunction of features that lead to the class labels at its leaves. . To understand this, let&#39;s look at an example of a decision tree that&#39;s very easy to understand because of the historical context of the data it&#39;s attempting to learn. . Example: . # Imports import pandas as pd import numpy as np from sklearn.datasets import fetch_openml import pydotplus from sklearn.tree import DecisionTreeClassifier, export_graphviz from IPython.display import Image # Setting random state for replicability of results RS = 2022 # Fetching data as features and labels from OpenML as a Pandas dataframe X, y = fetch_openml(&quot;titanic&quot;, version = 1, as_frame = True, return_X_y = True) # Cleaning and engineering data: # Dropping unneeded columns and those with too many missing values... X.drop( columns = [ &#39;boat&#39;, &#39;body&#39;, &#39;home.dest&#39;, &#39;cabin&#39;, &#39;ticket&#39;, &#39;name&#39;, &#39;embarked&#39;, &#39;fare&#39; ], inplace = True ) # Dropping data points with any missing values and subseting labels accordingly X[&#39;labels&#39;] = y # Temporarily adding labels to data as a column X.dropna(inplace = True) # Dropping the rows with any null values y = X[&#39;labels&#39;] # Subsetting the labels X.drop(columns = &#39;labels&#39;, inplace = True) # Dropping the column of labels # Converting &#39;male&#39; and &#39;female&#39; feature values into the Boolean values 0 and 1 respectively X[&#39;sex&#39;] = X[&#39;sex&#39;].apply(lambda x: 0 if x == &#39;male&#39; else 1) # Defining a decision tree classifier clf = DecisionTreeClassifier( random_state = RS, max_depth = 3 ) # Training the decision tree classifier clf = clf.fit(X, y) # Visualizing the decision tree feature_cols = [] png = export_graphviz( clf, feature_names = X.columns, class_names = [&#39;died&#39;,&#39;survived&#39;], impurity = False, ) graph = pydotplus.graph_from_dot_data(png) Image(graph.create_png()) . . Bad pipe message: %s [b&#39;a/5.0 (Macintosh; Intel Mac OS X 10.14; rv:96.0) Gecko/20100101 Firefox/96.0 r nAccept: */* r nAccept-Language:&#39;] . The above decision tree has identified the isolated features, as well as the conjunction of features, that best predict the chances of a given passenger of the Titanic to survive. These features, ordered loosely in terms of importance, are sex, age, sibsp (number of siblings or spouses), and pclass (passenger class). . As we can infer from the tree, were you a male passenger (sex &lt;= 0.5) on the Titanic over the age of 9.5 you probably did not survive the crash. If, however, you were either a female passenger or a male child below the age of 9.5 with fewer than 2.5 siblings (a conjunction of features), it&#39;s likely that you survived. . Setup . Simplifying Assumptions . In the rest of this article, for simplicity, we will assume binary input and binary output for decision trees. That is, the training set is ${S = {(x^1,y^1), ... ,(x^k, y^k) }}$ with ${x^i in {0,1 }^n}$ and ${y^i in {0,1 } forall i}$. This means that the decision tree itself is simply a binary function which also receives binary input. . The task is to learn this function. . Potential Function .",
            "url": "https://v-poghosyan.github.io/blog/machine%20learning/decision%20trees/random%20forests/2021/12/29/Machine-Learning-Decision-Trees.html",
            "relUrl": "/machine%20learning/decision%20trees/random%20forests/2021/12/29/Machine-Learning-Decision-Trees.html",
            "date": " • Dec 29, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Python for Machine Learning - Pandas",
            "content": "Importing Pandas . Pandas is a library that contains pre-written code to help wrangle with data. We can think of it as Python&#39;s equivalent of Excel. . We import Pandas into our development environment as we import any other library — using the import command. . import pandas . It&#39;s standard to import Pandas with the shorthand pd in order to avoid typing pandas all the time. . import pandas as pd . This gives us access to a vast array of pre-built objects, functions, and methods which are detailed in the API reference. . Two Underlying Data Types . Series . The basic unit of Pandas is the pandas.Series object which, in keeping with the Excel analogy, can be thought of as a column in an Excel table. It&#39;s a one-dimensional data structure that&#39;s derived from a NumPy array. However, unlike a NumPy array, the indices of a Series object aren&#39;t limited to the integer values $0,1,...n$ — they can also be descriptive labels. . Let&#39;s create a Series object representing the populations of the G-7 countries in units of millions. . import pandas as pd g7_pop = pd.Series([35,63,80,60,127,64,318]) . As we can see, creating a series is a matter of passing a Python list (or a Numpy array) into the Series constructor. . Indexing . Indexing a Series object is similar to indexing a Python list. For instance, let&#39;s print the first element in the above series. . g7_pop[0] . 35 . Let&#39;s now swap out the integer-based indices with descriptive labels. Each Series object has an index argument that can be overwritten. . g7_pop.index = [ &#39;Canada&#39;, &#39;France&#39;, &#39;Germany&#39;, &#39;Italy&#39;, &#39;Japan&#39;, &#39;UK&#39;, &#39;US&#39; ] . Now, we can print the first element of the series using its descriptive label. . g7_pop[&#39;Canada&#39;] . 35 . We may notice a similarity between a standard Python dictionary and a labeled Series object. Namely, indexing a series with a label and keying into a Python dictionary have similar syntax. In fact, it&#39;s possible to create a labeled Series object directly from a Python dictionary. . g7_pop = pd.Series({ &#39;Canada&#39; : 35, &#39;France&#39; : 63, &#39;Germany&#39; : 80, &#39;Italy&#39; : 60, &#39;Japan&#39; : 127, &#39;UK&#39; : 64, &#39;US&#39; : 318 }) g7_pop . Canada 35 France 63 Germany 80 Italy 60 Japan 127 UK 64 US 318 dtype: int64 . In the event of overwritting the integer-based indices, it&#39;s still possible to access the elements of a Series sequentially using the iloc method (short for &quot;integer location&quot;). To retrieve the population of Canada, we can do as follows: . g7_pop.iloc[0] . 35 . It&#39;s also possible to use a range when indexing. For instance, suppose we&#39;d like to retrieve the populations of the first three countries from g7_pop. We can simply write: . g7_pop[&#39;Canada&#39;:&#39;Germany&#39;] . Canada 35 France 63 Germany 80 dtype: int64 . Or equivalently: . g7_pop.iloc[0:3] . Canada 35 France 63 Germany 80 dtype: int64 . Since the Series object is based on a Numpy array, it also supports multi-indexing through passing a list of indices or a Boolean mask. . For instance, to print the populations of Canada and Germany at the same time, we can pass in the list [&#39;Canada&#39;,&#39;Germany&#39;] or the Boolean mask [True, False, True, False, False, False, False]. . g7_pop[[&#39;Canada&#39;,&#39;Germany&#39;]] . Canada 35 Germany 80 dtype: int64 . g7_pop[[True, False, True, False, False, False, False]] . Canada 35 Germany 80 dtype: int64 . Broadcasted and Vectorized Operations . Since it&#39;s based on a NumPy array, a Series object also supports vectorization and broadcasted operations. . As a quick reminder, vectorization is the process by which NumPy optimizes looping in Python. It stores the array internally in a contiguous block of memory and restricts its contents to only one data type. Letting Python know this data type in advance, NumPy can then skip the per-iteration type checking that Python normally does in order to speed up our code. In fact, NumPy delegates most of the operations on such optimized arrays to pre-written C code under the hood. . Broadcasting, on the other hand, is the optimized process by which NumPy performs arithmetic and Boolean operations on arrays of unequal dimensions. . For instance, suppose the projected population growth of each G-7 country is 10 mln by the year 2030. Instead of looping through the Series object and adding 10 to each row or using a list comprehension, we can simply use broadcasted addition. . g7_2030_pop = g7_pop + 10 g7_2030_pop . Canada 45 France 73 Germany 90 Italy 70 Japan 137 UK 74 US 328 dtype: int64 . Filtering . Thanks to broadcasted Boolean operations and multi-indexing with a Boolean mask, it&#39;s possible to write concise and readable filtering expressions on Series. . For instance, let&#39;s return the list of countries with a population over 70 mln. . g7_pop[g7_pop &gt;= 70] . Germany 80 Japan 127 US 318 dtype: int64 . The expression g7_pop &gt;= 70 is a broadcasted Boolean operation on the Series object g7_pop which returns a Boolean array [False, False, True, False, True, False, True]. Then g7_pop is multi-indexed using this Boolean mask. . As another example of readable filtering expressions, we can return the list of countries whose populations exceed the mean population. . g7_pop[g7_pop &gt;= g7_pop.mean()] . Japan 127 US 318 dtype: int64 . DataFrame . Each DataFrame is composed of one or more Series. Whereas a Series is analogous to a column of an Excel table, a DataFrame is analogous to the table itself. . The DataFrame constructor accepts a variety of input types, among them an ndarray and a dictionary. . If we&#39;re passing an ndarray, it becomes necessary to specify the column labels separately. Additionally, we may overwrite the integer-based indexing as we did with the Series object. . data = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) df = pd.DataFrame(data, index = [&#39;R1&#39;,&#39;R2&#39;,&#39;R3&#39;], columns = [&#39;C1&#39;, &#39;C2&#39;, &#39;C3&#39;]) df . C1 C2 C3 . R1 1 | 2 | 3 | . R2 4 | 5 | 6 | . R3 7 | 8 | 9 | . We may bypass specifying columns manually by passing in a dictionary instead: . data = { &#39;C1&#39; : [1, 2, 3], &#39;C2&#39; : [4, 5, 6], &#39;C3&#39; : [7, 8, 9] } df = pd.DataFrame(data, index = [&#39;R1&#39;,&#39;R2&#39;,&#39;R3&#39;]) df . C1 C2 C3 . R1 1 | 4 | 7 | . R2 2 | 5 | 8 | . R3 3 | 6 | 9 | . . Note: Whereas in a Series the keys of the input dictionary were the row labels, in a DataFrame they&#8217;re the column labels. . In practice we often create a DataFrame from a CSV file using the pandas.read_csv() method like so: . csv_path = &#39;file.csv&#39; #Stores the path to a CSV df = pd.read_csv(csv_path) . . Tip: We may optionally pass header = None as an argument to read_csv() after csv_path if the first row of the CSV file itself is a data point, and not a header. . Tip: Pandas also supports reading an Excel file into a DataFrame using the pandas.read_excel() method. . Common Methods . Here are the common DataFrame methods that we should keep in our toolbox. These methods give us an overview of our data, and help us clean it up. . df.head() shows, by default, the first 5 rows of the dataset. Accepts an integer argument for the number of rows to display. | df.tail() shows the last 5 rows, and also accepts an integer argument. | df.info() gives a bird&#39;s eye overview of the dataset by showing the total rows/columns, the number of non-null datapoints, and the data types. | df.describe() returns statistically significant values for each column such as, the mean, standard deviation, minimum, and maximum values. | df.shape - returns the dimension of the dataset as an $(m,n)$ tuple. | . Indexing . Let&#39;s add to the dataset of G-7 countries the columns &#39;GDP&#39; and &#39;Surface Area&#39;. . g7_df = pd.DataFrame({ &#39;Population&#39; : g7_pop, &#39;GDP&#39; : [1.7, 2.8, 3.8, 2.1, 4.6, 2.9, 1.7], &#39;Surface Area&#39; : [9.0, 0.6, 0.3, 0.3, 0.3, 0.2, 9.0] }) g7_df . Population GDP Surface Area . Canada 35 | 1.7 | 9.0 | . France 63 | 2.8 | 0.6 | . Germany 80 | 3.8 | 0.3 | . Italy 60 | 2.1 | 0.3 | . Japan 127 | 4.6 | 0.3 | . UK 64 | 2.9 | 0.2 | . US 318 | 1.7 | 9.0 | . Whereas in a Series, the primary axis of indexing are the rows, in a DataFrame the primary axis are the columns. Thus, indexing a column works as expected. . g7_df[&#39;GDP&#39;] . Canada 1.7 France 2.8 Germany 3.8 Italy 2.1 Japan 4.6 UK 2.9 US 1.7 Name: GDP, dtype: float64 . Multi-indexing also works in the familiar way: . g7_df[[&#39;Population&#39;,&#39;GDP&#39;]] . Population GDP . Canada 35 | 1.7 | . France 63 | 2.8 | . Germany 80 | 3.8 | . Italy 60 | 2.1 | . Japan 127 | 4.6 | . UK 64 | 2.9 | . US 318 | 1.7 | . If we want to index rows, however, we must use the loc or iloc methods. . For instance, say we are interested in the population, GDP, and surface area of only the first three countries. We could query the dataset like so: . g7_df.loc[&#39;Canada&#39;:&#39;Germany&#39;] . Population GDP Surface Area . Canada 35 | 1.7 | 9.0 | . France 63 | 2.8 | 0.6 | . Germany 80 | 3.8 | 0.3 | . If we&#39;re only interested in the population and GDP of the first three countries, then we could instead do the following: . g7_df[[&#39;Population&#39;, &#39;GDP&#39;]].loc[&#39;Canada&#39;:&#39;Germany&#39;] . Population GDP . Canada 35 | 1.7 | . France 63 | 2.8 | . Germany 80 | 3.8 | . Noting that loc accepts two inputs, one for the selection of rows and one for columns, we can achieve the above more concisely as follows: . g7_df.loc[&#39;Canada&#39;:&#39;Germany&#39;, [&#39;Population&#39;, &#39;GDP&#39;]] . Population GDP . Canada 35 | 1.7 | . France 63 | 2.8 | . Germany 80 | 3.8 | . Filtering . Since the loc method also accepts a Boolean mask as input, we use it to filter the DataFrame by row. For instance, suppose we want the GDP of countries with a population over 70 mln. We query the dataset as follows: . g7_df.loc[g7_df[&#39;Population&#39;] &gt;= 70, &#39;GDP&#39;] . Germany 3.8 Japan 4.6 US 1.7 Name: GDP, dtype: float64 . As additional exercise, suppose we are only interested in the population and GDP of countries that are smaller than 1.0 mln square kilometers. . g7_df.loc[g7_df[&#39;Surface Area&#39;] &lt;= 1.0, [&#39;Population&#39;,&#39;GDP&#39;]] . Population GDP . France 63 | 2.8 | . Germany 80 | 3.8 | . Italy 60 | 2.1 | . Japan 127 | 4.6 | . UK 64 | 2.9 | . Adding, Dropping, and Renaming Columns and Rows . Let&#39;s add a &#39;Languages&#39; column to g7_df. We simply follow the same syntax as adding a key to a dictionary... . languages = pd.Series({ &#39;Canada&#39; : &#39;French, English&#39;, &#39;France&#39; : &#39;French&#39;, &#39;Germany&#39; : &#39;German&#39;, &#39;Italy&#39; : &#39;Italian&#39;, &#39;Japan&#39; : &#39;Japanese&#39;, &#39;UK&#39; : &#39;English&#39;, &#39;US&#39; : &#39;English&#39; }) # Next, we add the series as a column to the DataFrame g7_df[&#39;Languages&#39;] = languages g7_df . Population GDP Surface Area Languages . Canada 35 | 1.7 | 9.0 | French, English | . France 63 | 2.8 | 0.6 | French | . Germany 80 | 3.8 | 0.3 | German | . Italy 60 | 2.1 | 0.3 | Italian | . Japan 127 | 4.6 | 0.3 | Japanese | . UK 64 | 2.9 | 0.2 | English | . US 318 | 1.7 | 9.0 | English | . We can also drop a column or a row using the drop() method. . Let&#39;s drop the newly created &#39;Languages&#39; column. To drop a coulumn, we specify a columns argument in the drop() method like so: . g7_df.drop(columns = &#39;Languages&#39;) . Population GDP Surface Area . Canada 35 | 1.7 | 9.0 | . France 63 | 2.8 | 0.6 | . Germany 80 | 3.8 | 0.3 | . Italy 60 | 2.1 | 0.3 | . Japan 127 | 4.6 | 0.3 | . UK 64 | 2.9 | 0.2 | . US 318 | 1.7 | 9.0 | . The method drop() returns a new DataFrame which does not contain the unneeded column, but the original g7_df still contains this column. To prove this, let&#39;s print it: . g7_df . Population GDP Surface Area Languages . Canada 35 | 1.7 | 9.0 | French, English | . France 63 | 2.8 | 0.6 | French | . Germany 80 | 3.8 | 0.3 | German | . Italy 60 | 2.1 | 0.3 | Italian | . Japan 127 | 4.6 | 0.3 | Japanese | . UK 64 | 2.9 | 0.2 | English | . US 318 | 1.7 | 9.0 | English | . As we can see the &#39;Languages&#39; column is still there. The solution is to specify an inplace = True argument so that the column is dropped in-place. . g7_df.drop(columns = &#39;Languages&#39;, inplace = True) g7_df . Population GDP Surface Area . Canada 35 | 1.7 | 9.0 | . France 63 | 2.8 | 0.6 | . Germany 80 | 3.8 | 0.3 | . Italy 60 | 2.1 | 0.3 | . Japan 127 | 4.6 | 0.3 | . UK 64 | 2.9 | 0.2 | . US 318 | 1.7 | 9.0 | . In order to drop rows, we specify the index argument instead. Suppose we want to remove Canada and Italy from the dataset. . g7_df.drop(index = [&#39;Canada&#39;, &#39;Italy&#39;]) . Population GDP Surface Area . France 63 | 2.8 | 0.6 | . Germany 80 | 3.8 | 0.3 | . Japan 127 | 4.6 | 0.3 | . UK 64 | 2.9 | 0.2 | . US 318 | 1.7 | 9.0 | . It is also possible to rename a column or a row using the rename() method. . Suppose we&#39;d like to rename the columns to include the units of measurement, and suppose we&#39;d also like to expand the UK and US to their full names. . g7_df.rename( columns = { &#39;Population&#39; : &#39;Population (mln)&#39;, &#39;GDP&#39; : &#39;GDP (USD)&#39;, &#39;Surface Area&#39; : &#39;Surface Area (mln sq. km)&#39; }, index = { &#39;UK&#39; : &#39;United Kingdom&#39;, &#39;US&#39; : &#39;United States&#39; } ) . Population (mln) GDP (USD) Surface Area (mln sq. km) . Canada 35 | 1.7 | 9.0 | . France 63 | 2.8 | 0.6 | . Germany 80 | 3.8 | 0.3 | . Italy 60 | 2.1 | 0.3 | . Japan 127 | 4.6 | 0.3 | . United Kingdom 64 | 2.9 | 0.2 | . United States 318 | 1.7 | 9.0 | . Manipulating Columns . Those of us familiar with Excel know the functions feature which allows users to select specific cells or combinations of cells, perform algebraic or logical operations with their contents, and store the results in new cells. The way to do that in Pandas is, once again, through broadcasted operations. . For instance, suppose we&#39;d like to add a new &#39;GDP Per Capita&#39; column to the g7_df dataset. This is simply a matter of dividing the GDP of each country by its population. . Using broadcasted division, the code is simply: . g7_df[&#39;GDP Per Capita&#39;] = g7_df[&#39;GDP&#39;] / g7_df[&#39;Population&#39;] g7_df . Population GDP Surface Area GDP Per Capita . Canada 35 | 1.7 | 9.0 | 0.048571 | . France 63 | 2.8 | 0.6 | 0.044444 | . Germany 80 | 3.8 | 0.3 | 0.047500 | . Italy 60 | 2.1 | 0.3 | 0.035000 | . Japan 127 | 4.6 | 0.3 | 0.036220 | . UK 64 | 2.9 | 0.2 | 0.045312 | . US 318 | 1.7 | 9.0 | 0.005346 | . Worked Example - Bitcoin Price Timeseries: Cleaning and Reindexing . Sometimes we may wish to use a certain column to index a DataFrame. For instance, if we&#39;re working with a dataset of Bitcoin prices, it would be wise to use the &#39;Time&#39; column as the index so that we can easily access the value of Bitcoin at a given time. . Note: Data that&#8217;s indexed by time is called a timeseries... . For this example, we will retrieve the actual daily Bitcoin price history from CoinCap API 2.0. Feel free to check out the code that fetches the data as JSON and converts it into a Pandas DataFrame in the collapsable code below. . import requests import json # Specifying request URL, payload, and headers url = &#39;https://api.coincap.io/v2/assets/bitcoin/history?interval=d1&#39; payload = {} headers = {} # Making the request and parsing it as JSON response = requests.request(&#39;GET&#39;, url, headers = headers, data = payload) json_data = json.loads(response.text)[&#39;data&#39;] # Converting the result into a DataFrame bitcoin_df = pd.json_normalize(json_data) # Cleanup bitcoin_df.rename( columns = { &#39;priceUsd&#39; : &#39;priceInUSD&#39;, &#39;time&#39; : &#39;Time&#39;, &#39;date&#39; : &#39;Date&#39; }, inplace = True ) . . The result of this is the following DataFrame: . bitcoin_df.head() . priceInUSD Time Date . 0 37480.8939504110899664 | 1610668800000 | 2021-01-15T00:00:00.000Z | . 1 36853.8623471143090244 | 1610755200000 | 2021-01-16T00:00:00.000Z | . 2 35670.6623897365179078 | 1610841600000 | 2021-01-17T00:00:00.000Z | . 3 36061.4760792230247237 | 1610928000000 | 2021-01-18T00:00:00.000Z | . 4 36868.3293610208260669 | 1611014400000 | 2021-01-19T00:00:00.000Z | . Now that we have the dataset as a cleaned-up Pandas DataFrame called bitcoin_df, we can get to work. . First order of business is to reindex the dataset based on the &#39;Time&#39; column. We can set a column as index using the set_index() method in the following way: . bitcoin_df.set_index(&#39;Time&#39;, inplace=True) bitcoin_df.index.name = None # The index column shouldn&#39;t have a name — this removes the name &#39;Time&#39; bitcoin_df.head() . priceInUSD Date . 1610668800000 37480.8939504110899664 | 2021-01-15T00:00:00.000Z | . 1610755200000 36853.8623471143090244 | 2021-01-16T00:00:00.000Z | . 1610841600000 35670.6623897365179078 | 2021-01-17T00:00:00.000Z | . 1610928000000 36061.4760792230247237 | 2021-01-18T00:00:00.000Z | . 1611014400000 36868.3293610208260669 | 2021-01-19T00:00:00.000Z | . As we can see the column that was previously named &#39;Time&#39; now acts as index. . Next, we should convert the entries of the index from a Unix timestamp into a Python datetime for more clarity. While doing this, let&#39;s also convert the entries of the &#39;Date&#39; column which are currently in string format. . bitcoin_df.index = pd.to_datetime(bitcoin_df.index, unit=&#39;us&#39;) # Converting index to datetime from Unix seconds bitcoin_df[&#39;Date&#39;] = pd.to_datetime(bitcoin_df[&#39;Date&#39;]) # Converting &#39;Date&#39; to datetime from string bitcoin_df.head() . priceInUSD Date . 2021-01-15 37480.8939504110899664 | 2021-01-15 00:00:00+00:00 | . 2021-01-16 36853.8623471143090244 | 2021-01-16 00:00:00+00:00 | . 2021-01-17 35670.6623897365179078 | 2021-01-17 00:00:00+00:00 | . 2021-01-18 36061.4760792230247237 | 2021-01-18 00:00:00+00:00 | . 2021-01-19 36868.3293610208260669 | 2021-01-19 00:00:00+00:00 | . Now we can comfortably access the price of Bitcoin on any given day. Suppose we&#39;d like to know its price on 2021-12-28, the day of writing this post... We can simply do: . bitcoin_df.loc[&#39;2021-12-28&#39;, &#39;priceInUSD&#39;] . &#39;48995.0145281203441155&#39; . Common Workflows . Finding the Unique Elements in a Column . Pandas comes with the method unique() which can be applied to a Series object. . Let&#39;s fetch some data about the planets in our solar system from the devstronomy repository. . planets_df = pd.read_csv(&#39;https://raw.githubusercontent.com/devstronomy/nasa-data-scraper/master/data/csv/planets.csv&#39;) planets_df . planet mass diameter density gravity escape_velocity rotation_period length_of_day distance_from_sun perihelion ... orbital_period orbital_velocity orbital_inclination orbital_eccentricity obliquity_to_orbit mean_temperature surface_pressure number_of_moons has_ring_system has_global_magnetic_field . 0 Mercury | 0.3300 | 4879 | 5427 | 3.7 | 4.3 | 1407.6 | 4222.6 | 57.9 | 46.0 | ... | 88.0 | 47.4 | 7.0 | 0.205 | 0.034 | 167 | 0 | 0 | No | Yes | . 1 Venus | 4.8700 | 12104 | 5243 | 8.9 | 10.4 | -5832.5 | 2802.0 | 108.2 | 107.5 | ... | 224.7 | 35.0 | 3.4 | 0.007 | 177.400 | 464 | 92 | 0 | No | No | . 2 Earth | 5.9700 | 12756 | 5514 | 9.8 | 11.2 | 23.9 | 24.0 | 149.6 | 147.1 | ... | 365.2 | 29.8 | 0.0 | 0.017 | 23.400 | 15 | 1 | 1 | No | Yes | . 3 Mars | 0.6420 | 6792 | 3933 | 3.7 | 5.0 | 24.6 | 24.7 | 227.9 | 206.6 | ... | 687.0 | 24.1 | 1.9 | 0.094 | 25.200 | -65 | 0.01 | 2 | No | No | . 4 Jupiter | 1898.0000 | 142984 | 1326 | 23.1 | 59.5 | 9.9 | 9.9 | 778.6 | 740.5 | ... | 4331.0 | 13.1 | 1.3 | 0.049 | 3.100 | -110 | Unknown* | 79 | Yes | Yes | . 5 Saturn | 568.0000 | 120536 | 687 | 9.0 | 35.5 | 10.7 | 10.7 | 1433.5 | 1352.6 | ... | 10747.0 | 9.7 | 2.5 | 0.057 | 26.700 | -140 | Unknown* | 62 | Yes | Yes | . 6 Uranus | 86.8000 | 51118 | 1271 | 8.7 | 21.3 | -17.2 | 17.2 | 2872.5 | 2741.3 | ... | 30589.0 | 6.8 | 0.8 | 0.046 | 97.800 | -195 | Unknown* | 27 | Yes | Yes | . 7 Neptune | 102.0000 | 49528 | 1638 | 11.0 | 23.5 | 16.1 | 16.1 | 4495.1 | 4444.5 | ... | 59800.0 | 5.4 | 1.8 | 0.011 | 28.300 | -200 | Unknown* | 14 | Yes | Yes | . 8 Pluto | 0.0146 | 2370 | 2095 | 0.7 | 1.3 | -153.3 | 153.3 | 5906.4 | 4436.8 | ... | 90560.0 | 4.7 | 17.2 | 0.244 | 122.500 | -225 | 0.00001 | 5 | No | Unknown | . 9 rows × 21 columns . If we want to find out the unique number of moons each planet has, we can simply do: . planets_df[&#39;number_of_moons&#39;].unique() . array([ 0, 1, 2, 79, 62, 27, 14, 5], dtype=int64) . As we can see, the 9 planets in our solar system (counting Pluto) have 8 unique number of moons. This is because, as we can see from the dataset, Mercury and Venus both have 0 moons. . Saving Data . After all the data manipulation, it would be useful to save the resulting dataset locally on our machine. Pandas offers us a way to do that using the DataFrame.to_csv() method. . Working with the planets_df defined above, we can narrow the dataset down to the planets which have a gravitational force that&#39;s close to that of the Earth ($9.8 m/s^2$). . earthlike_planets_df = planets_df[(planets_df[&#39;gravity&#39;] &gt;= 9.8 - 1) &amp; (planets_df[&#39;gravity&#39;] &lt;= 9.8 + 1)] earthlike_planets_df . planet mass diameter density gravity escape_velocity rotation_period length_of_day distance_from_sun perihelion ... orbital_period orbital_velocity orbital_inclination orbital_eccentricity obliquity_to_orbit mean_temperature surface_pressure number_of_moons has_ring_system has_global_magnetic_field . 1 Venus | 4.87 | 12104 | 5243 | 8.9 | 10.4 | -5832.5 | 2802.0 | 108.2 | 107.5 | ... | 224.7 | 35.0 | 3.4 | 0.007 | 177.4 | 464 | 92 | 0 | No | No | . 2 Earth | 5.97 | 12756 | 5514 | 9.8 | 11.2 | 23.9 | 24.0 | 149.6 | 147.1 | ... | 365.2 | 29.8 | 0.0 | 0.017 | 23.4 | 15 | 1 | 1 | No | Yes | . 5 Saturn | 568.00 | 120536 | 687 | 9.0 | 35.5 | 10.7 | 10.7 | 1433.5 | 1352.6 | ... | 10747.0 | 9.7 | 2.5 | 0.057 | 26.7 | -140 | Unknown* | 62 | Yes | Yes | . 3 rows × 21 columns . . Tip: Pandas prefers the use of bitwise Boolean operators &amp; and |, instead of the Python&#8217;s default and and or. This is because Pandas relies on NumPy, which in turn relies on the capacity of the bitwise operators to be overloaded. . We can now save this new dataset to our desktop as follows: . earthlike_planets_df.to_csv(&#39;C:/Users/Vahram/Desktop/earthlike_planets.csv&#39;) .",
            "url": "https://v-poghosyan.github.io/blog/python%20for%20ml/pandas/machine%20learning/2021/12/28/Python-for-ML-Pandas.html",
            "relUrl": "/python%20for%20ml/pandas/machine%20learning/2021/12/28/Python-for-ML-Pandas.html",
            "date": " • Dec 28, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "LeetCode 11 - Container with Most Water",
            "content": "Introduction . Problem Statement . You are given an integer array height of length n. There are n vertical lines drawn such that the two endpoints of the ith line are (i, 0) and (i, height[i]). . Find two lines that together with the x-axis form a container, such that the container contains the most water. . Return the maximum amount of water a container can store. . Example: . . Input: height = [1,8,6,2,5,4,8,3,7] Output: 49 Explanation: The above vertical lines are represented by array [1,8,6,2,5,4,8,3,7]. In this case, the max area of water (blue section) the container can contain is 49. . Foreword . The brute force solution to this problem consists of checking each pair of vertical lines. Since order of any given pair does not matter, this solution has time complexity ${O({n choose 2}) = O(n^2)}$ where $n$ is the length of the height array. . The non-brute-force solution to this problem (i.e. the &#39;two pointer solution&#39;) is pretty intuitive - the difficulty lies in its proof of correctness. Therefore, I will give away the procedure in this foreword and then proceed to prove its correctness. . The Procedure . The following is the overall procedure in words: . Initialize left and right pointers at 1 and n respectively (assuming indices start at 1). | While the pointers do not intersect: . Fix the pointer whose corresponding vertical line is longer. | Advance the pointer whose corresponding vertical line is shorter towards the fixed one. | . | . The Code . #hide-output class Solution: def maxArea(self, height: List[int]) -&gt; int: i, j = 0, len(height) - 1 water = 0 while i &lt; j: water = max(water, (j - i) * min(height[i], height[j])) if height[i] &lt; height[j]: i += 1 else: j -= 1 return water . . Proof of Correctness . Optimal Substructure . The procedure is inspired by the following recursive optimal substructure of the problem: . Let $h(i)$ denote the height of the $i$-th vertical line. . | Let $a(i,j)$ denote the area of the container formed by the pair of vertical lines $(i,j)$. . | Let $maxArea([i:j])$ denote the maximum area formed by the lines ${i,...,j}$ – that is the output of the procedure on the subarray ${[i:j]}$. . | . Claim . The problem has optimal substructure: $$maxArea([1:n]) = max {a(1,n), maxArea([2:n]) }$$ . Proof of Claim . For the initial pair $(1,n)$ where, WLOG, ${h(1) &lt; h(n)}$ we have ${a(1,n) &gt; a(1,k) forall k}$. This is because we&#39;re starting out from the widest container formed by ${(1,n)}$ and considering containers of decreasing width formed by the pairs ${(1, n-1), (1, n-2), ..., (1,2)}$. . There are two cases: . In case ${h(k) &gt; h(1)}$ for some ${1 &lt; k leq n}$ the area of the container formed by ${(1,k)}$ is still determined by ${h(1)}$, except now it&#39;s less wide. Whereas if ${h(k) &lt; h(1)}$ the area of the container decreases not only in width but also in height. . In both cases we have ${a(1,n) &gt; a(1,k)}$ which means in general ${a(1,n) &gt; a(1, k) forall k}$. . Therefore, we may omit the first vertical line from consideration and consider the subproblem on the indices ${2,...,n}$. The overall optimal solution will then be $$maxArea([1:n]) = max {a(1,n), maxArea([2:n]) }$$ as was the claim. . Inductive Proof . As with all problems that have a recursive optimal substructure, an inductive proof of correctness is immediately what springs to mind. . Base Case . For the case when $n = 2$, ${maxArea([1:2]) = max {a(1,2), maxArea([2:2]) } = a(1,2)}$ since ${maxArea[2:2] = 0}$. This is obviously correct. . Inductive Step . Suppose for an array of length $m$, the procedure $maxArea$ is correct. We would like to show that for an array of length $m+1$ it is still correct. . Assume, WLOG, ${h(1) &lt; h(m+1)}$. Note that by the optimal substructure shown above, ${maxArea([1:m+1]) = max {a(1,m+1), maxArea([2:m+1]) }}$. In omitting the first element of the input array, the only pairs we remove from consideration are ${(1,m), (1,m-1),..., (1,2)}$ which we have already shown to be suboptimal to ${(1,m+1)}$ in the proof of the optimal substructure. And since by assumption the procedure $maxArea$ on the $m$-element subarray ${[2:m+1]}$ is correct, we are done! . Conclusion . With some problems, it is the case that figuring out why the solution works gives more insight into the problem than simply solving it based on raw intuition... .",
            "url": "https://v-poghosyan.github.io/blog/leetcode/toy%20problems/proofs/2021/12/23/LeetCode-11.html",
            "relUrl": "/leetcode/toy%20problems/proofs/2021/12/23/LeetCode-11.html",
            "date": " • Dec 23, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Thanks for dropping by! I’m Vahram, a software developer and graduate student of Computer Science @ UT Austin. . 🎓 Education . After graduating from community college in 2018, I transfered to UCLA where I pursued my undergraduate studies in Applied Mathematics. In 2020, having graduated from UCLA with honors, I enrolled in the M.S. in Computer Science program @ UT Austin. . 💻 Work . While still at community college I got my first job as a Software Engineer Intern @ Omron Automation. There, I was part of the team that developed embedded systems for the motion controllers and control panels, as well as the team that built and maintained the company’s proprietary Visual Studio based IDE. . In the Summer of 2022 I will be joining the team over @ Capital One as a Software Engineer Intern. . 🙋‍♂️ Social Media . For a detailed rundown of my work history, check out my LinkedIn. . Feel free to also email me at: vahram@utexas.edu .",
          "url": "https://v-poghosyan.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://v-poghosyan.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}