{
  
    
        "post0": {
            "title": "Convex Optimization - Algorithms for Unconstrained Optimization, Gradient-Descent",
            "content": "PLAN . Unconstrained algorithms | Oracle Access Model of order 1 | Develop GD from two perspectives - linear and quadratic | Run GD on model problems x^2 and |x| | Develop the notion of M-smooth and m-strongly convex based on step 4 | Analyze the performance of GD . | Develop Accelerated GD . | Develop Subgradient Descent | Introduction . Gradient descent is a powerful, yet incredibly simple, algorithm for unconstrained, convex optimization. We can think of it as the analog of a greedy algorithm in the setting of convex, continuous optimization. That is, it is our attempt to do best locally given limited information about the objective $f$, and limited computational power. . The Gradient Descent Algorithm . As an iterative algorithm, it relies on initialization and an update step. . Let $x$ be the initial iterate, and let the update be given by $x^+ = x + eta d$ for some directional unit-vector $d$ and step-size parameter $ eta &gt; 0$. . We base the algorithm on the assumption that the linear approximation of the objective at a the next iterate $x^+$ is a good-enough local approximation of its true value at $x^+$. That is: . $$f(x + eta d) approx f(x) + eta nabla f(x)^T d tag{1.1}$$ . Since we wish to minimize $f$, it would be wise to insist that the objective at $x^+$ improves or, at the very least, does not worsen. This is the locally optimal choice that presents itself to us. We insist: . $$f(x + eta d) approx f(x) + eta nabla f(x)^T d leq f(x) tag{1.2}$$ . Since $f(x)$ is fixed and $ eta &gt; 0$, this amounts to minimizing the scaled inner-product $ nabla f(x)^Td$. So, we choose $d$ opposite and parallel to the gradient, i.e. $d = - frac{ nabla f(x)}{|| nabla f(x)||_2}$. . The update step becomes: . $$x^+ = x - eta frac{ nabla f(x)}{|| nabla f(x)||_2}$$ . By re-labeling, $ eta$ absorbs the normalization constant obtaining the final gradient descent update step: . $$x^+ = x - eta nabla f(x) tag{1.3}$$ . This makes intuitive sense because the negative gradient direction is the direction of steepest decrease. So, it&#39;s only natural that the update should take us in this most enticing direction given that we&#39;ve specified our goal is to minimize the objective. . Important Questions . Given the ease with which we came up with the algorithm, we should ask ourselves the following questions. . Is gradient descent guaranteed to converge for all step-sizes? | How should we choose a step-size that guarantees convergence? | What&#39;s the rate of convergence of gradient descent? Does the rate depend on step-size? Does it depend on properties of the objective function? | How should we choose a step-size that maximizes convergence rate? | We will explore each of this questions shortly but, before doing that, it&#39;s worth taking a high-level look at convex optimization itself. Perhaps the most important question to ask is this: does gradient descent&#39;s convergence rate, for an optimally chosen step-size, give a taxonomy of easier-to-harder problems within the scope of convex optimization? The answer, as it turns out, is yes. . Let&#39;s start by running gradient descent on two quintessential convex problems in $ mathbb{R}$, $f(x) = x^2$ and $h(x) = |x|$. . If $f$ is $M$-smooth, then we have the tightest possible point-wise quadratic upper bound as: . $$f(y) leq f(x) + nabla f(x)^T(y-x) + frac{M}{2} ||y-x||_2^2 forall y$$ . How does this help us choose step size in a better way? . Plug the step $x^+ = x - eta nabla f(x)$ into the upper bound: . $$ begin{aligned} f(x^+) &amp; leq f(x) + nabla f(x)^T( eta nabla f(x)) + frac{M}{2} || eta nabla f(x)||_2^2 iff f(x^+) &amp; leq f(x) + eta || nabla f(x) ||_2^2 + frac{M eta^2}{2} || nabla f(x)||_2^2 end{aligned} $$ . But the RHS is a quadratic function in $ eta$, the step-size. So, the upper bound on the next iterate $f(x^+)$, and consequently also the next iterate itself, can be minimized w.r.t. $ eta$. Essentially, we trust the upper bound is a good quadratic approximation and choose its minimizer as the next iterate. Just like in NM we trust the quadratic estimate, and set its minimizer as the next iterate. But here, we require no second-order information about the function. . The non-NM view is this. We have a fixed $x$ and the quadratic upper bound at that $x$. Any next iterate will be smaller than the qudratic UB at that iterate. So we choose a step size that minimizes this UB. This guarantees $f(x^+) leq$ min of UB which is the tightest guarantee we can get on $f(x^+)$ with the information at hand. So we, once again, do the locally optimal thing and choose that. . The best choice turns out to be $ eta = frac{1}{M}$ . FOR FUNCTIONS THAT ARE M SMOOTH M WORKS FOR EVERY POINT. WHEREAS WE ARE SHIT OUT OF LUCK IF SUCH AN M DOESN&#39;T EXIST. THEN WE DON&#39;T HAVE THIS UB. IT IS THEN THAT WE USE EITHER NM OR QUASI-NM. . |||| .",
            "url": "https://v-poghosyan.github.io/blog/optimization/applied%20mathematics/proofs/2022/03/03/Convex-Optimization-Algorithms-for-Unconstrained-Optimization-Gradient-Descent.html",
            "relUrl": "/optimization/applied%20mathematics/proofs/2022/03/03/Convex-Optimization-Algorithms-for-Unconstrained-Optimization-Gradient-Descent.html",
            "date": " • Mar 3, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Optimization - Robust LP's - Modelling Discrete Failures",
            "content": "Model . We are faced with the task of modeling a scenario in which at most $k$ of the total $n$ workers, machines, sensors, or other system components can fail. The task is to minimize the amount of system components needed, thereby minimizing cost, subject to certain known and robust constraints. . We will assume the linear cost function to be $c^Tx$, and the known constraints to be $Ax leq b$. The robust constraint is $a_R^Tx leq b_R, forall a_R in D_k$ where $b_R$ is a known vector, and $D_k$ is the following interval uncertainty set with an additional combinatorial constraint: . $D_k = { a : a_i in [ hat a_i - delta_i, hat a_i + delta_i] wedge textrm{at least $n-k$ of the $a_i$&#39;s exactly equal $ hat a_i$} }$ . In $D_k$, we can think of each $ hat a_i$ as the spec at which the $i$-th component should operate, and the $ delta_i$&#39;s as the $i$-th component&#39;s deviation from this spec. Thus, $D_k$ models the discrete failures scenario exactly... . Formulating as an Optimization Problem . Let&#39;s attempt to formulate this problem as a robust LP. . So far we have . $ begin{cases} min: c^Tx s.t.: begin{aligned} &amp;Ax leq b &amp;a_R^Tx leq b_R forall a_r in D_k end{aligned} end{cases} $ . Which is, of course, not in standard form. . For a fixed $x$, $a_R^Tx leq b_R forall a_r implies a_{R max}^Tx leq b_R$. But since we don&#39;t know the value of $a_{R max}$, we can formulate an inner optimization problem for which it is the optimal value. . So we can state the following equivalent optimization problem . $ begin{cases} min: c^Tx s.t.: Ax leq b begin{cases} max_{a_R}: &amp;a_R^Tx s.t.: &amp;a_R in D_k end{cases} leq b_R end{cases} $ . Formulating the Inner Problem as a Linear Program . Let&#39;s focus on the inner problem . $ begin{cases} max_{a_R}: &amp;a_R^Tx s.t.: &amp;a_R in D_k end{cases} leq b_R $ . Our strategy now is to expand the constraint set $D_k$. . To that end, let&#39;s introduce slack variables $-1 leq z_i leq 1 forall i$, which represent the direction of each component&#39;s deviation from its spec. We can now rewrite the objective as: . $ begin{aligned} a_R^Tx &amp; = sum a_ix_i &amp; = sum ( hat a_i + z_i delta_i)x_i &amp; = sum hat a_ix_i + sum delta_iz_ix_i end{aligned} $ . So the optimization problem, which is now in the variables $z_i$, becomes . $ begin{cases} max_{z}: sum hat a_ix_i + sum delta_iz_ix_i s.t.: begin{aligned} &amp;-1 leq z_i leq 1 forall i &amp; textrm{at most $k$ of the $z_i ne 0$} end{aligned} end{cases} $ . We still have the combinatorial constraint &#39;at most $k$ of the $z_i ne 0$,&#39; which makes this into a mixed optimization problem... . We need to massage this problem more to bring it to standard form. . Note that we&#39;re dealing with a problem of maximization. In the objective, $ sum hat a_ix_i$ is fixed by virtue of the $ hat a_i$&#39;s being fixed by the given $D_k$ and the $x_i$&#39;s being fixed by the outer optimization problem. Note that the $ delta_i$&#39;s are also fixed by $D_k$. Therefore, what would maximize the objective is each term of $ sum delta_i z_i x_i$ contributing positively to the sum. . This happens when $z_i$ and $x_i$ have the same sign $ forall i$. That is, their product $z_ix_i$ takes values in $[0, |x_i|]$. . The remaining cases can be disposed of without changing the optimal value of the optimization problem. . Rewriting the problem, we have . $ begin{cases} max_{z}: sum hat a_ix_i + sum delta_iz_i|x_i| s.t.: begin{aligned} &amp;0 leq z_i leq 1 forall i &amp; textrm{at most $k$ of the $z_i ne 0$} end{aligned} end{cases} dagger $ . . Note: that $|x_i|$ does not make the objective non-linear because the $x_i$&#8217;s are fixed values, and not the decision variables. . Relaxing the Combinatorial Constraint with a Continuous Constraint . We will relax the combinatorial constraint &#39;at most $k$ of the $z_i ne 0$&#39; by replacing it with $ sum z_i leq k$. . Although this is a relaxation of the constraint, we will show that it makes no difference within the context of preserving the optimization problem. That is, it does not affect the optimal value of the problem. . First and foremost, it&#39;s easy to see that &#39;at most $k$ of the $z_i ne 0$&#39; $ implies$ $ sum z_i leq k$ since each $z_i in [0,1]$. . We claim the converse is true as well, given that we restrict our attention to the optimal solution to the above LP $ dagger$. That is &#39;at most $k$ of the $z_i ne 0$&#39; $ impliedby$ $ sum z_i leq k$. . This claim is true by the geometry of linear programs. An optimal solution to the LP can only occur at an extreme point, and those are defined exactly by $n$ independent active constraints. . In the above LP, $0 leq z_i leq 1 forall i$ represent a set of $2n$ independent constraints, and $ sum z_i leq k$ is just one additional constraint. . If all of the $n$ active constraints come from $0 leq z_i leq 1 forall i$, then since a given $z_i$ cannot simultaneously be $0$ and $1$ the $z_i$&#39;s of the optimal solution must take integral values (that is, either $0$ or $1$ and nothing in between). . In the general case, at least $n-1$ constraints must come from $0 leq z_i leq 1 forall i$, which implies at least $n-1$ of the $z_i$&#39;s take integral values and the remaining active constraint is $ sum z_i = k$. But $n$ numbers, of which $n-1$ are integers, cannot add up to an integer value $k$ unless the remaining number is also an integer. So, once again we have that all the $z_i$&#39;s are integral valued. . Then $ sum z_i leq k$ $ implies$ at most $k$ of the $z_i = 1$ $ implies$ &#39;at most $k$ of the $z_i ne 0$&#39; as was the claim. . This leaves us with the inner optimization problem . $ begin{cases} max_{z}: sum hat a_ix_i + sum delta_iz_i|x_i| s.t.: begin{aligned} &amp;0 leq z_i leq 1 forall i &amp; sum z_i leq k end{aligned} end{cases} $ . which is finally a linear program. . Putting the Inner and Outer Problems Together . The combined optimization problem becomes . $ begin{cases} min: c^Tx s.t.: Ax leq b begin{cases} max_{z}: sum hat a_ix_i + sum delta_iz_i|x_i| s.t.: begin{aligned} &amp;0 leq z_i leq 1 forall i &amp; sum z_i leq k end{aligned} end{cases} leq b_R end{cases} $ . This is, of course, still not a linear program. Firstly, it&#39;s a mixture between minimization and maximization. Secondly, since the variables are $x_i$, and $z_i$, the term $ sum delta_iz_ix_i$ is not linear in the decision variables. Thirdly, $|x_i|$ is not linear in $x_i$. . We can address these issues one by one... . Taking the Dual of the Inner . We can turn the inner maximization problem to an inner minimization problem by taking its dual. As we know, by LP-duality (otherwise known as strong duality) this does not affect the optimal value of the problem. . The overall problem becomes . $ begin{cases} min: c^Tx s.t.: Ax leq b begin{cases} min_{ lambda}: sum hat a_ix_i + sum lambda_i + lambda_0k s.t.: begin{aligned} &amp; lambda_0 + lambda_i geq delta_i|x_i| forall i &amp; lambda geq 0 end{aligned} end{cases} leq b_R end{cases} $ . Flattening the problems, since both are now minimization, we arrive at the following . $ begin{cases} min_{x, lambda}: c^Tx s.t.: begin{aligned} &amp;Ax leq b &amp; sum hat a_ix_i + sum lambda_i + lambda_0k leq b_R &amp; lambda_0 + lambda_i geq delta_i|x_i| forall i &amp; lambda geq 0 end{aligned} end{cases} $ . Linearizing the Absolute Value Constraints . This is almost a linear program, except for the fact that $|x_i|$&#39;s are nonlinear terms in the constraint. The last step is to split these constraints into corresponding pairs of linear constraints. . For each $i$, . $ begin{aligned} lambda_0 + lambda_i geq delta_i|x_i| &amp; implies - lambda_0 - lambda_i leq delta_ix_i leq lambda_0 + lambda_i &amp; implies begin{aligned} lambda_0 + lambda_i &amp; geq delta_ix_i &amp; textrm{and} lambda_0 + lambda_i &amp; geq - delta_ix_i end{aligned} end{aligned}$ . So, the final problem, which is a linear program in every right, is . $ begin{cases} min_{x, lambda}: c^Tx s.t.: begin{aligned} &amp;Ax leq b &amp; sum hat a_ix_i + sum lambda_i + lambda_0k leq b_R &amp; lambda_0 + lambda_i geq delta_ix_i forall i &amp; lambda_0 + lambda_i geq - delta_ix_i forall i &amp; lambda geq 0 end{aligned} end{cases} $ . Conclusion . Using the geometry of linear programs and LP duality, we were able to sidestep the complexity of a robust problem with a combinatorial constraint by formulating it as a linear program which can be solved by a number of fast algorithms such as the Simplex Algorithm or the Interior Point Method. . This shows the versatility of linear programs in addressing a variety of interesting mixed optimization problems. .",
            "url": "https://v-poghosyan.github.io/blog/optimization/combinatorics/applied%20mathematics/2022/02/09/Optimization-Robust-Linear-Programs-Modelling-Discrete-Failures.html",
            "relUrl": "/optimization/combinatorics/applied%20mathematics/2022/02/09/Optimization-Robust-Linear-Programs-Modelling-Discrete-Failures.html",
            "date": " • Feb 9, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Optimization -  Duality",
            "content": "Introduction . Every convex optimization problem, designated as the primal, has a related problem called its dual which can be colloquially thought of as its evil twin. The primal and the dual represent two different perspectives on the same problem. . In the most general case, if the primal is a minimization problem, its dual is a maximization problem. In the case of constrained optimization, if the primal is minimization in $n$ variables and $m$ constraints then its dual is a maximization in $m$ variables and $n$ constraints. . Furthermore, any feasible value of the dual is a lower-bound for all feasible values of the primal. In particular, should they both exist, the dual optimum is a lower bound for the primal optimum. This property, called weak duality, is at the core of duality theory. Having a problem that obtains, at the very least, a useful lower-bound for the primal optimum and, possibly, the primal optimum itself is the nascent idea of formulating the dual. . In the best case scenario, problems exhibit a property called strong duality, which guarantees the primal and the dual optima agree with each other. Strongly dual problems include, but are not limited to, all linear programs and a category of convex non-linear optimization problems. For such problems, solving the dual guarantees that we&#39;ve also solved the primal. Furthermore, taking the dual of the dual gives back the primal. So this relationship is true in the converse — if we&#39;ve solved the primal then we&#39;ve also solved its dual. . This is what makes duality theory so useful in practice. Having a related, usually easier, optimization problem gives applied scientists a huge computational advantage. However, even if the dual does not turn out to be any easier to solve or strong duality fails to hold, we still stand to gain structural insights about the primal problem. . In this post we will show how the dual of a problem arises, we will examine in detail its relationship with the primal, and list all possible primal-dual outcomes. In doing so, we will look at duality in the general case of constrained optimization problems, in the specific case of linear programs, and in a certain category of unconstrained problem. . The Dual of a Constrained Problem . First, let&#39;s focus on deriving the dual of a constrained optimization problem. We shall see that, in a sense, constraints are what give rise to duality through the Lagrangian. Certain types of unconstrained problems also have duals which arise from introducing dummy constraints or directly through the Fenchel-Legendre Transform. . Take the most general form of a convex, constrained problem with $m$ inequality and $n$ equality constraints. To make the discussion interesting, assume the problem is non-trivial (i.e. its constraint set is non-empty and contains more than one feasible point). Furthermore, so that we may have a solution to speak of, assume the problem is bounded with the finite optimum $f_0(x^*)$ for some optimizer $x^*$. . $$ begin{aligned} min_x &amp;: f_0(x) s.t. &amp;: begin{aligned} &amp;f_i(x) leq 0 i = 1, ...,m &amp;h_i(x) = 0 i = 1, ... ,p end{aligned} end{aligned} tag{P} $$ . . Note: The $f_i$&#8217;s and the $h_i$&#8217;s in the constraints must necessarily be convex in order for their sublevel-sets, and hence the problem itself, to be convex. However, the equality constraints may be given as $Ax = b$ in some other sources. These representations are almost equivalent. The $0$-th level-set of $Ax - b$ is indeed a convex set. However, $h_i$&#8217;s in the equality constraints $h_i(x) = 0$ need not be linear for the their $0$-th level-set $ { x : h_i(x) = 0 }$ to be convex. For example, in $ mathbb{R}$, $x^2 = 0$ does represent a convex level-set. Note, however, that $x^2 = 0$ can be reduced to $x = 0$ which is, indeed, linear. The notion of quasi-linearity is what&#8217;s needed here but, in practice, we simply define a general convex problem as having only linear equality constraints. Doing so assists in the analysis of problems and in the development of computational methods. . Since optimizing an unconstrained problem is considerably easier than optimizing a constrained problem, we seek to augment the constrained problem into an equivalent unconstrained problem. . The idea is to penalize infeasible $x$ using functions that express our displeasure for certain choices. . At first we use the infinitely-hard penalty functions $ mathbb{1}_-$ and $ mathbb{1}_0$ which are defined as follows: . $$ mathbb{1}_-(u) = begin{cases} begin{aligned} &amp;0 &amp; textrm{if} u leq 0 &amp; infty &amp; textrm{if} u &gt; 0 end{aligned} end{cases}$$ $$ mathbb{1}_0(u) = begin{cases} begin{aligned} &amp;0 &amp; textrm{if} u = 0 &amp; infty &amp; textrm{if} u ne 0 end{aligned} end{cases}$$ . Then the equivalent unconstrained problem can be stated as: . $$ min_x: mathcal{J}(x)$$ . where $ mathcal{J}(x) = f_0(x) + sum_{i=1}^m mathbb{1}_-(f_i(x)) + sum_{i=1}^p mathbb{1}_0(h_i(x))$. . Equivalently, we can express the objective $ mathcal{J}(x)$ as: . $$ mathcal{J}(x) = begin{cases} begin{aligned} &amp;f_0(x) textrm{if $x$ is feasible} &amp; infty textrm{otherwise} end{aligned} end{cases}$$ . # with a simple constraint set . Informally, if $ hat x$ is chosen s.t one or more of the constraints are broken then the minimization incurs an infinitely positive penalty. Therefore, such a $ hat x$ will never be selected over any feasible choice $x$ which gives a finite value $f_0(x)$. Moreover, by optimality of $x^*$ in the original problem, we have $f_0(x) leq f_0(x^*) forall x$. So, the optimum of $ mathcal{J}(x)$ will also be $f_0(x^*)$. . That is: . $$ min_x mathcal{J}(x) = f_0(x^*) tag{1}$$ . Moreover, since the optimizer $x^*$ for the original problem is feasible, $ mathcal{J}(x^*) = f_0(x^*)$ by definition. It follows from $(1)$ that: . $$ mathcal{J}(x^*) = min_x mathcal{J}(x) tag{2.1}$$ . Or, equivalently: . $$x^* = arg min_x mathcal{J}(x) tag{2.2}$$ . $(1)$ says that it suffices to minimize the unconstrained objective $ mathcal{J}(x)$ instead of the original problem since doing so results in $f_0(x^*)$, the optimum of the unconstrained problem. $(2.1)$ and $(2.2)$, on the other hand, say that it suffices to find an optimizer $x^*$ of the unconstrained problem, since such a point will also be an optimizer of the constrained problem. . As we know, the local optima of unconstrained problems occur at their stationary points which can be easily identified using the unconstrained optimality condition. . Unconstrained Optimality Condition: &nbsp; If $x^*$ is an optimizer of the unconstrained objective $f_0(x)$ then $ nabla f_0(x^*) = 0$. That is $x^*$ is a stationary point of $f_0(x)$. . Once the stationary points have been found, a global minimizer can be identified among them simply by evaluating the objective at each stationary point. . However, we&#39;re immediately beset by a problem. We cannot find the gradient of $ mathcal{J}(x)$ and set it to zero because the infinitely-hard penalty functions are discontinuous and non-differentiable. That is, $ nabla mathcal{J}(x)$ simply does not exist. . To sidestep this difficulty we use linear relaxations instead of $ mathbb{1}_-$ and $ mathbb{1}_0$. . The Lagrangian, Dual Variables, and the Dual Function . The Lagrangian linear relaxation, sometimes simply referred to as the Lagrangian, is: . $$ mathcal{L}(x, lambda, mu) = f_0(x) + sum_{i=1}^m lambda_i f_i(x) + sum_{i=1}^p mu_i h_i(x)$$ $$ textrm{where} lambda geq 0$$ . We call the $ lambda_i$&#39;s the Lagrange multipliers corresponding to the inequality constraints, and the $ mu_i$&#39;s those corresponding to the equality constraints. The vectors $ lambda$ and $ mu$, composed of these Lagrange multipliers, are called the Lagrange multiplier vectors or, for reasons that will soon become apparent, the dual variables. . . Note: In some sources, the Lagrangian is simply stated as $ mathcal{L}(x, lambda) = f_0(x) + sum_{i=1}^n lambda_i f_i(x)$. Indeed, by separating the equality constraints $h_i(x) = 0$ into $h_i(x) leq 0$ and $-h_i(x) leq 0$, we can transform a problem with equality constraints into one with only inequality constraints. So, this formulation of the Lagrangian is still general enough to account for problems with equality constraints. . A Lagrangian Lower-Bound . Not only does the Lagrangian relax the unconstrained augmentation of the constrained problem, it also plays a natural role in the formulation of the dual problem as promised. . The first thing to note about the Lagrangian is that the coordinate-wise $ lambda geq 0$ condition is crucial. This is because, in the event that an inequality constraint is violated, say $f_i(x) &gt; 0$, the corresponding $ lambda_i$ must be non-negative in order to apply a positive penalty to the minimization. . On the other hand, $ mu$ is free to assume any value since the equality constraints can be violated in either direction and both scenarios must be penalized. . The second thing to note about the Lagrangian is that, even though it applies a positive penalty that scales linearly in the severity of the violation, this penalty is, nevertheless, not as severe as the infinite penalty applied in $ mathcal{J}(x)$. Also, in the Lagrangian, we may actually be rewarding feasible choices of $x$ that have margin. That is, in the event that $f_i(x) &lt; 0$, $ lambda_if_i(x)$ is a non-positive reward for the minimization problem. . All of this is to say that the Lagrangian is a point-wise lower-bound of the unconstrained problem. That is, the following inequality holds: . $$ mathcal{L}(x, lambda, mu) leq J(x) forall x, lambda geq 0, mu tag{3.1}$$ . This fact is also true by graphing each of the linear and infinite penalties and noticing that $ lambda_i f_i(x) leq mathbb{1}_-(f_i(x))$ and $ mu_i h_i(x) leq mathbb{1}_0(h_i(x))$ for all $i$ constraints. . Taking $ inf$ w.r.t. $x$ of the LHS in $(3.1)$, we obtain something of interest. . $$ inf_x mathcal{L}(x, lambda, mu) leq J(x) forall x, lambda geq 0, mu tag{3.2}$$ . . Note: The Lagrangian may not attain its $ min$ w.r.t. $x$, in which case the LHS is simply $- infty$. We shall see later, once we define the dual function and the duality gap, that this corresponds to the dual function being $- infty$ $ forall lambda geq0, mu$ and the duality gap being $ infty$. In a sense, this is a useless lower bound. So, for now, we assume the interesting case in which the minimum is attained and thus $ inf_x mathcal{L}(x, lambda, mu) = min_x mathcal{L}(x, lambda, mu)$. . Designating the original problem as the primal, we call $g( lambda, mu) := min_x mathcal{L}(x, lambda, mu)$ the dual function because it exhibits the property of weak duality. That is, per $(3.2)$, any feasible value of $g( lambda, mu)$ is a lower-bound for any feasible value of the primal. . Taking min of both sides in $(3.1)$, we have a more specific flavor of weak duality. . $$g( lambda, mu) leq min_x mathcal{J}(x) forall lambda geq 0, mu$$ . And, since $ mathcal{J}(x^*) = f_0(x^*) = min_x mathcal{J}(x)$, we have: . $$g( lambda, mu) leq f_0(x^*) forall lambda geq 0, mu tag{3.3}$$ . That is, any feasible value of the dual is a lower-bound for the primal optimum. . Maximizing both sides of $(3.3)$ by noticing that the RHS is a constant, and by assuming the LHS attains its $ max$ we get an even more specific flavor of weak duality. . $$ max_{ lambda geq 0, mu} g( lambda, mu) leq f_0(x^*) tag{3.4}$$ . That is, the dual optimum is a lower-bound for the primal optimum. . From here we move, quite naturally, to defining the dual problem. . The Lagrange Dual Problem . It&#39;s natural, to ask what the tightest lower bound on the primal optimal value $f_0(x^*)$ is. This amounts to finding the values $ lambda^* geq 0$, and $ mu^*$ for which $g( lambda^*, mu^*)$ is maximized. We call this the Lagrange dual problem or, simply, the dual problem. . It can be stated as: . $$ begin{aligned} max_{ lambda, mu} &amp;: g( lambda, mu) s.t. &amp;: lambda geq 0 end{aligned} $$ . Looking at the above, it becomes immediately clear why we were motivated to call $ lambda$, and $ mu$ the dual variables. They are the variables of the dual problem. . Weak Duality and Interpretations . We now return to the general setting of constrained optimization. . We&#39;ve already seen weak duality formulated as $(3.2)$, $(3.3)$, and $(3.4)$. But, there&#39;s yet another, more symmetric, formulation of weak duality. . Suppose $x^*$ and $( lambda^*, mu^*)$ are primal-dual optimal. Then from $(3.4)$ we have weak duality in terms of the primal and dual optima $g( lambda^*, mu^*)$ as: . $$g( lambda^*, mu^*) leq f_0(x^*) tag{3.5}$$ . But since $g( lambda^*, mu^*)$ is the solution to the dual, and $g( lambda, mu) = min_x mathcal{L}(x, lambda, mu)$: . $$g( lambda^*, mu^*) = max_{ lambda geq 0, mu} left { min_x mathcal{L}(x, lambda, mu) right } tag{4.1}$$ . Similarly, it can be shown that: . $$f_0(x^*) = min_x left { max_{ lambda geq 0, mu} mathcal{L}(x, lambda, mu) right } tag{4.2}$$ . To see this, note that for some $x$ fixed by the outer minimizer, maximizing the Lagrangian over $ lambda geq 0$ and $ mu$ recovers $ mathcal{J}(x)$. . If all of the inequality constraints are respected, that is $f_i(x) leq 0$ $ forall i$, then, in order to maximize the Lagrangian, the best we can do is set $ lambda_i = 0$ $ forall i$. In case any inequality constraint is violated, that is $f_i(x) &gt; 0$ for some $i$, the result of maximizing the Lagrangian can be made $ infty$ by choosing $ lambda_i rightarrow infty$ and $ lambda_j = 0$ $ forall j ne i$. . Using similar logic, if all equality constraints are respected then $h_i(x) = 0$ $ forall i$. In this case $ mu_i$ can be chosen to be any value. If, on the other hand, some equality constraint is violated then $h_i(x) ne 0$ for some $i$. By choosing $ mu_i rightarrow pm infty$, where the sign depends on the direction of the violation, the result can be made $ infty$. . Thus we have shown that: . $$ begin{aligned} max_{ lambda geq 0, mu} mathcal{L}(x, lambda, mu) &amp;= begin{cases} begin{aligned} &amp;f_0(x) textrm{if $x$ is feasible} &amp; infty textrm{otherwise} end{aligned} end{cases} &amp;= mathcal{J}(x) end{aligned}$$ . Now, since $x^*$ is the solution to the primal $(P)$ and $ min_x J(x) = f_0(x^*)$ we have $(4.2)$ as promised. . Then, weak duality can be stated as: . $$ max_{ lambda geq 0, mu} left { min_x mathcal{L}(x, lambda, mu) right } leq min_x left { max_{ lambda geq 0, mu} mathcal{L}(x, lambda, mu) right } tag{3.6}$$ . The Max-Min Inequality . The inequality expressed as $(3.6)$ is, in fact, a general result in mathematics called the Max-Min Inequality. To summarize, the Max-Min Inequality makes no assumptions about the function. It&#39;s simply true for all functions of the form $f: X times Y rightarrow mathbb{R}$, and it states that: . $$ sup_{x in X} left { inf_{y in Y} f(x,y) right } leq inf_{y in Y} left { sup_{x in X} f(x,y) right }$$ . Since no assumption is made on $f$, the inequality also holds for the Lagrangian, $ mathcal{L}$. And, since we&#39;re in the special case where the optimal values of the primal and the dual are assumed to exist, the functions attain the respective optima. That is, we can replace $ sup$ and $ inf$ in the inequality with $ max$ and $ min$ which obtains the promised symmetric formulation of weak duality as $(3.6)$. . We can now prove weak duality through a non-optimization lens simply by proving the Max-Min Inequality. . For any $f$, and $x in X$, $y in Y$ we have: . $$f(x,y) leq sup_y f(x,y) forall x$$ . The right hand side is now only a function of $x$, so minimizing both sides w.r.t. $x$ yields: . $$ inf_x f(x,y) leq inf_x left { sup_y f(x,y) right } forall y$$ . The right hand side is now a constant, so maximizing both sides w.r.t. $y$ results in the desired conclusion. . $$ sup_y left { inf_x f(x,y) right } leq inf_x left { sup_y f(x,y) right }$$ . . Note: As we can see, the Max-Min Inequality proof mirrors the steps taken to obtain $(3.2)$ through $(3.4)$ from $(3.1)$. In fact, $(3.1)$ is of form $f(x,y) leq sup_y f(x,y) forall x$, the first step of the Max-Min Inequality proof, since, in $(3.1)$, $J(x)$ is, as shown earlier, equivalent to $ max_{ lambda geq 0, mu} L(x, lambda, mu)$. . Game-Theoretic Interpretation . The Max-Min Inequality is perhaps best understood intuitively as a game between two adversarial optimizers. . The LHS of the Max-Min Inequality can be interpreted as the following game. First, the outer maximizer, player $Y$, fixes its choice $y$. Then, the inner minimizer, player $X$, chooses $x_y = arg inf_x f(x,y)$ which depends on the outer&#39;s choice of $y$. Suppose $y^* = arg inf_y f(x,y)$ is what player $Y$&#39;s choice would have been were it to act independently of the actions of player $X$. We can imagine a scenario in which the score $f(x_{y^*}, y^*)$ is less than the score $f(x_y, y)$ for some other choice of $y$. So, player $Y$ cannot do as well as it would&#39;ve done independently, whereas player $X$ is free to do its best. Hence, player $X$, the second player, restricts the choices of player $Y$, the first player. . If the goal is to score low then player $X$ has the advantage by playing second turn. Conversely, if the goal is to score high player $Y$ has the advantage if it goes after player $X$. This is exactly what the Max-Min Inequality says. . Strong Duality . Strong duality is the case in which the primal and the dual optima agree strictly. . $$g( lambda^*, mu^*) = f_0(x^*) tag{5.1}$$ . Alternatively, in its Max-Min characterization: . $$ max_{ lambda geq 0, mu} left { min_x mathcal{L} (x, lambda, mu) right } = min_x left { max_{ lambda geq 0, mu} mathcal{L} (x, lambda, mu) right } tag{5.2}$$ . Another common way to say a problem is strongly dual is to say its duality gap is zero. The duality gap is defined as the difference between the primal and dual optima, that is $f_0(x^*) - g( lambda^*, mu^*)$. This characterization of strong duality follows immediately from the first definition of strong duality as it&#39;s stated in $(5.1)$. . Optimization problems that exhibit the property of strong duality are called strongly dual. . As mentioned briefly in the introduction, strong duality gives applied scientists the ability to solve an equivalent, usually easier, dual optimization problem instead of the primal one which may be difficult to solve. Strong duality also obtains powerful optimality conditions which allow us to check if suspected optima are, indeed, optimal. We will soon make both of these claims rigorous but, for now, it&#39;s enough to think of them as benefits of strong duality so that we understand the value in knowing, in advance, whether or not a given problem is strongly dual. . We shall see that all linear programs are strongly dual by a direct proof. When it comes to non-linear optimization, however, strong duality is not a guarantee. The good news is that sufficient conditions for strong duality do exist and will be provided next. . Slater&#39;s Condition - Sufficient Condition for Strong Duality . Note that the existence of a primal optimal $x^*$ is needed in order to talk about strong duality at all. To show this, suppose the primal is feasible but unbounded. Then $ not exists x^*$ that is primal optimal. Suppose further that the dual is feasible. Then $ exists ( lambda, mu)$ such that weak duality obtains $g( lambda, mu) leq f(x) forall x$. That is $g( lambda, mu)$ is a lower-bound of the primal objective. This contradicts the assumption of primal unboundedness. To avoid this contradiction, it must be the case that primal unboundedness $ implies$ dual infeasibility. But if the dual is infeasible there can be no talk of strong duality since the dual optimal does not exist. . While the rare non-convex problem could exhibit strong duality, the latter is mostly a property enjoyed by convex problems. However, not all convex problems are strongly dual. There are many results that establish conditions on the problem, beyond convexity and the existence of a primal optimal, under which strong duality holds. These conditions are called constraint qualifications. In this section we will explore such conditions for convex problems and discuss the specific case of linear programs. . One of these constraint qualification conditions is Slater&#39;s condition. . Slater&#39;s Condition: &nbsp; $ exists hat x$ s.t. $f_i( hat x) &lt; 0$, and $h_i( hat x) = 0$ $ forall i$. . Informally, Slater&#39;s condition says that the existence of a feasible point which has margin w.r.t. all the inequality constraints is needed in addition to convexity. In even simpler terms, the feasible region must have an interior point. . The sufficient condition for strong duality in convex problems is then: . Sufficient Condition for Strong Duality: &nbsp; Any convex optimization problem satisfying Slater&#39;s condition has zero duality gap. . The proof of this is beyond what we&#39;re trying to accomplish in this post. . A weaker constraint qualification condition guarantees strong duality in the case of linear constraints. If $k$ of the $m$ inequality constraints are linear then the condition becomes: . $$ begin{aligned}f_i( hat x) &amp; leq 0, i = 1,...,k, f_i( hat x) &amp;&lt; 0, i = k+1,...,m, h_i( hat x) &amp;= 0, i = 1,...,p end{aligned} $$ . In other words, the linear constraints need not have margin. . Note that if all the constraints are linear, which is the case in linear programming, the above constraint qualification reduces to feasibility. . So, while a sufficient condition of strong duality in convex programs is, both, the existence of a feasible interior point and a primal optimal, the situation is simpler in linear programs. Since a primal optimal for a linear program is also feasible, it satisfies the weaker constraint qualification. Thus, for a linear program to be strongly dual, the existence of a primal optimal suffices. . The Max-Min Equality . Just as weak duality is the Max-Min Inequality in disguise, strong duality is the Max-Min Equality which, in general, holds for functions $f: X times Y rightarrow mathbb{R}$ that have additional structure. Roughly speaking, when $f$ is saddle-shaped, that is convex in one variable and concave in the other, the Max-Min Inequality holds with strict equality. . The following theorem, which we offer without proof, translates this result into the setting of optimization. . Saddle Point Theorem: &nbsp; If $x^*$ and $( lambda^*, mu^*)$ are primal and dual optimal solutions for a convex problem which satisfies Slater&#39;s condition, they form a saddle point of the associated Lagrangian. Furthermore, if $(x^*, ( lambda^*, mu^*))$ is a saddle point of a Lagrangian, then $x^*$ is primal optimal and $( lambda^*, mu^*)$ is dual optimal for the associated problem, and the duality gap is zero. . . Note: This theorem should not be taken as a certificate of strong duality. If the Lagrangian is saddle-shaped then the associated problem is strongly dual, however the converse is not true. Since not all strongly dual problems are convex problems which satisfy Slater&#8217;s condition, if a problem is strongly dual it is not guaranteed that its Lagrangian is saddle-shaped. . In keeping with the game theoretic intuition developed in the section on weak duality, one can imagine a game in which the first player&#39;s optimal choice is independent of the second player&#39;s actions. In such a game, both players are free to play their best strategies and, consequently, the order of play is not important. . An Easier Dual Problem . Let&#39;s further qualify what we mean when we say strong duality gives an equivalent, usually easier, problem to solve. . At the start of this post we considered a general convex program. However, everything we&#39;ve discussed about Lagrangian duality applies to non-convex problems too. Suppose the primal problem is non-convex. The task is that of finding the primal optimum: . $$f_0(x^*) = min_x left { max_{ lambda geq 0, mu} mathcal{L} (x, lambda, mu) right }$$ . But maximizing the Lagrangian over $ lambda geq 0$ and $ mu$ for a fixed $x$, recovers $ mathcal{J}(x)$: a non-differentiable objective. So, we cannot use the unconstrained optimality condition in finding the stationary points of $ mathcal{J}(x)$ which is what&#39;s required in the next step. . Meanwhile, the dual problem is that of finding the dual optimum: . $$g( lambda^*, mu^*) = max_{ lambda geq 0, mu} left { min_x mathcal{L} (x, lambda, mu) right }$$ . Minimizing the Lagrangian over $x$ for fixed $ lambda geq 0$ and $ mu$ may still be a difficult problem but, at least, it lends itself to using the method of unconstrained optimization. Moreover, the resulting dual function $g( lambda, mu) = min_x mathcal{L}(x, lambda, mu)$ is a point-wise minimum of linear functions in $ lambda$ and $ mu$, so its always concave in those variables. Meanwhile, the constraint $ lambda geq 0$ is a simple convex, in fact linear, constraint. So, overall, the dual problem is a convex problem regardless of the convexity of the primal. . Solving the convex dual problem is usually easier that solving the non-convex primal. However, even if the primal is a convex problem to begin with, the dual may still be easier to solve. The primal could have more variables than constraints, in which case its dual is a problem with more constraints than variables making it easier to solve. . So, in the settings described above, if strong duality holds we&#39;ve found an easier approach to the primal problem. If strong duality fails to hold then, at the very least, we&#39;ve found a useful lower-bound to the primal optimum. . Optimality Conditions . Strong duality also obtains two powerful optimality conditions known as stationarity condition and complementary slackness. These are often bundled into the Karush–Kuhn–Tucker (KKT) Conditions which will be provided shortly. . Stationarity Condition . In the section titled An Easier Dual Problem we mentioned that the dual problem is that of finding the dual optimal value: . $$g( lambda^*, mu^*) = max_{ lambda geq 0, mu} left { min_x mathcal{L} (x, lambda, mu) right }$$ . If strong duality holds, this dual optimum agrees with the primal optimum. That is: . $$g( lambda^*, mu^*) = f_0(x^*)$$ . Turns out there&#39;s more that can be said. As we saw earlier optimizing the unconstrained objective $ mathcal{J}(x)$ not only resulted in the primal optimum $f_0(x^*)$ for some optimal $x^*$ of the constrained problem, the very same point $x^*$ itself turned out to be an optimizer of $ mathcal{J}(x)$. Similarly, we can show that the primal optimum $x^*$ for some primal-dual optimal pair $(x^*, ( lambda^*, mu^*))$ optimizes $ mathcal{L}(x, lambda^*, mu^*)$. In other words, the primal optimum $x^*$ is a stationary point of the Lagrangian at the dual optimum $( lambda^*, mu^*)$. . That is: . $$x^* = arg min_x mathcal{L} (x, lambda^*, mu^*) tag{6.1}$$ . Or, equivalently: . $$ min_x mathcal{L}(x, lambda^*, mu^*) = mathcal{L}(x^*, lambda^*, mu^*) tag{6.2}$$ . We can think of $(6.1)$ and $(6.2)$ as the analogs of $(2.1)$ and $(2.2)$ for the Lagrangian. This is exactly what we&#39;ve been digging for. Recall that the original motivation in augmenting the constrained problem into $ mathcal{J}(x)$ was to find the former&#39;s optimizer using the unconstrained optimality condition on $ mathcal{J}(x)$. $(2.1)$ or $(2.2)$ would then guarantee that the optimizer of $ mathcal{J}(x)$ we found was, itself, the optimizer of the original problem. Failing that, we relaxed $ mathcal{J}(x)$ into $ mathcal{L}(x, lambda, mu)$ hoping we can still do the same. $(6.1)$ and $(6.2)$ guarantee we can. They say that the optimizer $x^*$ of the original problem can be found by optimizing the unconstrained objective $ mathcal{L}(x, lambda^*, mu^*)$ and, since the latter is everywhere differentiable w.r.t. $x$, we can now proceed. . In practice, however, $(6.1)$ and $(6.2)$ only give us a way to solve for a primal optimal $x^*$ directly if a dual optimal $( lambda^*, mu^*)$ is already known. That is, any time the dual problem is easier to solve than the primal. . More generally, this fact gives us a way to check if a pair $(x^*,( lambda^*, mu^*))$ is primal-dual optimal – an optimality condition known as stationarity condition. . Stationarity Condition: &nbsp; Suppose $x^*$ and $( lambda^*, mu^*)$ are primal-dual optimal for a strongly dual problem. Then: $$ nabla_x f_0(x^*) + sum_i^m lambda^*_i nabla_xf_i(x^*) + sum_{i=1}^p mu^*_i nabla_xh_i(x^*) = 0$$ . The stationary condition is obtained simply by an application of the unconstrained optimality condition to $ mathcal{L}(x, lambda^*, mu^*)$. . $$ nabla_x mathcal{L} (x^*, lambda^*, mu^*) = 0$$ . Then, expanding the LHS gives: . $$ nabla_x f_0(x^*) + sum_i^m lambda^*_i nabla_xf_i(x^*) + sum_{i=1}^p mu^*_i nabla_xh_i(x^*) = 0$$ . For the sake of completeness, since we stated them without offering a proof, let&#39;s prove the equivalent claims $(6.1)$ and $(6.2)$ from which stationarity condition ultimately follows. . Proof of Claims (6.1) and (6.2) . Suppose $x^*$ and $( lambda^*, mu^*)$ are primal-dual optimal for a strongly dual problem. . The following point-wise inequality holds in general since its LHS is a minimization over $x$ and its RHS is a maximization over $( lambda, mu)$ of the Lagrangian. . $$g( lambda, mu) leq mathcal{L}(x, lambda, mu) leq mathcal{J}(x) forall x, lambda geq 0, mu$$ . It is also, in particular, true for the primal-dual optimal pair. That is: . $$g( lambda^*, mu^*) leq mathcal{L}(x^*, lambda^*, mu^*) leq mathcal{J}(x^*) tag{7.1}$$ . However, $ mathcal{J}(x^*) = f_0(x^*)$ and, by strong duality, $g( lambda^*, mu^*) = f_0(x^*)$. Hence, $g( lambda^*, mu^*) = mathcal{J}(x^*)$ and $(7.1)$ is actually the equality. . $$ mathcal{L}(x^*, lambda^*, mu^*) = g( lambda^*, mu^*) tag{7.2}$$ . Substituting, the definition of the dual function for the RHS of $(7.2)$, we get: . $$ mathcal{L}(x^*, lambda^*, mu^*) = min_x mathcal{L}(x, lambda^*, mu^*)$$ . Which is exactly $(6.2)$ and, by equivalence, also $(6.1)$. . Complementary Slackness . Strong duality also obtains another optimality condition known as complementary slackness (CS). . Complementary Slackness (CS): &nbsp; Suppose $x^*$ and $( lambda^*, mu^*)$ are primal-dual optimal for a strongly dual problem. Then: $$ lambda^*_i f_i(x^*) = 0 forall i$$ . Informally, if a primal constraint at an optimal $x^*$ is loose, that is $f_i(x^*) ne 0$, then its corresponding dual variable $ lambda^*_i$ in the dual optimal $ lambda^*$ must be zero. Conversely, if the dual variable $ lambda_i^*$ is positive then the corresponding constraint must be tight. . Note that if a primal constraint is tight at $x^*$, complementary slackness tells us nothing about its corresponding dual variable. . Proof of Complementary Slackness . Suppose $x^*$ and $( lambda^*, mu^*)$ are primal-dual optimal for a strongly dual problem. . Expanding the RHS we obtain: . $$ begin{aligned} f_0(x^*) &amp;= g( lambda^*, mu^*) &amp;= min_x mathcal{L}(x, lambda^*, mu^*) &amp;= mathcal{L}(x^*, lambda^*, mu^*) &amp;= f_0(x^*) + sum_{i=1}^m lambda_i^* f_i(x) + sum_{i=1}^p mu_i^* h_i(x^*) &amp; leq f_0(x^*) end{aligned} tag{8.1} $$ . The first equality holds by strong duality, the second holds by the definition of the dual function, the third equality holds by $(6.2)$, and the fourth is true by expansion of $ mathcal{L}(x^*, lambda^*, mu^*)$. . To see why the last inequality holds, note that: . $$ sum_{i=1}^p mu_i^* h_i(x^*) = 0$$ . since, by feasibility of $x^*$, $h_i(x^*) = 0 forall i$. Then again, by feasibility of $x^*$, we have: . $$f_i(x^*) leq 0 forall i tag{8.2}$$ . Furthermore, by construction of the Lagrangian, $ lambda geq 0$. So, together with $(8.2)$, we have: . $$ sum_{i=1}^m lambda^*_i f_i(x^*) leq 0$$ . But taken altogether $(8.1)$ says $f_0(x^*) leq f_0(x^*)$ which can only hold through strict equality. . Then it must be the case that $ sum_{i=1}^m lambda^*_i f_i(x^*) = 0$ . Being a sum of non-positive terms, $ sum_{i=1}^m lambda^*_i f_i(x^*) = 0$ if and only if . $$ lambda^*_i f_i(x^*) = 0 forall i tag{8.3}$$ . which is complementary slackness. . Karush-Kuhn-Tucker (KKT) Conditions . As mentioned above, complementary slackness and stationarity condition are often bundled into the KKT Conditions. . In the absence of strong duality the KKT Conditions are necessary but insufficient for optimality. However, for problems which are strongly dual the KKT Conditions become a certificate of optimality. That is, they are both necessary and sufficient. . KKT Conditions: &nbsp; The primal-dual pair $(x^*, ( lambda^*, mu^*))$ satisfies the KKT conditions if the following hold:&nbsp; . $ nabla_x f_0(x^*) + sum_{i=1}^m lambda^*_i nabla_xf_i(x^*) + sum_{i=1}^p mu^*_i nabla_xh_i(x^*) = 0$ | $ lambda^*_if_i(x^*) = 0 forall i$ | $g_i(x^*) leq 0 forall i$ | $h_i(x^*) = 0 forall i$ | $ lambda^* geq 0$ | We recognize KKT-1 as the stationarity condition, and KKT-2 as complementary slackness. KKT-3 through 5 simply ensure primal-dual feasibility. . Primal-dual pairs which satisfy the KKT Conditions are called KKT pairs. . . Note: These conditions only apply to problems with differentiable objective and constraints. For the case in which one or more of the objective or constraints is non-differentiable, there is an easy generalization of the KKT conditions using sub-differentials. However, sub-differentials are beyond the scope of this post. . As promised, KKT Conditions together with strong duality obtain the following certificate of optimality. . Certificate of Optimality: &nbsp; If strong duality holds, then $x^*, ( lambda^*, mu^*)$ are primal-dual optimal if and only if they are a KKT Pair. . We have already shown one direction of this in the sections on stationarity condition and complementary slackness, where we proved that being a primal-dual optimal pair in a strongly convex problem guarantees $(x^*, ( lambda^*, mu^*))$ is also a KKT pair. . Showing the other direction affords us with a geometric viewpoint of the KKT conditions. Farka&#39;s Lemma, which we will shortly provide, is what underpins this result. We will offer this proof for the specific case of linear programs. In it, we will construct a dual variable from a primal optimal $x^*$ by enforcing the KKT conditions, and show that the dual variable obtained through this construction is, in fact, dual optimal. . Something to note in the general case of this proof is that the existence of a primal optimal $x^*$ guarantees, by construction of a KKT pair, the existence of $( lambda, mu)$ that are dual feasible. However, it is strong duality which guarantees that the $( lambda, mu)$ obtained by this construction is also dual optimal. . Generalization of Unconstrained Optimization . UNREVISED SECTION BEGINS . The KKT conditions represent a strict generalization of the unconstrained optimality condition for use in constrained problems. . Note that if there are no constraints, the KKT conditions simply reduce to the familiar unconstrained optimality condition: . $$ nabla_x f_0(x^*) = 0$$ . In order to discuss optimality in constrained problems, we must first define the concept of a feasible direction. . . Feasible Direction: &nbsp; A unit vector $d$ is called a feasible direction at any $x$ if $x + epsilon d$ remains feasible for $ epsilon &gt; 0$ small enough. . Then, we can generalize the unconstrained optimality condition by using Taylor Expansion as follows. . For small enough $ epsilon &gt; 0$, we can estimate $f_0(x^* + epsilon d)$, where $x^*$ is optimal, for any feasible $d$ by its linear approximation: $$f_0(x^* + epsilon d) = f_0(x^*) + epsilon nabla f_0(x^*)^Td$$ . But since $x^*$ is optimal, we have: $$ begin{aligned} f_0(x^*) &amp; leq f_0(x^* + epsilon d) &amp; = f_0(x^*) + epsilon nabla f_0(x^*)^Td end{aligned} $$ . Which means $ nabla f_0(x^*)^Td geq 0$. And since $d$ was an arbitrary feasible direction, the result holds for all feasible directions $d$. . . Constrained Optimality Condition: &nbsp; If $x^*$ is an optimizer of $f_0$ over some constraint set then, for any feasible direction $d$ at $x^*$, $ nabla f_0(x^*)^Td geq 0$. . In words, the directional derivative of the objective function in any feasible direction at the optimizer must be non-negative. This ensures that moving in any allowable direction does not improve the objective. . We&#39;ve already seen that being a primal-dual optimal pair guarantees that $(x^*, ( lambda^*, mu^*))$ is also a KKT Pair. But, in showing that, we did not use the constrained optimality condition at $x^*$. Doing so is worth it, however, because it provides a key geometric insight. So, let&#39;s show that if $x^*$ satisfies the constrained optimality condition then its KKT Pair exists. . If a particular constraint is loose at $x^*$ then taking a small enough step in any direction from $x^*$ does not violate it. Formally, if $f_i(x^*) &lt; 0$, then $f_i(x^* + epsilon d) leq 0 forall d$. So, loose constraints do not pose any restrictions on the feasible directions. . However, if a constraint is tight at $x^*$, that is $f_i(x^*) = 0$, then we must be careful not to violate it. For small enough $ epsilon &gt; 0$, we can estimate $f_i(x^* + epsilon d)$ by its linear Taylor Expansion as: $$f_i(x^* + epsilon d) = f_i(x^*) + epsilon nabla f_i(x^*)^Td$$ . For feasibility, we want $f_i(x^* epsilon d) leq 0$. So, we require: $$f_i(x^*) + epsilon nabla f_i(x^*)^Td leq 0$$ . But since $f_i$ is tight at $x^*$, $f_i(x^*) = 0$, which leaves us with: $$ nabla f_i(x^*)^Td leq 0 forall i textrm{that are binding at $x^*$}$$ . Clearly the above is a restriction on $d$. The feasible directions can now be stated as: $$d textrm{s.t.} nabla f_i(x^*)^Td leq 0 forall i textrm{that are binding at $x^*$} tag{8.1}$$ . Or, equivalently: $$d textrm{s.t.} - nabla f_i(x^*)^Td geq 0 forall i textrm{that are binding at $x^*$} tag{8.2}$$ . But, since $x^*$ is optimal, by the generalized unconstrained optimality condition: $$ nabla f_0(x^*)^Td geq 0 forall textrm{feasible} d tag{8.3}$$. . That is, for all $d$ as in $(8.2)$. . But together, $(8.2)$ and $(8.3)$ say that $ not exists d$ which defines a separating hyperplane between $ nabla f_0(x^*)$ and $- nabla f_i(x^*)$ for all binding constraints $i$. This means that the only other alternative scenario must be true — $ nabla f_0(x^*)$ must lie in the cone of the $- nabla f_i(x^*)$&#39;s. Incidentally, this is what&#39;s known as a theorem of the alternative, specifically Farka&#39;s Lemma, which will soon be covered in detail. . Formally, $ exists lambda^* geq 0$ s.t. $$ nabla f_0(x^*) + sum_{i in I} lambda^*_i f_i(x^*) = 0 tag{8.4}$$ . Where $I = {i : f_i(x^*) = 0 }$ is the set of active inequality constraints. . But, upon closer examination, $(8.4)$ is exactly $(KKT 1)$, $(KKT 2)$, and $(KKT 5)$ rolled into one condition. The remaining conditions, $(KKT 3)$ and $(KKT 4)$, of course, follow from the assumed feasibility of $x^*$. . We can also show that if $(x^*, ( lambda^*, mu^*))$ is a KKT Pair then $x^*$ is optimal by the constrained optimality condition. Note, however, this says nothing about the dual optimality of $( lambda^*, mu^*)$ if no assumption of Strong Duality is made. The argument is simply reversed. If $ nabla f_0(x^*)$ is in the aforementioned cone then going in any feasible direction makes life worse. Which is exactly what the constrained optimality condition says. This, of course, provides a strong geometric interpretation of the Stationary Condition. . The Dual of an Unconstrained Problem . As mentioned briefly, in the case of certain types of unconstrained problems, the Fenchel-Legendre (FL) Transform is what gives rise to the dual. . First, we define the FL transform which is also known as a convex conjugate for reasons that will soon become apparent. . . FL Transform / Convex Conjugate: &nbsp; The FL Transform or Convex Conjugate of a function $f: mathbb{R}^n rightarrow mathbb{R}$ is: $$f^*(y) = sup_x left {y^Tx - f(x) right }$$ . We note some key properties of the FL Transform. . FL Transform - a Convex Operation . The FL Transform $f^*$ is always convex regardless of the convexity of $f$. . That&#39;s because, for a fixed $x$, $y^Tx - f(x)$ is a linear function in $y$. So, $f^*$ is a point-wise supremum of linear functions, making it convex. . The Case of Involution . The double FL Transform $f^{**}$ does not always recover $f$. To see this fact note that, as an FL Transform of the some function (namely, $f^*$), $f^{**}$ is always convex. Therefore, $f^{**} ne f$ if $f$ is non-convex. . But convexity alone is not enough to guarantee involution. We need an additional condition on $f$, namely that its sub-level sets must be closed, to ensure $f^{**} = f$. . Inverse Gradients . If $f$ has closed sub-level sets and is convex then the gradients of $f$ and $f^*$ are inverses. That is, assuming both $f$ and $f^*$ are differentiable: $$y = nabla f(x) iff x = nabla f^*(y)$$ . Let&#39;s prove the $ implies$ direction. . Suppose $y = nabla f(x)$. By $f$&#39;s convexity: $$f( hat x) geq f(x) + y^T( hat x - x) forall hat x$$ . And so: $$y^T hat x - f( hat x) leq y^T x - f(x) forall hat x$$ . By taking supremum over $x$ and by noting that, since the sub-level sets are closed, the supremum is attained, we obtain: $$f^*(y) = y^T x - f(x)$$ . The desired result follows by taking the gradient of both sides w.r.t. $y$. That is: $$ nabla f^*(y) = x$$ . The $ impliedby$ direction is similar. We start from the assumption that $x = nabla f^*(y)$ and get the desired result by using the involution property $f^{**} = f$. . FL Duality . As mentioned, the FL Transform has a natural role in duality. . Suppose the unconstrained optimization problem is: $$ min_x : f(x) + h(Ax)$$ . Where $f$ and $h$ are convex functions, and $A$ is a matrix representing a bounded linear transformation. . We introduce a dummy variable $y$ and form the artificial constraint $y = Ax$. The problem becomes: $$ begin{aligned} min_{x,y} &amp;: f(x) + h(y) s.t. &amp;: Ax = y end{aligned}$$ . Forming the Lagrangian gives us: $$ mathcal{L}(x,y,z) = f(x) + h(y) + z^T(Ax - y)$$ . Then, the dual function is the following FL Transform: $$ begin{aligned} g(z) &amp;= min_{x,y} mathcal{L}(x,y,z) &amp;= min_{x,y} f(x) + h(y) + z^T(Ax - y) &amp;= min_{x,y} (A^Tz)^Tx + f(x) - z^Ty + h(y) &amp;= min_x left { (A^Tz)^Tx + f(x) right } + min_y left { -z^Ty + h(y) right } &amp;= min_x left { - left((-A^Tz)^Tx - f(x) right) right } + min_y left { - left(z^Ty - h(y) right) right } &amp;= - max_x left { (-A^Tz)^Tx - f(x) right } - max_y left { z^Ty - h(y) right } &amp;= - f^*(-A^Tz) - h^*(z) end{aligned} $$ . And, consequently, the dual problem is: $$ max_z: - f^*(-A^Tz) - h^*(z)$$ . Note that the dual is, indeed, an easy problem since the negative of an FL Transform is always concave regardless of the convexity of $f$ and $h$. So, the dual problem is a maximization of a concave function which is an easy optimization problem. . LINEAR PROGRAMS . BEST WAY TO UNDERSTAND INTUITIVELY COMPLEMENTARY SLACKNESS AND OTHER PROPERTIES OF STRING DUALITY IN KKT CONDITIONS . Weak Duality in Linear Programs . Let&#39;s now focus on linear programs. . Suppose the primal is an LP of the form . $ begin{cases} min_x: c^Tx s.t.: begin{aligned} &amp;Ax geq b &amp;x geq 0 end{aligned} end{cases} $ . By constructing the Lagrangian and going through the steps above we can show its dual is for the form . $ begin{cases} max_p: b^Tp s.t.: begin{aligned} &amp;A^Tp leq c &amp;p geq 0 end{aligned} end{cases} $ . Weak Duality states . Weak Duality: &nbsp; For any primal feasible $x$ and for all dual feasible $p$, $c^Tx geq b^Tp$. . That is, any dual feasible solution $b^Tp$ is a lower bound for all primal feasible solutions $c^Tx$. Conversely, any primal feasible solution $c^Tx$ is an upper bound for all dual feasible solutions $b^Tp$. . Proof of Weak Duality . Let $(p, x)$ be respectively dual-primal feasible. Then $c^Tx = x^Tc geq x^TA^Tp geq b^Tp$. . Strong Duality . LPS THAT HAVE PRIMAL OPTIMALS ARE STRONGLY DUAL BY CONSTRAINT QUALIFICATION BUT THIS PROOF IS A MORE DIRECT PROOF THAT DOES NOT RELY ON THAT. . While Weak Duality is a useful result, the real strength of duality theory lies in Strong Duality. Strong duality is a re-statement of Von Neumann&#39;s Minimax Theorem which lays out the conditions for which the max-min inequality holds with strict equality. Roughly speaking, it holds for functions that are saddle-shaped — convex in one variable and concave in the another. . Instead of proving the Minimax Theorem in the general case, we will stay topical and prove Strong Duality for LP&#39;s. That is, the Minimax Theorem as it pertains to the special case of linear programs... . Strong Duality: &nbsp; If the primal is feasible and bounded with optimal $x^*$ then the dual is also feasible and bounded. Furthermore, a dual optimal $p^*$ is s.t. $c^Tx^* = b^Tp^*$. . To prove Strong Duality, we require Farkas&#39; Lemma. . Farkas&#39; Lemma . Farkas&#39; Lemma belongs to the class of theorems called Theorems of the Alternative — these are a theorems stating that exactly one of two statements holds true. . The lemma simply states that a given vector $c$ is either a conic combination of $a_i$&#39;s for some $i in I$, or it&#39;s separated from their cone by some hyperplane. . We state Farkas&#39; Lemma without offering proof since it has such an obvious geometric interpretation. . Farkas&#39; Lemma: &nbsp; For any vector $c$ and $a_i (i in I)$ either the first or the second statement holds: &nbsp; . $ exists p geq 0$ s.t. $c = sum_{i in I} a_ip_i$ | $ exists$ vector $d$ s.t. $d^Ta_i geq 0 forall i in I$ but $d^Tc &lt; 0$ | . Proof of Strong Duality in LP&#39;s . ANOTHER SUFFICIENT CONDITION FOR STRONG DUALITY IS BEING A LINEAR PROGRAM. THE PROOF BELOW IS INDEPENDENT OF PAST SUFFICIENT CONDITIONS. /// . The proof is by construction. . Suppose $x^*$ is a primal optimal solution. Let the set $I_{x^*} = { i : a_i^Tx^* = b_i }$ be the set of the indices of the active constraints at $x^*$. Our goal is to construct a dual optimal solution $p^*$ s.t. $c^Tx^* = b^Tp^*$. . Let $d$ be any vector that satisfies $d^Ta_i geq 0 forall i in I_{x^*}$. That is, $d$ is a feasible direction w.r.t. to all the active constraints. . A small, positive $ epsilon$-step in the direction of $d$ results in point $x^* + epsilon d$ that&#39;s still feasible. The fact that the step is small is what guarantees no inactive constraints are violated. . Let&#39;s compare the value of the objective at $x^* + epsilon d$ to the value of the objective at $x^*$. . By the assumption that $x^*$ is optimal, we have $c^Tx^* leq c^T(x^* + epsilon d) = c^Tx^* + epsilon c^Td$. Thus, $c^Td = d^Tc geq 0$ . Note: $d^Tc$ is nothing but the directional derivative at the minimizer $x^*$. It is a first-order necessary-condition that the directional derivative in any feasible direction $d$ be non-negative at any minimizer $x^*$. This is analogous to the first-derivative test for scalar-valued functions. So, this result should have been expected... . But since $d$ is a vector s.t. $d^Ta_i geq 0 forall i in I_{x^*}$ and $d^Tc geq 0$, then $d$ does not separate $c$ from the cone of the $a_i$&#39;s. And since $d$ was arbitrary, this puts us in the setting of Farkas&#39; Lemma. Namely, there exist no vectors $d$ that separate $c$ from the cone. This means the second statement in Farkas&#39; Lemma is violated and the first must be true — $c$ must a conic combination of the $a_i$&#39;s that are active at the minimizer. In other words, $ exists p geq 0$ s.t. $c = sum_{i in I_{x^*}} p_ia_i$. . Note: $c = sum_{i in I_{x^*}} p_ia_i$ should remind us of the Lagrange optimality condition in the general case of convex optimization. Recall that the Lagrange condition states that a point $x^*$ is optimal for a convex problem with objective $f(x)$ and constraints $g_i(x)=0$ if and only if $ exists lambda_i$ for each active constraint s.t. $ nabla f(x^*) = sum_i lambda_i nabla g_i(x^*)$. In fact, Farka&#8217;s lemma is what underpins the Lagrange condition through the assumption that the non-linear objective $f$ and non-linear constraints $g_i$ behave linearly in a small neighborhood of $x^*$. . But $p$ has dimension equal to only the number of active constraints at $x^*$. To be a dual variable at all, it must have dimension equal to the number of all primal constraints. We extend $p$ to $p^*$ by setting all the entries that do not correspond to the active constraints at $x^*$ to be zero. . That is $p^*_i = begin{cases} p_i textrm{if} i in I_{x^*} 0 textrm{if} i notin I_{x^*} end{cases}$. . Now $A^Tp^* = sum_{i} p^*_ia_i = c$, so any feasibility condition in the dual, whether it be $A^Tp leq c$, $A^Tp geq c$, or $A^Tp = c$, is satisfied by $p^*$. . Furthermore, the dual objective at $p^*$ agrees with the primal objective at $x^*$. . $$b^Tp^* = sum_{i} b_ip_i^* = sum_{i in I_{x^*}} b_ip_i^* + sum_{i notin I_{x^*}} b_ip_i^* = sum_{i in I_{x^*}} a_i^Tx^*p_i^* = ( sum_{i in I_{x^*}} p_ia_i^T)x^* = c^Tx^* $$ . However, it still remains to be shown that $p^*$ is dual optimal. . Whenever the primal objective and the dual objective agree on a value, the respective solutions must be primal-dual optimal. This is simply true by Weak Duality, which states that $b^Tp leq c^Tx^*$ $ forall p$. So, $c^Tx^*$ is an upper bound for any dual feasible solution. But the dual is a maximization problem, so the dual optimal must be $p^*$ s.t. $b^Tp^* = c^Tx^*$. . NOTE THAT WE HAVE CONSTRUCTED THE DUAL OPTIMAL BY EXPLICITY SATISFYING THE KKT CONDITIONS IN THE LINEAR CASE. SO THIS IS THE PROOF THAT KKT PAIR =&gt; PRIMAL/DUAL OPTIMAL. . Theorems of the Alternative . As mentioned earlier, these are theorems that describe exclusively disjoint scenarios that together comprise the entire outcome space. Formally, these are theorems of the form $A implies neg B land neg A implies B$ where $A$, and $B$ are logical statements. . Note that theorems of equivalence (i.e. theorems of the form &#39;the following are equivalent - TFAE&#39;) can also be formulated as theorems of the alternative. To say that $A$ and $B$ are equivalent means $ A iff B$. But this breaks down as $A implies B land B implies A$. Letting $ hat B = neg B$ we can rewrite the above as $A implies neg hat B land B implies A$. But, by taking the contrapositive, $B implies A$ becomes $ neg A implies neg B$, which is to say $ neg A implies hat B$. In summary, we have shown that $A iff B$ is equivalent to $A implies neg hat B land neg A implies hat B$. . So, the class of theorems of the alternative is much broader than it appears and includes theorems of equivalence. . Example of a Theorem of the Alternative . To see how we can prove a theorem of the alternative, it helps to state one. . Theorem: &nbsp; Exactly one of the following two statements most hold for a given matrix A.&nbsp; . $ exists x ne 0$ s.t. $Ax = 0$ and $x geq 0$ | $ exists p$ s.t. $p^TA &gt; 0$ | Using a Separation Argument . At the heart of separation arguments lies this simple fact. . Separating Hyperplane Theorem: For any convex set $C$, if a point $ omega notin C$ then there exists a hyperplane separating $ omega$ and $C$. . Farkas&#39; Lemma, for instance, is proved by a separation argument that uses, as its convex set, the conic combination of the $a_i$&#39;s. The conclusion is immediate since in Farkas&#39; Lemma the first statement plainly says that a vector belongs to the convex set, and the second statement plainly says there exists a separating hyperplane between the two. . This is the pattern all separation arguments must follow. However, in general, it may take a bit of work to define the problem-specific convex set and also to show that the two statements are really talking about belonging to this set, and separation from it. However, once these three things are accomplished the proof is complete. . Using this idea, let&#39;s give a proof of the above theorem of the alternative using a separation argument. . Proof . First order of business is to come up with a convex set. . Let&#39;s take $C = { z : z = Ay, sum_i y_i = 1, y geq 0 }$ to be the convex hull of the columns of $A$. . The first statement in the theorem was that $ exists x ne 0$ s.t. $Ax = 0$ and $x geq 0$. . Since $x ne 0$ and $x geq 0$ we can scale as $x$ as $y = alpha x$ until $ sum_i y_i = 1$. . So, the first statement is equivalent to saying the origin belongs to the convex hull $C$ (i.e. $0 in C$) . The second statement was that $ exists p$ s.t. $p^TA &gt; 0$. This is equivalent to saying that all the columns of $A$ lie to one side of the separating hyperplane introduced by $p$. . But all $z in C$ are convex combinations of $A$&#39;s columns. In particular since they&#39;re a convex combination they&#39;re also a conic combination, so all $z in C$ also lie on the same side of the hyperplane. That is $p^Tz &gt; 0 forall z in C$. . But, of course, $p^T0 = 0$ (not $&gt; 0$). So, according to the second statement, the origin is separated from $C$. . This concludes the proof since the two statements must be mutually exclusive. . Using Strong Duality . Strong duality isn&#39;t just a tool for applied science, it has important theoretical uses. For instance, now that we&#39;ve proven it we can use Strong Duality, instead of a separation argument, to prove theorems of the alternative. . Since it gives us feasibility of two different constraint sets, it makes sense to use duality to prove theorems of existence. . Let&#39;s take the aforementioned theorem of the alternative for example... . Proof . To prove the theorem we need to show two things. First, we need to show $1 implies neg 2$, then we need to show $ neg 1 implies 2$. . The $1 implies neg 2$ direction is simple. . Suppose $ exists x ne 0$ s.t. $Ax = 0$ and $x geq 0$. . Then $ forall p (p^TA)x = p^T(Ax) = p^T0 = 0$ (not $&gt; 0$). . We tackle the $ neg 1 implies 2$ direction using duality. . The strategy is to construct a linear program based on $ neg 1$ such that the feasibility of its dual implies $2$. . We can express $ neg 1$ as &#39;$ forall x ne 0$, either $Ax ne 0$ or $x &lt; 0$.&#39; Equivalently, &#39;$x ne 0 implies Ax ne 0$ or $x &lt; 0$.&#39; Taking the contrapositive, statement $1$ becomes &#39;$Ax = 0$ and $x geq 0 implies x = 0$.&#39; . So, let&#39;s form the LP . $ begin{cases} max_x: textbf{1}^Tx s.t.: begin{aligned} &amp;Ax = 0 &amp;x geq 0 end{aligned} end{cases} $ . Note that $x = 0$ is a feasible solution to the LP. Furthermore, assuming statement $1$ guarantees that $x = 0$ is the only feasible solution. Thus, the LP is feasible and bounded. . By Strong Duality, its dual must also be feasible and bounded. . The dual is... . $ begin{cases} min_p: textbf{0}^Tp s.t.: p^TA geq textbf{1} end{cases} $ . ... and since it&#39;s feasible, $ exists p$ s.t. $p^TA geq 1 &gt; 0$ which demonstrates the truth of statement $2$. . Complementary Slackness . Complementary Slackness is a fundamental property that exists between any primal optimal solution and any dual optimal solution. . In the preceding section on Strong Duality we constructed a dual optimal by setting those of its variables that corresponded to the inactive constraints of the primal optimal to be zero. . This is true in general, for all primal-dual optimal pairs. . If a primal&#39;s constraint is loose at a some primal optimal, then the corresponding variable in the dual optimal is zero, and vice versa. . Formally, this can be stated as . Complementary Slackness: if $x$ is primal feasible and $p$ is dual feasible, then $x$ and $p$ are respectively optimal iff:&nbsp; . $(b_i - sum_{j} a_{ij}x_j)p_i = 0 forall i$ | $( sum_{i} a_{ij}p_i - c_j)x_j = 0 forall j$ | If we recall, in the proof of Strong Duality we constructed a dual optimal by setting those of its variables that corresponded to the primal&#39;s slack constraints to be zero. In other words, we constructed a dual optimal in such a way as to satisfy the Complementary Slackness theorem. So, the fact that this generalizes to all primal-dual optima shouldn&#39;t surprise us. . However, the above does not constitute a proof of Complementary Slackness, so let&#39;s offer one. . Take as a starting point the primal-dual pair . $ textrm{P} begin{cases} min_x: c^Tx s.t.: begin{aligned} &amp;Ax geq b &amp;x geq 0 end{aligned} end{cases} $ . $ textrm{D} begin{cases} max_p: b^Tp s.t.: begin{aligned} &amp;A^Tp leq c &amp;p geq 0 end{aligned} end{cases} $ . Proof of Complementary Slackness . Sufficiency $ impliedby$: . Suppose both equalities hold. . Summing each over all $i$&#39;s and $j$&#39;s respectively and adding the results we get . $$ sum_i left(b_i - sum_j a_{ij}x_j right)p_i + sum_j left( sum_i a_{ij}p_i - c_j right)x_j = 0$$ . Which simplifies to . $$ sum_i b_ip_i - sum_i sum_j a_{ij}x_jy_i + sum_j sum_i a_{i,j}y_ix_j - sum_j c_jx_j = 0$$ . Or, in matrix-vector form . $$b^Tp - p^TAx + p^TAx - c^Tx = 0$$ . The middle two terms cancel, and we get $b^Tp = c^Tp$. . By Weak Duality, $x$ and $p$ are primal-dual optimal. . Necessity $ implies$: . Suppose $x$ and $p$ are primal-dual optimal. . By Strong Duality $b^Tp = c^Tx$. . In other words, $b^Tp - c^Tx = 0$. Adding and subtracting the terms canceled in the first part, we can bring the sum to the form . $$b^Tp - p^TAx + p^TAx - c^Tx = 0$$ . Which is, once again, the same as . $$ sum_i left(b_i - sum_j a_{ij}x_j right)p_i + sum_j left( sum_i a_{ij}p_i - c_j right)x_j = 0$$ . But $p$ is dual feasible, so $p_i geq 0 forall i$. And since $x$ is primal feasible, $Ax geq b$ implies $(b_i - sum_j a_{ij}x_j) leq 0 forall i$. . Similarly, $x_j geq 0 forall j$ and $( sum_i a_{ij}p_i - c_j) geq 0 forall j$. . So the above expression is a sum of all non-positive terms that adds up to zero. This can only happen if each term is equal to zero. .",
            "url": "https://v-poghosyan.github.io/blog/optimization/applied%20mathematics/proofs/2022/02/07/Optimization-Duality.html",
            "relUrl": "/optimization/applied%20mathematics/proofs/2022/02/07/Optimization-Duality.html",
            "date": " • Feb 7, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Optimization - Linear Programs and LP Geometry",
            "content": "Introduction . A linear program is a special type of convex optimization problem in $n$-dimensions that has a linear objective and a constraint set that&#39;s a polytope. That is, its constraint set is an intersection of $n$-dimensional linear inequalities (halfspaces) and linear equalities (hyperplanes). . In matrix form, it may be stated as . $ begin{cases} min_x: c^Tx s.t.: begin{aligned} &amp;A_1x leq b_1 &amp;A_2x geq b_2 &amp;A_3x = b_3 end{aligned} end{cases} $ . where $c in mathbb{R}^n$ is the cost vector of the objective function, $x in mathbb{R^n}$ is the decision variable, $A_1 in mathbb{R}^{m times n}$, $b_1 in mathbb{R}^m$ and $A_2 in mathbb{R}^{p times n}$, $b_2 in mathbb{R}^p$ together define the collection of linear inequality constraints, and $A_3 in mathbb{R}^{q times n}$ and $b_3 in mathbb{R}^q$ define the collection of linear equality constraints. . As we will shortly prove, an LP in any form such as the one above can be converted into its standard form . $ begin{cases} min_x: c^Tx s.t.: begin{aligned} &amp;Ax = b &amp;x geq 0 end{aligned} end{cases} dagger $ . Applications . Linear programs are only a small subset of convex optimization problems (in fact, a strict subset of semidefinite programs) but they&#39;re robust enough to model many real-life scenarios. For instance, even though they are continuous optimization problems, due to their geometry — namely the fact that optimal solutions to an LP may occur only at the extreme points of the constraint set — they have a strong combinatorial flavor. This is why LP&#39;s are highly successful at modeling problems that are inherently combinatorial — problems of scheduling, finding the shortest path, modeling a discrete failures scenario, etc. . The reason LP&#39;s are of special interest in the study of optimization is due to the availability of fast algorithms that solve them. So, if a convex optimization problem happens to also be an LP, we can solve it much faster. . Feasibility and Boundedness . There are two ways in which LP&#39;s may fail to have an optimal solution, by either being infeasible or unbounded. Both of these cases are typically uninteresting in practice however they give important theoretical results. It&#39;s also useful to check whether or not a given LP is feasible and bounded before attempting to optimize. . Infeasible LP&#39;s are those LP&#39;s that have an empty constraint set, whereas unbounded LP&#39;s have open constraint sets. However, it&#39;s important to understand that an LP may have an open constraint set without being unbounded. . Consider the two examples below . $$ begin{cases} min_{x_1}: x_1 s.t.: x_1 leq 4 end{cases} tag{1} $$ $$ begin{cases} min_{x_1}: x_1 s.t.: x_1 geq 4 end{cases} tag{2} $$ The first problem is unbounded, since $x_1$ can be taken arbitrarily small. However, the second problem is bounded despite having an open constraint set. The optimal value of $(2)$ is $x_1 = 4$. . Thus, an LP is said to be unbounded not when its constraint polytope is open, but when it&#39;s feasible and has no optimal solution. . Converting to Standard Form . Given any starting point, an LP can be written in standard form $ dagger$. This is useful for standardization of the problem from an algorithmic perspective, and it&#39;s what the Simplex algorithm relies on to solve LP&#39;s. . In the most general case an LP can be stated with inequality constraints going in both directions and equality constraints as follows . $ begin{cases} min_x: c^Tx s.t.: begin{aligned} &amp;A_1x leq b_1 &amp;A_2x geq b_2 &amp;A_3x = b_3 end{aligned} end{cases} $ . But the inequality constraints can always be combined into $Ax leq b$ for $A = [A_1, A_2]$ and $b = [b_1, -b_2]^T$ and relabeled as $A_1$ and $b_1$. . So there&#39;s no qualitative difference between having two types of inequalities versus just one. Simply concatenate and relabel the matrices to get . $ begin{cases} min_x: c^Tx s.t.: begin{aligned} &amp;A_1x leq b_1 &amp;A_2x = b_2 end{aligned} end{cases} $ . The real challenge lies in converting the inequality constraints $A_1x leq b_1$ into equality constraints $A_1x = b_1$. . Introducing Slack Variables . The inequality constraint $A_1x leq b_1$ has slack. Formally, we can define vector $s geq 0$ (component-wise), that bridges the gap between $A_1x$ and $b_1$, that is s.t. $A_1x + s = b_1$. . Since this introduces new variables, we have to represent those in the objective and the equality constraints in a way that doesn&#39;t affect the optimization outcome. . The LP becomes . $ begin{cases} min_x: c^Tx + mathbf{0}^Ts s.t.: begin{aligned} &amp;A_1x + s = b_1 &amp;A_2x + 0s = b_2 &amp;s geq 0 end{aligned} end{cases} $ . This LP is equivalent to the one before. Namely, if the previous optimizer was $x^*$, the optimizer in the new LP is the concatenation $[x^*, b_1 - A_1x]$ which gives the same optimal value in the objective function. . This is almost in standard form, an LP with only equality constraints, and non-negativity constraints. However, the decision variable of this LP is the concatenation $[x,s]^T$, whereas the non-negativity applies to $s$ alone. . The next step is to decompose $x$ as $x = x^+ - x^-$ where $x^+,x^- geq 0$ respectively contain only the positive and only the negative entries of $x$. That is, $x^+$, and $x^-$ have entries $x_i^+ = max {0, x_i }$ and $x_i^- = - min {0, x_i }$. . With this substitution we get . $ begin{cases} min_x: c^Tx^+ - c^Tx^- + mathbf{0}^Ts s.t.: begin{aligned} &amp;A_1x^+ - A_1x^- + s = b_1 &amp;A_2x^+ - A_2x^- + 0s = b_2 &amp;x^+, x^-, s geq 0 end{aligned} end{cases} $ . Which is an LP in standard form $ dagger$. . Geometry of Linear Programs . Take a simple feasible, bounded LP in two dimensions that has a unique solution, draw its polygonal constraint set. Then draw the level sets of the objective function noting that the direction of steepest change (the positive or negative gradient) is perpendicular to the level sets. The conclusion is almost immediate — the unique optimal solution occurs at a vertex (i.e. an extreme point) of the polygonal constraint set. . To formalize this, we need to introduce a few definitions and prove a theorem called The Extreme Point Theorem which can be found towards the end of this post. . Extreme Points - Geometric Definitions . First, let&#39;s give a couple of geometric definitions of an extreme point. . Definition 1: &nbsp; A point $x$ is an extreme point of a polytope $P$ if it is not the convex combination of any other two points in the polytope. . That is, if $ exists y,z in P$ and $ lambda in [0,1]$ s.t. $x = lambda y + (1- lambda)z$ then $x$ is not an extreme point of $P$. . Definition 2: &nbsp; A point $x$ is an extreme point of a polytope $P$ if it is the unique optimum for some cost vector $c$. . That is, if $ exists c in mathbb{R}^n$ s.t. $c^Tx &lt; c^Ty forall y in P$ then $x$ is an extreme point. . Extreme Points - Algebraic Definition . It&#39;s useful to define an extreme point algebraically. To that end, let&#39;s define the concept of a basic feasible solution (BFS). . Suppose we have the polytope $ {x : Ax leq b, Dx = f }$. . Definition: &nbsp; An active constraint at $x$ is a constraint that&#39;s satisfied through strict equality. . That is, the $i$-th constraint is said to be active at x if $a_i^Tx = b_i$. . This can be thought of as $x$ being on the edge of the halfspace defined by $a_i^Tx leq b_i$. . We can also define the active set at $x$ as the set of all active constraints at $x$. . So, the active set at $x$ is $ mathcal{A}_x = { a_i : a_i^Tx = b_i } cup { d_i : d_i^Tx = f_i }$, where $ { d_i : d_i^Tx = f_i }$ is included for completeness. . Basic Feasible Solution . We are now ready to define what it means for a point $x$ to be a basic feasible solution of a linear program. . Definition: &nbsp; The point $x$ is a basic feasible solution (BFS) of the linear program if its active set $ mathcal{A}_x$ contains exactly $n$ linearly independent vectors where $n$ is the dimension of $x$. . Let&#39;s ponder the BFS definition for a minute. . Imagine a closed polytope in 2D. Each of its vertices are defined by, at least, two intersecting lines. It&#39;s possible that a vertex is the result of the intersection of three or more lines, but deleting all but two of those lines will still retain the vertex. In other words, two linearly independent (i.e. non-parallel) constraints define an extreme point in 2D. . The BFS definition is simply a generalization of this insight to $n$-dimensions. . As we will prove shortly, BFS and extreme point are synonymous. In fact, the following are equivalent: . $x$ is an extreme point by Definition 1. | $x$ is an extreme point by Definition 2. | $x$ is a basic feasible solution. | . Matrix-Vector Formulation of Basic Feasible Solutions . Taking as our starting point an LP in standard form $ dagger$ we can characterize basic feasible solutions in matrix-vector form. . Take the standard constraint set $ Omega = { Ax = b, x geq 0 }$ and let&#39;s make a few simplifying assumptions. . $A$ is $m times n$ with $m leq n$. | $A$ is full-rank | $b geq 0$ | $A$ has form $A = [B,D]$ where $B$ is an $m times m$ full-rank matrix and $D$ is the rest of $A$. | Some of these assumptions impose restrictions on $ Omega$, whereas others are without loss of generality. . Assumption 1 is simply there to make the problem interesting. Were $n &gt; m$, the system of equalities would be over-determined and the constraint set would either be empty or contain a single point, which itself would be the optimum. It is, therefore an assumption which is not done without loss of generality. . Assumption 2 is equivalent to saying $rank(A) = m$. That is to say, all $m$ rows of $A$, as well as $m$ of the $n$ columns of $A$, are linearly independent. This assumption is also not done without loss of generality. Having less linearly independent rows corresponds to having less non-redundant constraints which clearly affects the constraint set $ Omega$. . Assumption 3 is done W.L.O.G. since the signs of $A$&#39;s row entries can always be flipped. . Assumption 4 is also done W.L.O.G. because if $A$ contains a full-rank $m times m$ submatrix per Assumption 2, then $A = [B,D]$ is a re-ordering of $A$ which adds no further restrictions on $ Omega$. . For $ Omega$ that satisfies Assumptions 1-4, the basic feasible solutions can be reformulated as follows. . Definition: &nbsp; Let $x_B$ be such that. $Bx_B = b$. Then the concatenation $x = [x_B, 0]^T$ is a solution to $Ax = b$. Such solutions are called feasible solutions. Furthermore, if $x_B geq 0$, such solutions are called basic feasible solutions. . Note that, for the case we&#39;re in, this is consistent with the earlier definition of a BFS. . Let $x$ be a BFS according to this definition. $Ax = b$ poses a set of $m$ linearly independent constraints since $rank(A) = m$, whereas $x geq 0$ poses a set of $n$. But $x = [x_B, 0]^T$ is a vector at which all $m$ of the equality constraints $Ax = b$ are active, and $n-m$ of the inequality constraints $x geq 0$ are also active. So in total $n$ linearly independent constraints are active at a BFS, which is consistent with the earlier definition. . Basic Feasible Solutions and Extreme Points are Equivalent . To formally prove that basic feasible solutions are extreme points in the geometric sense, consider the following theorem and its proof. . Theorem: &nbsp; The point $x$ is an extreme point of $ Omega = { Ax =b, x geq 0 }$ if and only if it is a basic feasible solution. . Proof . Sufficiency $ implies$: . Let $x$ be an extreme point of $ Omega$. Since it&#39;s in $ Omega$, $x geq 0$ and $Ax = b$. . Equivalently, $ sum_{i=1}^n x_ia_i = b$ where the $a_i$&#39;s are the column vectors of $A$. . Note that $x$ must contain zero entries, since it takes $n$ linearly independent active constraints to be an extreme point and only $m$ come from the equality constraints $Ax = b$. . By Assumption 1, $m$ of $A$&#39;s columns are linearly independent. We&#39;d like to claim that these $m$ are exactly those corresponding to the non-zero $x_i$ entries. . If this claim turns out to be true, then the full-rank $m times m$ submatrix $B$ will contain exactly those $m$ columns. And $x = [x_B, 0]^T$, where $x_B$ are the non-zero entries of $x$, would be a BFS. $ ast$ . So, let&#39;s prove the linear independence claim using a contradiction argument. . Without loss of generality, through rearrangement, let the first $m$ elements be the nonzero entries. That is, $x_1, ..., x_m &gt; 0$, and $x_{m+1}, ... ,x_n = 0$. . Then $ sum_{i=1}^n x_ia_i = sum_{i=1}^m x_ia_i = b$. . Towards contradiction, assume $a_1, ..., a_m$ are linearly dependent. Then $ exists y_1, ..., y_m in mathbb{R}$ not all zero s.t. $y_1a_1 + ... + y_ma_m = 0$ . Take $ epsilon &gt; 0$ to be very small. Small enough so that $x_i pm epsilon y_i &gt; 0 forall i = 1,...,m$. . Define two points as . $z^1 = [x_1 - epsilon y_1, ..., x_p - epsilon y_p, 0, ..., 0]^T$ and, $z^2 = [x_1 + epsilon y_1, ..., x_p + epsilon y_p, 0, ..., 0]^T$. . These points clearly satisfy $z_1,z_2 geq 0$, so they they satisfy one of $ Omega$&#39;s constraints. . Furthermore, . $Az^1 = sum_{i=1}^m z^1_ia_i = sum_{i=1}^m x_ia_i - epsilon sum_{i=1}^m y_ia_i = b$ since $ sum_{i=1}^m y_ia_i = 0$. . and similarly $Az^2 = b$. . So, $z^1$, and $z^2$ are indeed in $ Omega$. . But note that $x = frac{z^1 + z^2}{2}$ is a convex combination of two points in $ Omega$, which contradicts the assumption that it&#39;s an extreme point. . Hence, $a_1,...,a_m$ must be linearly independent. This concludes the proof by $ ast$. . Necessity $ impliedby$: . Suppose $x$ is a BFS and assume, towards contradiction, that it&#39;s not and extreme point of $ Omega$. . Then $ exists y,z in Omega$ with $y ne z$ s.t. $x = alpha y + (1- alpha) z$ for some $ alpha in (0,1)$. . But since $y,z in Omega$ they satisfy $Ay = Az = b$, so $Ay - Az = A(y - z) = 0$. . That is $(y_1 - z_1)a_1 + ... + (y_m - z_m)a_m = 0$. . But since $y ne z$, not all $(y_i - z_i) = 0$. So, $a_1,..., a_m$ are linearly dependent. This contradicts the assumption that $x$ was a BFS. . The Extreme Point Theorem . Why devote so much time defining extreme points geometrically, and then again algebraically? As hinted earlier and as shall be proved shortly, optima of linear programs occur at the extreme points. This is the reason LP&#39;s are a class of easy convex optimization problems — the search space for their optima can be reduced to a finite number of extreme points. . The Extreme Point Theorem: &nbsp; If a linear program &nbsp; . has a finite optimum, and | its constraint polytope has at least one extreme point, | . then there is an extreme point which is optimal. . So, if we want to solve linear programs we need only consider the extreme points. . Let&#39;s prove the theorem through induction on the dimension. . Proof . Take the following general LP and assume it has a finite optimum. Assume also that its constraint polytope has, at least, one extreme point. . $ begin{cases} min_{x}: c^Tx s.t.: x in mathcal{P} end{cases} $ . Assume the theorem is true for this LP with an $(n-1)$-dimensional constraint polytope. The objective is to show that it&#39;s also true for the same LP with an $n$-dimensional constraint polytope. . Let $v$ be the optimal value of the LP. . Let $Q = P cap { x : c^Tx = v }$ be the intersection of the constraint polytope with the level set of the objective function at the optimal value. . Since $Q$ is the intersection of an $n$-dimensional polytope $P$ with an additional linear constraint (a hyperplane), it is $(n-1)$-dimensional. . By the inductive hypothesis, there is an extreme point $x^* in Q$ that&#39;s optimal for the LP. . By a contradiction argument, $x^*$ is also an extreme point in $P$. . Suppose it is not an extreme point in $P$. Then by Definition 1 of extreme point, $x^*$ is a convex combination of two points in $P$. That is, $ exists y,z in P$ s.t. $ lambda y + (1- lambda)z = x^*$ for some $ lambda in [0,1].$ . But then $ lambda c^Ty + (1- lambda)c^Tz = c^Tx^* = v$, since $x^*$ is optimal. But the left hand side is a convex combination of scalars, so $c^Ty = c^Tz = v$. This means $y,z in Q$, which contradicts the fact that $x^*$ is an extreme point in $Q$. . Hence, $x^*$ must be an extreme point in $P$. . Sketch for an Alternate Proof . We can also prove the Extreme Point Theorem using a recursive argument. Recall that a continuous $1$-dimensional function $f: mathcal{D} rightarrow mathbb{R}$ on a closed interval $ mathcal{D} subset mathbb{R}$ necessarily achieves a min/max either on the endpoints of $ mathcal{D}$ or somewhere inside. If we additionally stipulate that $f$ is linear, the only possibilities are the endpoints. Extending this logic to linear programs in $n$-dimensions which have finite optimal solutions, we conclude that the optimal solution cannot occur at any interior point of the constraint polytope $ mathcal{P}$ and, instead, must occur somewhere on its boundary. But now we can consider the $(n-1)$-dimensional polytopes forming $ mathcal{P}$&#39;s boundary separately and apply the same logic to each one recursively. In the base case, we reach the conclusion that the optimal solution must occur at an endpoint of a $1$-dimensional polytope — a line segment such as $ mathcal{D}$. Such a point is an extreme point of the constraint polytope. . Conclusion . Linear programs are a sub-class of convex optimization problems for which the search space can be reduced to a finite set of basic feasible solutions or extreme points. Furthermore, all LP&#39;s can be written in a standard form which lends itself to being solved using a number of fast, iterative algorithms. So, although writing an optimization problem in LP form can be a difficult task, the payoff is worth it. .",
            "url": "https://v-poghosyan.github.io/blog/optimization/applied%20mathematics/proofs/2022/01/25/Optimization-Geometry-of-Linear-Programs.html",
            "relUrl": "/optimization/applied%20mathematics/proofs/2022/01/25/Optimization-Geometry-of-Linear-Programs.html",
            "date": " • Jan 25, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Optimization - Review of Linear Algebra and Geometry",
            "content": "Introduction . The study of optimization can be summed up as the attempt to find those parameter(s) that optimize some objective function, if such exist. The objective function can be almost anything — cost, profit, nodes in a wireless network, distance to a destination, similarity to a target image, etc. If the objective function describes cost we may wish to minimize it. If, on the other hand, it describes profit then a natural goal would be to maximize it. . The problems of minimization and maximization, summed up as optimization in one word, are the same problem up to a reflection with respect to the domain of the parameter(s). . Formally, let the objective function be $f: mathbb{R^n} to mathbb{R}$, and let it have minimizer $x^* in mathbb{R^n}$. Then, by definition of minimizer, $f(x^*) leq f(x) forall x in mathbb{R^n}$. It follows that $-f(x^*) geq -f(x) forall x in mathbb{R^n}$, so $x^*$ is the maximizer for $-f$. . Model of a Convex Optimization Problem . This series of posts will cover the ways in which we can solve an optimization problem of the form . $ textrm{minimize}: f(x) textrm{subject to}: x in mathcal{X} $ . where the objective function $f$ is a convex function, and the constraint set $ mathcal{X}$ is a convex set. Importantly, we will not cover the ways in which we can model a real-world problem as a convex optimization problem of the above form. . Why Convex Optimization? . First, let&#39;s define the size of an optimization problem as the dimensionality of the parameter $x$ added to the number of the problem constraints. . Convex optimization problems are a class of easy optimization problems — problems whose time and/or space complexity grows slowly with respect to problem size. . These problems are general enough to capture many scenarios of interest, even some that do not fall strictly into the convex category, but specific enough to be solvable through generic algorithms and numerical methods. . Review of Linear Algebra and Geometry . We start our exploration of convex optimization with a refresher on convexity and the linear algebra that&#39;s in common use in the subject. . Convexity . Set convexity is defined as follows: . Definition:&nbsp; A set $C subseteq mathbb{R^d}$ is convex if, for all points $x_1,x_2 in C$ and any $ theta in [0,1]$, the point $ theta x_1 + (1- theta) x_2$ (i.e. the parametrized line segment between $x_1$ and $x_2$) is also in $C$. . Some Operations that Preserve Convexity . Shifting, scaling, and rotation (i.e. affine transformations) preserve convexity. Let the matrix $A$ define such a transformation, and $b$ be a shift vector. Then $C&#39; = {Ax + b | x in C }$ is convex provided that $C$ was convex. . An intersection of convex sets is also convex. That is, $C&#39; = { x | x in C_1 cap x in C_2 }$ is convex provided that $C_1$ and $C_2$ were convex to begin with. The proof follows directly from the definition of intersection... . However, unions of convex sets need not be convex... . Examples of Convex Sets . The following are some common convex sets we will come across in practice. . Convex Hull of $n$ Points . A convex combination of points $x_1, ..., x_n$ is a point of the form $x = theta_1 x_1 + ... + theta_n x_n$ where $ sum_{i = 1}^{n} theta_i = 1$ and $ theta_i geq 0 forall i$. . Let $x_1,x_2,...,x_n$ be $n$ points in space. Their convex hull is the set of all points which can be written as some convex combination of them. Equivalently, by varying the $ theta_i$&#39;s we generate the convex hull as the set of all convex combinations of these points. . The convex hull can be visualized as the closed polygon formed when a rubber band is stretched around the $n$ points. The convex hull of two points is the line segment joining them. That of three points is the polygon (complete with its inner region). In general, for $n$ points, the concept generalizes to an $n$-dimensional polygon. . Formally, the convex hull is the set $ { theta_1 x_1 + ... + theta_n x_n | theta_1 + ... + theta_n = 1 textrm{and} theta_i geq 0 forall i }$ . Note: A handy interactive visualization, along with an efficient algorithm that generates a convex hull of $n$ points on a 2D plane can be found in the following blog post by Joel Gibson. . Convex Hull of a Set . The convex hull of a set can be similarly defined as all the convex combinations of the elements in the set. However, since the set may contain infinite elements, there&#39;s a more helpful, equivalent definition... . Let $C$ be a non-convex set. The convex hull of $C$ is the intersection of all convex supersets of $C$. That is, it&#39;s the intersection of all convex sets containing $C$. The result of such an intersection will be the smallest convex superset of $C$. . In fact, this minimal convex superset is unique 1 and can therefore be taken as yet another, equivalent, definition for the convex hull of a set. . Visualizing the convex hull of a non-convex set is similar to visualizing that of $n$ points — simply imagine the shape enclosed by a rubber band stretched around the non-convex set. . Affine Combination of $n$ Points . An affine combination of points $x_1,...,x_n$ is a point of the form $x = theta_1 x_1 + ... + theta_n x_n$ with $ sum_{i=1}^{n} theta_i = 1$ but where the $ theta_i$&#39;s need not be non-negative. . For two points, the set of all affine combinations is the line that passes through them, whereas for three points it&#39;s the plane. in general, it is the plane in $(n-1)$-dimensions passing through the $n$ points. . Linear Combinations - Hyperplanes and Halfspaces . A linear combination of $n$ vectors is all vectors of the form $x = theta_1 x_1 + ... + theta_n x_n$ with the $ theta_i$&#39;s totally unrestricted. . The set of all linear combinations of $n$ vectors (i.e. points) is called their span. Formally, it is the set $ { theta_1 x_1 + ... + theta_n x_n | forall theta_1,..., theta_n }$. . The span of a single vector is the line passing through it. For two vectors the span is the plane passing through them and, in general, the span of $n$ vectors is a plane in $n$-dimensions that contains these vectors. . Hyperplanes . For fixed weights $ theta_i = a_i forall i$, a hyperplane is the set of all points $x in mathbb{R^n}$ whose linear combination equals a fixed constant $b in mathbb{R}$. . Formally, a hyperplane is the set $ { x | a_1 x_1 + ... a_n x_n = b } = { x | a^T x = b }$ . There&#39;s a geometric interpretation of the parameters $a in mathbb{R^n}$ and $b in mathbb{R}$. Since the dot-product between perpendicular vectors is $0$, $ { x | a^T x = 0 }$ is simply the set of all vectors perpendicular to $a$ (whose tail, as with all vectors in linear algebra, is considered to be fixed at the origin), making $a$ the normal vector to the hyperplane passing through the origin. To allow for parallel hyperplanes that are translated from the origin, the offset $b in mathbb{R}$ is introduced in the generalization $ { x | a^T x = b }$. This is now the set of all vectors whose dot-product with $a$ is constant. These vectors are not quite perpendicular to $a$, but they form a parallel hyperplane that&#39;s been shifted from the origin by a distance of $ frac{|b|}{ |a |_2}$. . Since the sum $a_1 x_1 + ... a_n x_n = b$ is fixed, the last coordinate, which we&#39;ll call $x_k$ for some $k in [1,...,n]$, is fixed by the choice of the other $n-1$ coordinates. Therefore, a hyperplane in $ mathbb{R^n}$ spans $n-1$ dimensions instead of $n$. . Halfspaces . A halfspace is either of the two sub-spaces a hyperplane partitions the whole space into. Since the dot-product between vectors which are roughly in the same direction is positive, and vice versa, the two halfspaces associated to a hyperplane $ { x | a^T x = b }$ are $ { x | a^T x geq b }$ and $ { x | a^T x leq b } $. . Conic Combinations of $n$ Points . A conic combination of $x_1,...x_n$ is a point $x = sum_{i=1}^{n} theta_i x_i$ where $ theta_i geq 0 forall i$. Note that the absence of the restriction that $ sum_{i=1}^{n} theta_i = 1$ is what distinguishes a conic combination from a convex combination. . A visual example: . . Ellipses . Recall from Euclidean geometry that ellipses are conic sections. In general we define ellipses in $n$-dimensions as the sub-level sets of quadratic forms. That is $ { x | (x-c)^T M (x-c) leq 1 }$ where $M succeq 0$ defines the stretch along each principal axis, and $c in mathbb{R^n}$ is the center. . An equivalent definition of an ellipse using the L2-norm is $ { x | |Ax - b |_2 leq 1 }$. That is, for a given $A$ and $b$ in the L2-norm definition, we can find an $M$ and $c$ in the sub-level set definition and vice versa. . Note: More generally, the ellipse is $ { x | (x-c)^T M (x-c) leq r }$. However, since the scaling factor $r$ is positive, it can simply be absorbed into $Q$ without affecting $Q$&#8217;s positive semidefiniteness. . To quickly convince ourselves in the equivalence of these definitions, we take the simple case where $b = 0$. . $$ begin{aligned} |Ax |_2 &amp;= ((Ax)^T(Ax))^{1/2} &amp;= (x^TA^TAx)^{1/2} &amp;= (x^TU D U^Tx)^{1/2} &amp;= x^TU D^{1/2} U^Tx end{aligned} $$Where the third equality is by the spectral decomposition of the real symmetric matrix $A^TA$, in which $D = diag( lambda_1,..., lambda_n)$ is the diagnonal matrix of eigenvalues and the columns of $U$ are the corresponding eigenvectors. Taking $M= UD^{1/2}U^T$, where $D^{1/2}$ is simply $D^{1/2} = diag( sqrt lambda_1,..., sqrt lambda_n)$, we have the equivalent sub-level set definition of the ellipse. . Norm Balls . Related to ellipses are Euclidean balls, which are norm balls for the choice of the L2-norm. A Euclidean ball has the form $ { x | |x |_2 leq r }$, and is clearly convex as it&#39;s a generalizations of the sphere in $n$-dimensions. . But also, a Euclidean ball is the special ellipse for the choice of $M = rI$, and $c = 0$. . In general, norm balls $ { x | |x |_p leq r }$ where $ |x |_p = (x_1^p + ... + x_n^p)^{1/p}$ are convex for any choice of $p geq 1$. . Polyhedra . Where a halfspace is a set with one linear inequality constraint, a polyhedron is a set with many, but finite, such linear inequality constraints. These constraints can be packed into a matrix $A in mathbb{R^{m times n}}$ by vector $b in mathbb{R^m}$ multiplication form, making the polyhedron into the set $ {x | Ax leq b }$. . Since polyhedra are simply intersections of halfspaces and hyperplanes, and the latter are both convex, polyhedra are also convex sets. . The Set of All Positive Semidefinite Matrices . The set of all PSD matrices $ { Q | x^TQx geq 0 forall x in mathbb{R^m} }$ is convex. We can, of course, use the definition of convexity to show this. But, a more elucidative approach would be the following remark. . Note that $Q mapsto x^TQx$ is a linear functional that maps the space of all PSD matrices to its field of scalars. This is analogous to how $a mapsto x^Ta$ is a linear functional so, just as $ { a | x^Ta geq 0 }$ is a halfspace in the space of vectors, $H_x = { Q | x^TQx geq 0 }$ for a given choice of $x in mathbb{R^m}$ is a halfspace in the space of PSD matrices. Halfspaces, as we already know, are convex and $ { Q | x^TQx geq 0 forall x in mathbb{R^m} }$ is nothing but an intersection of halfspaces for each choice of $x$. That is, $ { Q | x^TQx geq 0 forall x in mathbb{R^m} } = bigcap_x H_x$, concluding the proof of its convexity. . 1. Proof of uniqueness of the minimal, convex superset: Suppose $C_1$ and $C_2$ are both minimal, convex supersets of $C$. Any convex set $D$ that contains $C$ must clearly contain the minimal, convex superset. Hence, $C_1 subseteq C_2$ and $C_2 subseteq C_1$, which implies $C_1 = C_2$.↩ .",
            "url": "https://v-poghosyan.github.io/blog/optimization/applied%20mathematics/proofs/2022/01/23/Optimization-Review-of-Linear-Algebra-and-Geometry.html",
            "relUrl": "/optimization/applied%20mathematics/proofs/2022/01/23/Optimization-Review-of-Linear-Algebra-and-Geometry.html",
            "date": " • Jan 23, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Machine Learning - Decision Trees",
            "content": "Introduction . Classification vs. Regression . We start our discussion of decision trees with a definition of classification and classifier. . Definition:&nbsp; Classification is the process of grouping data into discrete categories (i.e. class labels). . We may contrast this definition with regression which is the process of predicting a continuous (i.e. real or complex-valued) output. . A common example of a classification problem is the sorting of emails into the binary categories of &#39;spam&#39; and &#39;not spam&#39;. However, the labels in a classification problem need not be binary — they may be any discrete set. Whereas a common example of regression is learning a linear (or a non-linear) function that best fits a given dataset. . Note: The line between classification and regression is sometimes blurred. For instance, logistic regression is a regression algorithm which outputs a prediction in the continous probability range $[0,1]$. It&#8217;s commonly used with a decision rule which casts its output into discrete classes. Thus, even though it&#8217;s a regression algorithm, it can easily be converted into a classification algorithm and is often used for classification problems in practice. . This leads us to the expected definition of a classifier, which is: . Definition:&nbsp; A classifier is any algorithm that performs classification. . Decision Trees . Decision trees are one type of powerful classifier among many. . The nodes of a decision tree correspond to the features of the dataset and its leaves correspond to the class labels. The paths in a decision tree correspond to the conjunction of features that lead to the class labels at its leaves. . To understand this, let&#39;s look at an example of a decision tree that&#39;s very easy to understand because of the historical context of the data it&#39;s attempting to learn. . Example: . # Imports import pandas as pd import numpy as np from sklearn.datasets import fetch_openml import pydotplus from sklearn.tree import DecisionTreeClassifier, export_graphviz from IPython.display import Image # Setting random state for replicability of results RS = 2022 # Fetching data as features and labels from OpenML as a Pandas dataframe X, y = fetch_openml(&quot;titanic&quot;, version = 1, as_frame = True, return_X_y = True) # Cleaning and engineering data: # Dropping unneeded columns and those with too many missing values... X.drop( columns = [ &#39;boat&#39;, &#39;body&#39;, &#39;home.dest&#39;, &#39;cabin&#39;, &#39;ticket&#39;, &#39;name&#39;, &#39;embarked&#39;, &#39;fare&#39; ], inplace = True ) # Dropping data points with any missing values and subseting labels accordingly X[&#39;labels&#39;] = y # Temporarily adding labels to data as a column X.dropna(inplace = True) # Dropping the rows with any null values y = X[&#39;labels&#39;] # Subsetting the labels X.drop(columns = &#39;labels&#39;, inplace = True) # Dropping the column of labels # Converting &#39;male&#39; and &#39;female&#39; feature values into the Boolean values 0 and 1 respectively X[&#39;sex&#39;] = X[&#39;sex&#39;].apply(lambda x: 0 if x == &#39;male&#39; else 1) # Defining a decision tree classifier clf = DecisionTreeClassifier( random_state = RS, max_depth = 3 ) # Training the decision tree classifier clf = clf.fit(X, y) # Visualizing the decision tree feature_cols = [] png = export_graphviz( clf, feature_names = X.columns, class_names = [&#39;died&#39;,&#39;survived&#39;], impurity = False, ) graph = pydotplus.graph_from_dot_data(png) Image(graph.create_png()) . . Bad pipe message: %s [b&#39;a/5.0 (Macintosh; Intel Mac OS X 10.14; rv:96.0) Gecko/20100101 Firefox/96.0 r nAccept: */* r nAccept-Language:&#39;] . The above decision tree has identified the isolated features, as well as the conjunction of features, that best predict the chances of a given passenger of the Titanic to survive. These features, ordered loosely in terms of importance, are sex, age, sibsp (number of siblings or spouses), and pclass (passenger class). . As we can infer from the tree, were you a male passenger (sex &lt;= 0.5) on the Titanic over the age of 9.5 you probably did not survive the crash. If, however, you were either a female passenger or a male child below the age of 9.5 with fewer than 2.5 siblings (a conjunction of features), it&#39;s likely that you survived. . Setup . Simplifying Assumptions . In the rest of this article, for simplicity, we will assume binary input and binary output for decision trees. That is, the training set is ${S = {(x^1,y^1), ... ,(x^k, y^k) }}$ with ${x^i in {0,1 }^n}$ and ${y^i in {0,1 } forall i}$. This means that the decision tree itself is simply a binary function which also receives binary input. . The task is to learn this function. . Potential Function .",
            "url": "https://v-poghosyan.github.io/blog/machine%20learning/decision%20trees/random%20forests/2021/12/29/Machine-Learning-Decision-Trees.html",
            "relUrl": "/machine%20learning/decision%20trees/random%20forests/2021/12/29/Machine-Learning-Decision-Trees.html",
            "date": " • Dec 29, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "LeetCode 11 - Container with Most Water",
            "content": "Introduction . Problem Statement . You are given an integer array height of length n. There are n vertical lines drawn such that the two endpoints of the ith line are (i, 0) and (i, height[i]). . Find two lines that together with the x-axis form a container, such that the container contains the most water. . Return the maximum amount of water a container can store. . Example: . . Input: height = [1,8,6,2,5,4,8,3,7] Output: 49 Explanation: The above vertical lines are represented by array [1,8,6,2,5,4,8,3,7]. In this case, the max area of water (blue section) the container can contain is 49. . Foreword . The brute force solution to this problem consists of checking each pair of vertical lines. Since order of any given pair does not matter, this solution has time complexity ${O({n choose 2}) = O(n^2)}$ where $n$ is the length of the height array. . The non-brute-force solution to this problem (i.e. the &#39;two pointer solution&#39;) is pretty intuitive - the difficulty lies in its proof of correctness. Therefore, I will give away the procedure in this foreword and then proceed to prove its correctness. . The Procedure . The following is the overall procedure in words: . Initialize left and right pointers at 1 and n respectively (assuming indices start at 1). | While the pointers do not intersect: . Fix the pointer whose corresponding vertical line is longer. | Advance the pointer whose corresponding vertical line is shorter towards the fixed one. | . | . The Code . #hide-output class Solution: def maxArea(self, height: List[int]) -&gt; int: i, j = 0, len(height) - 1 water = 0 while i &lt; j: water = max(water, (j - i) * min(height[i], height[j])) if height[i] &lt; height[j]: i += 1 else: j -= 1 return water . . Proof of Correctness . Optimal Substructure . The procedure is inspired by the following recursive optimal substructure of the problem: . Let $h(i)$ denote the height of the $i$-th vertical line. . | Let $a(i,j)$ denote the area of the container formed by the pair of vertical lines $(i,j)$. . | Let $maxArea([i:j])$ denote the maximum area formed by the lines ${i,...,j}$ – that is the output of the procedure on the subarray ${[i:j]}$. . | . Claim . The problem has optimal substructure: $$maxArea([1:n]) = max {a(1,n), maxArea([2:n]) }$$ . Proof of Claim . For the initial pair $(1,n)$ where, WLOG, ${h(1) &lt; h(n)}$ we have ${a(1,n) &gt; a(1,k) forall k}$. This is because we&#39;re starting out from the widest container formed by ${(1,n)}$ and considering containers of decreasing width formed by the pairs ${(1, n-1), (1, n-2), ..., (1,2)}$. . There are two cases: . In case ${h(k) &gt; h(1)}$ for some ${1 &lt; k leq n}$ the area of the container formed by ${(1,k)}$ is still determined by ${h(1)}$, except now it&#39;s less wide. Whereas if ${h(k) &lt; h(1)}$ the area of the container decreases not only in width but also in height. . In both cases we have ${a(1,n) &gt; a(1,k)}$ which means in general ${a(1,n) &gt; a(1, k) forall k}$. . Therefore, we may omit the first vertical line from consideration and consider the subproblem on the indices ${2,...,n}$. The overall optimal solution will then be $$maxArea([1:n]) = max {a(1,n), maxArea([2:n]) }$$ as was the claim. . Inductive Proof . As with all problems that have a recursive optimal substructure, an inductive proof of correctness is immediately what springs to mind. . Base Case . For the case when $n = 2$, ${maxArea([1:2]) = max {a(1,2), maxArea([2:2]) } = a(1,2)}$ since ${maxArea[2:2] = 0}$. This is obviously correct. . Inductive Step . Suppose for an array of length $m$, the procedure $maxArea$ is correct. We would like to show that for an array of length $m+1$ it is still correct. . Assume, WLOG, ${h(1) &lt; h(m+1)}$. Note that by the optimal substructure shown above, ${maxArea([1:m+1]) = max {a(1,m+1), maxArea([2:m+1]) }}$. In omitting the first element of the input array, the only pairs we remove from consideration are ${(1,m), (1,m-1),..., (1,2)}$ which we have already shown to be suboptimal to ${(1,m+1)}$ in the proof of the optimal substructure. And since by assumption the procedure $maxArea$ on the $m$-element subarray ${[2:m+1]}$ is correct, we are done! . Conclusion . With some problems, it is the case that figuring out why the solution works gives more insight into the problem than simply solving it based on raw intuition... .",
            "url": "https://v-poghosyan.github.io/blog/leetcode/toy%20problems/proofs/2021/12/23/LeetCode-11.html",
            "relUrl": "/leetcode/toy%20problems/proofs/2021/12/23/LeetCode-11.html",
            "date": " • Dec 23, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Python for Machine Learning - NumPy",
            "content": "Importing NumPy . NumPy is a scientific computing library for Python. It&#39;s an extensive collection of pre-written code that optimizes and extends, among other things, the Python array (i.e. list) object into an n-dimensional NumPy array called ndarray. It comes with a variety of tools, such as matrix operations and common mathematical functions, that enable Python to perform complex mathematical tasks such as solve linear algebraic problems, generate pseudo-random numbers, perform Fourier analysis, etc. . We import NumPy, as we import any other library, using the import keyword (with or without a shorthand). . import numpy . Or, alternatively: . import numpy as np . Optimizations . As we&#39;ve briefly discussed in the &quot;Python for Machine Learning - Pandas&quot; post, NumPy works by delegating tasks to well-optimized C code under the hood. In this way it exploits the flexibility of Python while bypassing its speed limitations as an interpreted language and, instead, exploiting the speed advantages of a compiled language. . Scalable Memory Representation . One of the things NumPy optimizes is data storage. In contrast to Python 3.x&#39;s scalable memory representation of numeric values, such as integers, which can grow to accommodate a given number, NumPy stores numeric types in fixed-sized blocks of memory (e.g. int32 or int64). This means NumPy is able to take advantage of the low-level CPU instructions of modern processors that are designed for fixed-sized numeric types. Another advantage of fixed-sized storage is that consecutive blocks of memory can be allocated, which enables the libraries upon which NumPy relies to do extremely performant computations. This enforcement of fixed-sized data types is part of the optimization strategy NumPy uses called vectorization. . Vectorization . As already discussed in the aforementioned post, vectorization is the process by which NumPy stores an array internally in a contiguous block of memory, and restricts its contents to only one data type. Letting Python know this data type in advance, NumPy can then skip the per-iteration type checking that Python normally does as its iterating through a loop in order to speed up our code. Optimizing the array data structure in such a way enables NumPy to delegate most of the operations on such arrays to pre-written C code under the hood. In effect, this simply means that looping occurs in C instead of Python. . Broadcasting . The term broadcasting describes the process by which NumPy performs arithmetic operations on arrays of different dimensions. The process is usually as follows: the smaller array is “broadcast” across the larger array so that the two arrays have compatible dimensions. Broadcasting provides a means of vectorizing array operations. . Comparing Runtime . To demonstrate the performance optimizations of NumPy, let&#39;s compare squaring every element of a 1,000,000-element array and summing the results. . Using a Python List . First, we will use a Python list: . unoptimized_list = list(range(1000000)) . Squaring each element and summing: . import numpy as np %timeit np.sum([i**2 for i in unoptimized_list]) . 320 ms ± 2.81 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) . . Note: Even though we&#8217;re using NumPy&#8217;s sum() method, since the input we&#8217;re passing to it is a regular Python list, NumPy optimizations are not applied. . As we can see the whole thing took about 314 ms. . Using a NumPy Array . Now let&#39;s do the same with a NumPy array, which also gives us the opportunity to introduce the syntax for defining one using a range. . optimized_array = np.arange(1000000) . Let&#39;s check the type of optimized_array to convince ourselves that it is, indeed, a NumPy ndarray. . type(optimized_array) . numpy.ndarray . Now, finally, let&#39;s square each element and sum the results: . %timeit np.sum(optimized_array**2) . 1.61 ms ± 19.7 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each) . Remarkably, the run-time was cut from 314 ms to only around 1.61ms! . NumPy Basics . Let&#39;s explore some of the ways in which we can represent arrays and matrices in NumPy. . Creating Arrays . We&#39;ve already seen how we can create a 1-dimensional NumPy array of consecutive integers $0,1,...,n-1$ using the arrange() method. . The standard way of creating a NumPy array is passing a Python list to the constructor array() like so: . a = np.array([1,2,3]) a . array([1, 2, 3]) . We can also create some common arrays, such as an array of consecutive integers, with some special methods such as arange(), which takes an integer $n$ as input and creates a sequential array from $0,...,n-1$. . np.arange(5) # array([0, 1, 2, 3, 4]) . array([0, 1, 2, 3, 4]) . Representing Matrices . Let&#39;s represent a $2 times 3$ matrix $ A = begin{bmatrix} 1 &amp; 2 &amp; 3 4 &amp; 5 &amp; 6 end{bmatrix} $ using NumPy: . A = np.array([[1,2,3], [4,5,6]]) A . array([[1, 2, 3], [4, 5, 6]]) . There are also ways to quickly create some common matrices using special methods. . For example, ones() accepts a shape pair, and creates a matrix of $1$s with of the given shape. . np.ones((2,3)) . array([[1., 1., 1.], [1., 1., 1.]]) . The method, zeros() works the same way as ones(): . np.zeros((2,3)) . array([[0., 0., 0.], [0., 0., 0.]]) . Meanwhile, identity() accepts an integer $n$ as input and creates a square $n times n$ identity matrix. . np.identity(3) . array([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]]) . Creating a matrix with identical elements in general uses the full() method which takes a shape attribute, a value attribute and on optional dtype attribute as follows: . np.full((2,3), 7, dtype = int) . array([[7, 7, 7], [7, 7, 7]]) . Indexing . Indexing a 1-dimensional NumPy array is done as expected, through the use of the trusty brackets []. Indexing an n-dimensional matrix in NumPy still uses [] but it introduces a new, improved, syntax. . Suppose we&#39;d like to access the element in the first row, and last column of A. The standard way would be: . A[0][2] . 3 . As we can see, that still works. But the recommended and, subjectively speaking, prettier way is: . A[0,2] . 3 . Of course, slicing still works as expected. . For example, let&#39;s print the entire first row of A: . A[0,:] . array([1, 2, 3]) . The entire first column: . A[:,0] . array([1, 4]) . Finally, let&#39;s print the submatrix $ begin{bmatrix} 2 &amp; 3 end{bmatrix} $: . A[0,1:] . array([2, 3]) . Properties and Methods of NumPy Arrays . A few of the useful properties and methods of ndarray are highlighted in this section. . shape - returns the shape of the matrix as an $(m,n)$ pair . A.shape # (2,3) . | . ndim - returns the dimension of a matrix as a single digit . A.ndim # 2 . . Note: The output of the ndim property should not be understood in a linear algebraic sense as the dimension of either the domain or range of the corresponding transformation, nor the dimension of either of its four fundamental subspaces. It is to only be understood in the data structure sense as the level of nestedness of the array. . | . size - returns the total number of elements in the matrix . A.size # 6 . | . dtype - returns the data type of the elements in the matrix. . A.dtype # dtype(&#39;int32&#39;) . . Note: If the ndarray does not represent a matrix, such as B = np.array([[1,2,3],[4,5]]) then dtype outputs O signifying that the entries are general Python objects. In such a case, the array loses its optimizations. . | . Statistical and Mathematical Methods . There is also a vast selection of statistical, and more generally, mathematical methods that ndarrays come with. Here are a few of the common ones: . sum() - returns the sum of all the entries . A.sum() # 21 . It also accepts an axis attribute where axis = 0 refers to the sum along the columns, and axis = 1 refers to the sum along the rows. . A.sum(axis = 0) # [5,7,9] . A.sum(axis = 1) # [6,15] . | mean() - returns the empirical mean of all the entries . A.mean() # 3.5 . | var() - returns the variance of the entries . A.var() # 2.9166666666666665 . | std() - returns the standard deviation of the entries . A.std() # 1.707825127659933 . | . Multi-Indexing, Filtering, and Broadcasted Operations . Recall from the Pandas article the ways in which we were able to multi-index and filter, and how we eliminated the need for using Python loops and list comprehensions using broadcasted operators instead. Since both a Pandas Series and a DataFrame are extensions of NumPy&#39;s ndarray, all of these apply here as well. . As a refresher on broadcasted operations, here are a few filtering examples. . Let&#39;s obtain those elements of A that are greater than 3: . A[A &gt; 3] . array([4, 5, 6]) . Now let&#39;s obtain those elements of A that are greater than the empirical mean: . A[A &gt; A.mean()] . array([4, 5, 6]) . What about those elements of A that are less than or equal to the empirical mean? . A[~(A &gt; A.mean())] . array([1, 2, 3]) . Which is equivalent to: . A[A &lt;= A.mean()] . array([1, 2, 3]) . Matrix Operations . One of NumPy&#39;s key selling points is that it makes matrix operations in Python easy. It offers simple syntax to add, multiply, transpose, invert, flatten, etc. matrices. . Addition . Addition of matrices is, by default, per-element (as are all NumPy operations). There&#39;s no special syntax, it&#39;s done through the + operator. . For example: . A = np.ones((2,3)) B = np.ones((2,3)) A + B . array([[2., 2., 2.], [2., 2., 2.]]) . Multiplication . The operator * performs per-element multiplication. . A = np.full((2,3), 2, dtype = int) B = np.full((2,3), 3, dtype = int) A * B . array([[6, 6, 6], [6, 6, 6]]) . But this, as we know, isn&#39;t matrix multiplication as it&#39;s commonly defined in mathematics — the dot product of corresponding rows and columns. For instance, if we try to multiply an $m times n$ matrix with an $n times p$ matrix, NumPy will throw the following error: . A = np.full((2,3), 2, dtype = int) B = np.full((3,4), 3, dtype = int) A * B . ValueError: operands could not be broadcast together with shapes (2,3) (3,4) . This is because per-element operations require the shapes of the operands to be the same or compatible up to broadcasting. Here, since the shapes are different, NumPy attempts to broadcast one operand to match the shape of the other. But broadcasting is impossible between matrices of shapes $2 times 3$ and $3 times 4$ per the broadcasting rules. . There&#39;s a workaround that lets us use *. Since NumPy overloads the * operator, it works as it should for numpy.matrix types. . A = np.matrix([[2,2,2], [2,2,2]]) B = np.matrix([[3,3,3,3], [3,3,3,3], [3,3,3,3]]) A * B . matrix([[18, 18, 18, 18], [18, 18, 18, 18]]) . However, this is not the recommended way to do matrix multiplication in NumPy. Overloaded operators can produce convoluted code. For instance, we may have many different matrix and ndarray data structures and be unable to anticipate the result of a given * operation. . Instead, the recommended way to do matrix multiplication is through the @ operator. . When we use @ NumPy internally uses its matmul() method. So, the following are equivalent and both produce the matrix product of A and B. . A = np.full((2,3), 2, dtype = int) B = np.full((3,4), 3, dtype = int) A @ B . array([[18, 18, 18, 18], [18, 18, 18, 18]]) . A = np.full((2,3), 2, dtype = int) B = np.full((3,4), 3, dtype = int) np.matmul(A,B) . array([[18, 18, 18, 18], [18, 18, 18, 18]]) . NumPy also offers, dot() which is equivalent to matmul() for 1D and 2D matrices. So, the following is yet another way we can multiply matrices: . A = np.full((2,3), 2, dtype = int) B = np.full((3,4), 3, dtype = int) A.dot(B) . array([[18, 18, 18, 18], [18, 18, 18, 18]]) . However, matmul() is preferred over dot() because of the clarity of its name, and because the dot product has a distinct mathematical meaning separate from matrix multiplication. .",
            "url": "https://v-poghosyan.github.io/blog/python%20for%20ml/numpy/machine%20learning/2021/12/20/Python-for-ML-NumPy.html",
            "relUrl": "/python%20for%20ml/numpy/machine%20learning/2021/12/20/Python-for-ML-NumPy.html",
            "date": " • Dec 20, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "Python for Machine Learning - Pandas",
            "content": "Importing Pandas . Pandas is a library that contains pre-written code to help wrangle with data. We can think of it as Python&#39;s equivalent of Excel. . We import Pandas into our development environment as we import any other library — using the import command. . import pandas . It&#39;s standard to import Pandas with the shorthand pd in order to avoid typing pandas all the time. . import pandas as pd . This gives us access to a vast array of pre-built objects, functions, and methods which are detailed in the API reference. . Two Underlying Data Types . Series . The basic unit of Pandas is the pandas.Series object which, in keeping with the Excel analogy, can be thought of as a column in an Excel table. It&#39;s a one-dimensional data structure that&#39;s derived from a NumPy array. However, unlike a NumPy array, the indices of a Series object aren&#39;t limited to the integer values $0,1,...n$ — they can also be descriptive labels. . Let&#39;s create a Series object representing the populations of the G-7 countries in units of millions. . import pandas as pd g7_pop = pd.Series([35,63,80,60,127,64,318]) . As we can see, creating a series is a matter of passing a Python list (or a Numpy array) into the Series constructor. . Indexing . Indexing a Series object is similar to indexing a Python list. For instance, let&#39;s print the first element in the above series. . g7_pop[0] . 35 . Let&#39;s now swap out the integer-based indices with descriptive labels. Each Series object has an index argument that can be overwritten. . g7_pop.index = [ &#39;Canada&#39;, &#39;France&#39;, &#39;Germany&#39;, &#39;Italy&#39;, &#39;Japan&#39;, &#39;UK&#39;, &#39;US&#39; ] . Now, we can print the first element of the series using its descriptive label. . g7_pop[&#39;Canada&#39;] . 35 . We may notice a similarity between a standard Python dictionary and a labeled Series object. Namely, indexing a series with a label and keying into a Python dictionary have similar syntax. In fact, it&#39;s possible to create a labeled Series object directly from a Python dictionary. . g7_pop = pd.Series({ &#39;Canada&#39; : 35, &#39;France&#39; : 63, &#39;Germany&#39; : 80, &#39;Italy&#39; : 60, &#39;Japan&#39; : 127, &#39;UK&#39; : 64, &#39;US&#39; : 318 }) g7_pop . Canada 35 France 63 Germany 80 Italy 60 Japan 127 UK 64 US 318 dtype: int64 . In the event of overriding the integer-based indices, it&#39;s still possible to access the elements of a Series sequentially using the iloc method (short for &quot;integer location&quot;). To retrieve the population of Canada, we can do as follows: . g7_pop.iloc[0] . 35 . It&#39;s also possible to use a range when indexing. For instance, suppose we&#39;d like to retrieve the populations of the first three countries from g7_pop. We can simply write: . g7_pop[&#39;Canada&#39;:&#39;Germany&#39;] . Canada 35 France 63 Germany 80 dtype: int64 . Or equivalently: . g7_pop.iloc[0:3] . Canada 35 France 63 Germany 80 dtype: int64 . Since the Series object is based on a Numpy array, it also supports multi-indexing through passing a list of indices or a Boolean mask. . For instance, to print the populations of Canada and Germany at the same time, we can pass in the list [&#39;Canada&#39;,&#39;Germany&#39;] or the Boolean mask [True, False, True, False, False, False, False]. . g7_pop[[&#39;Canada&#39;,&#39;Germany&#39;]] . Canada 35 Germany 80 dtype: int64 . g7_pop[[True, False, True, False, False, False, False]] . Canada 35 Germany 80 dtype: int64 . Broadcasted and Vectorized Operations . Since it&#39;s based on a NumPy array, a Series object also supports vectorization and broadcasted operations. . As a quick reminder, vectorization is the process by which NumPy optimizes looping in Python. It stores the array internally in a contiguous block of memory and restricts its contents to only one data type. Letting Python know this data type in advance, NumPy can then skip the per-iteration type checking that Python normally does in order to speed up our code. In fact, NumPy delegates most of the operations on such optimized arrays to pre-written C code under the hood. . Broadcasting, on the other hand, is the optimized process by which NumPy performs arithmetic and Boolean operations on arrays of unequal dimensions. . For instance, suppose the projected population growth of each G-7 country is 10 mln by the year 2030. Instead of looping through the Series object and adding 10 to each row or using a list comprehension, we can simply use broadcasted addition. . g7_2030_pop = g7_pop + 10 g7_2030_pop . Canada 45 France 73 Germany 90 Italy 70 Japan 137 UK 74 US 328 dtype: int64 . Filtering . Thanks to broadcasted Boolean operations and multi-indexing with a Boolean mask, it&#39;s possible to write concise and readable filtering expressions on Series. . For instance, let&#39;s return the list of countries with a population over 70 mln. . g7_pop[g7_pop &gt;= 70] . Germany 80 Japan 127 US 318 dtype: int64 . The expression g7_pop &gt;= 70 is a broadcasted Boolean operation on the Series object g7_pop which returns a Boolean array [False, False, True, False, True, False, True]. Then g7_pop is multi-indexed using this Boolean mask. . As another example of readable filtering expressions, we can return the list of countries whose populations exceed the mean population. . g7_pop[g7_pop &gt;= g7_pop.mean()] . Japan 127 US 318 dtype: int64 . DataFrame . Each DataFrame is composed of one or more Series. Whereas a Series is analogous to a column of an Excel table, a DataFrame is analogous to the table itself. . The DataFrame constructor accepts a variety of input types, among them an ndarray and a dictionary. . If we&#39;re passing an ndarray, it becomes necessary to specify the column labels separately. Additionally, we may overwrite the integer-based indexing as we did with the Series object. . data = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) df = pd.DataFrame(data, index = [&#39;R1&#39;,&#39;R2&#39;,&#39;R3&#39;], columns = [&#39;C1&#39;, &#39;C2&#39;, &#39;C3&#39;]) df . C1 C2 C3 . R1 1 | 2 | 3 | . R2 4 | 5 | 6 | . R3 7 | 8 | 9 | . We may bypass specifying columns manually by passing in a dictionary instead: . data = { &#39;C1&#39; : [1, 2, 3], &#39;C2&#39; : [4, 5, 6], &#39;C3&#39; : [7, 8, 9] } df = pd.DataFrame(data, index = [&#39;R1&#39;,&#39;R2&#39;,&#39;R3&#39;]) df . C1 C2 C3 . R1 1 | 4 | 7 | . R2 2 | 5 | 8 | . R3 3 | 6 | 9 | . . Note: Whereas in a Series the keys of the input dictionary were the row labels, in a DataFrame they&#8217;re the column labels. . In practice we often create a DataFrame from a CSV file using the pandas.read_csv() method like so: . csv_path = &#39;file.csv&#39; #Stores the path to a CSV df = pd.read_csv(csv_path) . . Tip: We may optionally pass header = None as an argument to read_csv() after csv_path if the first row of the CSV file itself is a data point, and not a header. . Tip: Pandas also supports reading an Excel file into a DataFrame using the pandas.read_excel() method. . Common Methods . Here are the common DataFrame methods that we should keep in our toolbox. These methods give us an overview of our data, and help us clean it up. . df.head() shows, by default, the first 5 rows of the dataset. Accepts an integer argument for the number of rows to display. | df.tail() shows the last 5 rows, and also accepts an integer argument. | df.info() gives a bird&#39;s eye overview of the dataset by showing the total rows/columns, the number of non-null datapoints, and the data types. | df.describe() returns statistically significant values for each column such as, the mean, standard deviation, minimum, and maximum values. | df.shape - returns the dimension of the dataset as an $(m,n)$ tuple. | . Indexing . Let&#39;s add to the dataset of G-7 countries the columns &#39;GDP&#39; and &#39;Surface Area&#39;. . g7_df = pd.DataFrame({ &#39;Population&#39; : g7_pop, &#39;GDP&#39; : [1.7, 2.8, 3.8, 2.1, 4.6, 2.9, 1.7], &#39;Surface Area&#39; : [9.0, 0.6, 0.3, 0.3, 0.3, 0.2, 9.0] }) g7_df . Population GDP Surface Area . Canada 35 | 1.7 | 9.0 | . France 63 | 2.8 | 0.6 | . Germany 80 | 3.8 | 0.3 | . Italy 60 | 2.1 | 0.3 | . Japan 127 | 4.6 | 0.3 | . UK 64 | 2.9 | 0.2 | . US 318 | 1.7 | 9.0 | . Whereas in a Series, the primary axis of indexing are the rows, in a DataFrame the primary axis are the columns. Thus, indexing a column works as expected. . g7_df[&#39;GDP&#39;] . Canada 1.7 France 2.8 Germany 3.8 Italy 2.1 Japan 4.6 UK 2.9 US 1.7 Name: GDP, dtype: float64 . Multi-indexing also works in the familiar way: . g7_df[[&#39;Population&#39;,&#39;GDP&#39;]] . Population GDP . Canada 35 | 1.7 | . France 63 | 2.8 | . Germany 80 | 3.8 | . Italy 60 | 2.1 | . Japan 127 | 4.6 | . UK 64 | 2.9 | . US 318 | 1.7 | . If we want to index rows, however, we must use the loc or iloc methods. . For instance, say we are interested in the population, GDP, and surface area of only the first three countries. We could query the dataset like so: . g7_df.loc[&#39;Canada&#39;:&#39;Germany&#39;] . Population GDP Surface Area . Canada 35 | 1.7 | 9.0 | . France 63 | 2.8 | 0.6 | . Germany 80 | 3.8 | 0.3 | . If we&#39;re only interested in the population and GDP of the first three countries, then we could instead do the following: . g7_df[[&#39;Population&#39;, &#39;GDP&#39;]].loc[&#39;Canada&#39;:&#39;Germany&#39;] . Population GDP . Canada 35 | 1.7 | . France 63 | 2.8 | . Germany 80 | 3.8 | . Noting that loc accepts two inputs, one for the selection of rows and one for columns, we can achieve the above more concisely as follows: . g7_df.loc[&#39;Canada&#39;:&#39;Germany&#39;, [&#39;Population&#39;, &#39;GDP&#39;]] . Population GDP . Canada 35 | 1.7 | . France 63 | 2.8 | . Germany 80 | 3.8 | . Filtering . Since the loc method also accepts a Boolean mask as input, we use it to filter the DataFrame by row. For instance, suppose we want the GDP of countries with a population over 70 mln. We query the dataset as follows: . g7_df.loc[g7_df[&#39;Population&#39;] &gt;= 70, &#39;GDP&#39;] . Germany 3.8 Japan 4.6 US 1.7 Name: GDP, dtype: float64 . As additional exercise, suppose we are only interested in the population and GDP of countries that are smaller than 1.0 mln square kilometers. . g7_df.loc[g7_df[&#39;Surface Area&#39;] &lt;= 1.0, [&#39;Population&#39;,&#39;GDP&#39;]] . Population GDP . France 63 | 2.8 | . Germany 80 | 3.8 | . Italy 60 | 2.1 | . Japan 127 | 4.6 | . UK 64 | 2.9 | . Adding, Dropping, and Renaming Columns and Rows . Let&#39;s add a &#39;Languages&#39; column to g7_df. We simply follow the same syntax as adding a key to a dictionary... . languages = pd.Series({ &#39;Canada&#39; : &#39;French, English&#39;, &#39;France&#39; : &#39;French&#39;, &#39;Germany&#39; : &#39;German&#39;, &#39;Italy&#39; : &#39;Italian&#39;, &#39;Japan&#39; : &#39;Japanese&#39;, &#39;UK&#39; : &#39;English&#39;, &#39;US&#39; : &#39;English&#39; }) # Next, we add the series as a column to the DataFrame g7_df[&#39;Languages&#39;] = languages g7_df . Population GDP Surface Area Languages . Canada 35 | 1.7 | 9.0 | French, English | . France 63 | 2.8 | 0.6 | French | . Germany 80 | 3.8 | 0.3 | German | . Italy 60 | 2.1 | 0.3 | Italian | . Japan 127 | 4.6 | 0.3 | Japanese | . UK 64 | 2.9 | 0.2 | English | . US 318 | 1.7 | 9.0 | English | . We can also drop a column or a row using the drop() method. . Let&#39;s drop the newly created &#39;Languages&#39; column. To drop a column, we specify a columns argument in the drop() method like so: . g7_df.drop(columns = &#39;Languages&#39;) . Population GDP Surface Area . Canada 35 | 1.7 | 9.0 | . France 63 | 2.8 | 0.6 | . Germany 80 | 3.8 | 0.3 | . Italy 60 | 2.1 | 0.3 | . Japan 127 | 4.6 | 0.3 | . UK 64 | 2.9 | 0.2 | . US 318 | 1.7 | 9.0 | . The method drop() returns a new DataFrame which does not contain the unneeded column, but the original g7_df still contains this column. To prove this, let&#39;s print it: . g7_df . Population GDP Surface Area Languages . Canada 35 | 1.7 | 9.0 | French, English | . France 63 | 2.8 | 0.6 | French | . Germany 80 | 3.8 | 0.3 | German | . Italy 60 | 2.1 | 0.3 | Italian | . Japan 127 | 4.6 | 0.3 | Japanese | . UK 64 | 2.9 | 0.2 | English | . US 318 | 1.7 | 9.0 | English | . As we can see the &#39;Languages&#39; column is still there. The solution is to specify an inplace = True argument so that the column is dropped in-place. . g7_df.drop(columns = &#39;Languages&#39;, inplace = True) g7_df . Population GDP Surface Area . Canada 35 | 1.7 | 9.0 | . France 63 | 2.8 | 0.6 | . Germany 80 | 3.8 | 0.3 | . Italy 60 | 2.1 | 0.3 | . Japan 127 | 4.6 | 0.3 | . UK 64 | 2.9 | 0.2 | . US 318 | 1.7 | 9.0 | . In order to drop rows, we specify the index argument instead. Suppose we want to remove Canada and Italy from the dataset. . g7_df.drop(index = [&#39;Canada&#39;, &#39;Italy&#39;]) . Population GDP Surface Area . France 63 | 2.8 | 0.6 | . Germany 80 | 3.8 | 0.3 | . Japan 127 | 4.6 | 0.3 | . UK 64 | 2.9 | 0.2 | . US 318 | 1.7 | 9.0 | . It is also possible to rename a column or a row using the rename() method. . Suppose we&#39;d like to rename the columns to include the units of measurement, and suppose we&#39;d also like to expand the UK and US to their full names. . g7_df.rename( columns = { &#39;Population&#39; : &#39;Population (mln)&#39;, &#39;GDP&#39; : &#39;GDP (USD)&#39;, &#39;Surface Area&#39; : &#39;Surface Area (mln sq. km)&#39; }, index = { &#39;UK&#39; : &#39;United Kingdom&#39;, &#39;US&#39; : &#39;United States&#39; } ) . Population (mln) GDP (USD) Surface Area (mln sq. km) . Canada 35 | 1.7 | 9.0 | . France 63 | 2.8 | 0.6 | . Germany 80 | 3.8 | 0.3 | . Italy 60 | 2.1 | 0.3 | . Japan 127 | 4.6 | 0.3 | . United Kingdom 64 | 2.9 | 0.2 | . United States 318 | 1.7 | 9.0 | . Manipulating Columns . Those of us familiar with Excel know the functions feature which allows users to select specific cells or combinations of cells, perform algebraic or logical operations with their contents, and store the results in new cells. The way to do that in Pandas is, once again, through broadcasted operations. . For instance, suppose we&#39;d like to add a new &#39;GDP Per Capita&#39; column to the g7_df dataset. This is simply a matter of dividing the GDP of each country by its population. . Using broadcasted division, the code is simply: . g7_df[&#39;GDP Per Capita&#39;] = g7_df[&#39;GDP&#39;] / g7_df[&#39;Population&#39;] g7_df . Population GDP Surface Area GDP Per Capita . Canada 35 | 1.7 | 9.0 | 0.048571 | . France 63 | 2.8 | 0.6 | 0.044444 | . Germany 80 | 3.8 | 0.3 | 0.047500 | . Italy 60 | 2.1 | 0.3 | 0.035000 | . Japan 127 | 4.6 | 0.3 | 0.036220 | . UK 64 | 2.9 | 0.2 | 0.045312 | . US 318 | 1.7 | 9.0 | 0.005346 | . Worked Example - Bitcoin Price Timeseries: Cleaning and Reindexing . Sometimes we may wish to use a certain column to index a DataFrame. For instance, if we&#39;re working with a dataset of Bitcoin prices, it would be wise to use the &#39;Time&#39; column as the index so that we can easily access the value of Bitcoin at a given time. . Note: Data that&#8217;s indexed by time is called a timeseries... . For this example, we will retrieve the actual daily Bitcoin price history from CoinCap API 2.0. Feel free to check out the code that fetches the data as JSON and converts it into a Pandas DataFrame in the collapsable code below. . import requests import json # Specifying request URL, payload, and headers url = &#39;https://api.coincap.io/v2/assets/bitcoin/history?interval=d1&#39; payload = {} headers = {} # Making the request and parsing it as JSON response = requests.request(&#39;GET&#39;, url, headers = headers, data = payload) json_data = json.loads(response.text)[&#39;data&#39;] # Converting the result into a DataFrame bitcoin_df = pd.json_normalize(json_data) # Cleanup bitcoin_df.rename( columns = { &#39;priceUsd&#39; : &#39;priceInUSD&#39;, &#39;time&#39; : &#39;Time&#39;, &#39;date&#39; : &#39;Date&#39; }, inplace = True ) . . The result of this is the following DataFrame: . bitcoin_df.head() . priceInUSD Time Date . 0 37480.8939504110899664 | 1610668800000 | 2021-01-15T00:00:00.000Z | . 1 36853.8623471143090244 | 1610755200000 | 2021-01-16T00:00:00.000Z | . 2 35670.6623897365179078 | 1610841600000 | 2021-01-17T00:00:00.000Z | . 3 36061.4760792230247237 | 1610928000000 | 2021-01-18T00:00:00.000Z | . 4 36868.3293610208260669 | 1611014400000 | 2021-01-19T00:00:00.000Z | . Now that we have the dataset as a cleaned-up Pandas DataFrame called bitcoin_df, we can get to work. . First order of business is to re-index the dataset based on the &#39;Time&#39; column. We can set a column as index using the set_index() method in the following way: . bitcoin_df.set_index(&#39;Time&#39;, inplace=True) bitcoin_df.index.name = None # The index column shouldn&#39;t have a name — this removes the name &#39;Time&#39; bitcoin_df.head() . priceInUSD Date . 1610668800000 37480.8939504110899664 | 2021-01-15T00:00:00.000Z | . 1610755200000 36853.8623471143090244 | 2021-01-16T00:00:00.000Z | . 1610841600000 35670.6623897365179078 | 2021-01-17T00:00:00.000Z | . 1610928000000 36061.4760792230247237 | 2021-01-18T00:00:00.000Z | . 1611014400000 36868.3293610208260669 | 2021-01-19T00:00:00.000Z | . As we can see the column that was previously named &#39;Time&#39; now acts as index. . Next, we should convert the entries of the index from a Unix timestamp into a Python datetime for more clarity. While doing this, let&#39;s also convert the entries of the &#39;Date&#39; column which are currently in string format. . bitcoin_df.index = pd.to_datetime(bitcoin_df.index, unit=&#39;us&#39;) # Converting index to datetime from Unix seconds bitcoin_df[&#39;Date&#39;] = pd.to_datetime(bitcoin_df[&#39;Date&#39;]) # Converting &#39;Date&#39; to datetime from string bitcoin_df.head() . priceInUSD Date . 2021-01-15 37480.8939504110899664 | 2021-01-15 00:00:00+00:00 | . 2021-01-16 36853.8623471143090244 | 2021-01-16 00:00:00+00:00 | . 2021-01-17 35670.6623897365179078 | 2021-01-17 00:00:00+00:00 | . 2021-01-18 36061.4760792230247237 | 2021-01-18 00:00:00+00:00 | . 2021-01-19 36868.3293610208260669 | 2021-01-19 00:00:00+00:00 | . Now we can comfortably access the price of Bitcoin on any given day. Suppose we&#39;d like to know its price on 2021-12-28, the day of writing this post... We can simply do: . bitcoin_df.loc[&#39;2021-12-28&#39;, &#39;priceInUSD&#39;] . &#39;48995.0145281203441155&#39; . Common Workflows . Finding the Unique Elements in a Column . Pandas comes with the method unique() which can be applied to a Series object. . Let&#39;s fetch some data about the planets in our solar system from the devstronomy repository. . planets_df = pd.read_csv(&#39;https://raw.githubusercontent.com/devstronomy/nasa-data-scraper/master/data/csv/planets.csv&#39;) planets_df . planet mass diameter density gravity escape_velocity rotation_period length_of_day distance_from_sun perihelion ... orbital_period orbital_velocity orbital_inclination orbital_eccentricity obliquity_to_orbit mean_temperature surface_pressure number_of_moons has_ring_system has_global_magnetic_field . 0 Mercury | 0.3300 | 4879 | 5427 | 3.7 | 4.3 | 1407.6 | 4222.6 | 57.9 | 46.0 | ... | 88.0 | 47.4 | 7.0 | 0.205 | 0.034 | 167 | 0 | 0 | No | Yes | . 1 Venus | 4.8700 | 12104 | 5243 | 8.9 | 10.4 | -5832.5 | 2802.0 | 108.2 | 107.5 | ... | 224.7 | 35.0 | 3.4 | 0.007 | 177.400 | 464 | 92 | 0 | No | No | . 2 Earth | 5.9700 | 12756 | 5514 | 9.8 | 11.2 | 23.9 | 24.0 | 149.6 | 147.1 | ... | 365.2 | 29.8 | 0.0 | 0.017 | 23.400 | 15 | 1 | 1 | No | Yes | . 3 Mars | 0.6420 | 6792 | 3933 | 3.7 | 5.0 | 24.6 | 24.7 | 227.9 | 206.6 | ... | 687.0 | 24.1 | 1.9 | 0.094 | 25.200 | -65 | 0.01 | 2 | No | No | . 4 Jupiter | 1898.0000 | 142984 | 1326 | 23.1 | 59.5 | 9.9 | 9.9 | 778.6 | 740.5 | ... | 4331.0 | 13.1 | 1.3 | 0.049 | 3.100 | -110 | Unknown* | 79 | Yes | Yes | . 5 Saturn | 568.0000 | 120536 | 687 | 9.0 | 35.5 | 10.7 | 10.7 | 1433.5 | 1352.6 | ... | 10747.0 | 9.7 | 2.5 | 0.057 | 26.700 | -140 | Unknown* | 62 | Yes | Yes | . 6 Uranus | 86.8000 | 51118 | 1271 | 8.7 | 21.3 | -17.2 | 17.2 | 2872.5 | 2741.3 | ... | 30589.0 | 6.8 | 0.8 | 0.046 | 97.800 | -195 | Unknown* | 27 | Yes | Yes | . 7 Neptune | 102.0000 | 49528 | 1638 | 11.0 | 23.5 | 16.1 | 16.1 | 4495.1 | 4444.5 | ... | 59800.0 | 5.4 | 1.8 | 0.011 | 28.300 | -200 | Unknown* | 14 | Yes | Yes | . 8 Pluto | 0.0146 | 2370 | 2095 | 0.7 | 1.3 | -153.3 | 153.3 | 5906.4 | 4436.8 | ... | 90560.0 | 4.7 | 17.2 | 0.244 | 122.500 | -225 | 0.00001 | 5 | No | Unknown | . 9 rows × 21 columns . If we want to find out the unique number of moons each planet has, we can simply do: . planets_df[&#39;number_of_moons&#39;].unique() . array([ 0, 1, 2, 79, 62, 27, 14, 5], dtype=int64) . As we can see, the 9 planets in our solar system (counting Pluto) have 8 unique number of moons. This is because, as we can see from the dataset, Mercury and Venus both have 0 moons. . Saving Data . After all the data manipulation, it would be useful to save the resulting dataset locally on our machine. Pandas offers us a way to do that using the DataFrame.to_csv() method. . Working with the planets_df defined above, we can narrow the dataset down to the planets which have a gravitational force that&#39;s close to that of the Earth ($9.8 m/s^2$). . earthlike_planets_df = planets_df[(planets_df[&#39;gravity&#39;] &gt;= 9.8 - 1) &amp; (planets_df[&#39;gravity&#39;] &lt;= 9.8 + 1)] earthlike_planets_df . planet mass diameter density gravity escape_velocity rotation_period length_of_day distance_from_sun perihelion ... orbital_period orbital_velocity orbital_inclination orbital_eccentricity obliquity_to_orbit mean_temperature surface_pressure number_of_moons has_ring_system has_global_magnetic_field . 1 Venus | 4.87 | 12104 | 5243 | 8.9 | 10.4 | -5832.5 | 2802.0 | 108.2 | 107.5 | ... | 224.7 | 35.0 | 3.4 | 0.007 | 177.4 | 464 | 92 | 0 | No | No | . 2 Earth | 5.97 | 12756 | 5514 | 9.8 | 11.2 | 23.9 | 24.0 | 149.6 | 147.1 | ... | 365.2 | 29.8 | 0.0 | 0.017 | 23.4 | 15 | 1 | 1 | No | Yes | . 5 Saturn | 568.00 | 120536 | 687 | 9.0 | 35.5 | 10.7 | 10.7 | 1433.5 | 1352.6 | ... | 10747.0 | 9.7 | 2.5 | 0.057 | 26.7 | -140 | Unknown* | 62 | Yes | Yes | . 3 rows × 21 columns . . Tip: Pandas prefers the use of bitwise Boolean operators &amp; and |, instead of the Python&#8217;s default and and or. This is because Pandas relies on NumPy, which in turn relies on the capacity of the bitwise operators to be overloaded. . We can now save this new dataset to our desktop as follows: . earthlike_planets_df.to_csv(&#39;C:/Users/Vahram/Desktop/earthlike_planets.csv&#39;) .",
            "url": "https://v-poghosyan.github.io/blog/python%20for%20ml/pandas/machine%20learning/2021/12/17/Python-for-ML-Pandas.html",
            "relUrl": "/python%20for%20ml/pandas/machine%20learning/2021/12/17/Python-for-ML-Pandas.html",
            "date": " • Dec 17, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Thanks for dropping by! I’m Vahram, a software developer and graduate student of Computer Science @ UT Austin. . 🎓 Education . After graduating from community college in 2018, I transfered to UCLA where I pursued my undergraduate studies in Applied Mathematics. In 2020, having graduated from UCLA with honors, I enrolled in the M.S. in Computer Science program @ UT Austin. . 💻 Work . While still at community college I got my first job as a Software Engineer Intern @ Omron Automation. There, I was part of the team that developed embedded systems for the motion controllers and control panels, as well as the team that built and maintained the company’s proprietary Visual Studio based IDE. . In the Summer of 2022 I will be joining the team over @ Capital One as a Software Engineer Intern. . 🙋‍♂️ Social Media . For a detailed rundown of my work history, check out my LinkedIn. . Feel free to also email me at: vahram@utexas.edu .",
          "url": "https://v-poghosyan.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://v-poghosyan.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}