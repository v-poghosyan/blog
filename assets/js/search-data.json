{
  
    
        "post0": {
            "title": "Python for Machine Learning - NumPy",
            "content": "Importing NumPy . NumPy is a scientific computing library for Python. It&#39;s an extensive collection of pre-written code that optimizes and extends, among other things, the Python array (i.e. list) object into an n-dimensional NumPy array called ndarray. It comes with a variety of tools, such as matrix operations and common mathematical functions, that enable Python to perform complex mathematical tasks such as solve linear algebraic problems, generate pseudo-random numbers, perform Fourier analysis, etc. . We import NumPy, as we import any other library, using the import keyword (with or without a shorthand). . import numpy . Or, alternatively: . import numpy as np . Optimizations . As we&#39;ve briefly discussed in the &quot;Python for Machine Learning - Pandas&quot; post, NumPy works by delegating tasks to well-optimized C code under the hood. In this way it exploits the flexibility of Python while bypassing its speed limitations as an interpreted language and, instead, exploiting the speed advantages of a compiled language. . Scalable Memory Representation . One of the things NumPy optimizes is data storage. In contrast to Python 3.x&#39;s scalable memory representation of numeric values, such as integers, which can grow to accomodate a given number, NumPy stores numeric types in fixed-sized blocks of memory (e.g. int32 or int64). This means NumPy is able to take advantage of the low-level CPU instructions of modern processors that are designed for fixed-sized numeric types. Another advantage of fixed-sized storage is that consecutive blocks of memory can be allocated, which enables the libraries upon which NumPy relies to do extremely performant computations. This enforcement of fixed-sized data types is part of the optimization strategy NumPy uses called vectorization. . Vectorization . As already discussed in the aforementioned post, vectorization is the process by which NumPy stores an array internally in a contiguous block of memory, and restricts its contents to only one data type. Letting Python know this data type in advance, NumPy can then skip the per-iteration type checking that Python normally does as its iterating through a loop in order to speed up our code. Optimizing the array data structure in such a way enables NumPy to delegate most of the operations on such arrays to pre-written C code under the hood. In effect, this simply means that looping occurs in C instead of Python. . Broadcasting . The term broadcasting describes the process by which NumPy performs arithmetic operations on arrays of different dimensions. The process is usually as follows: the smaller array is “broadcast” across the larger array so that the two arrays have compatible dimensions. Broadcasting provides a means of vectorizing array operations. . Comparing Runtime . To demonstrate the performance optimizations of NumPy, let&#39;s compare squaring every element of a 1,000,000-element array and summing the results. . Using a Python List . First, we will use a Python list: . unoptimized_list = list(range(1000000)) . Squaring each element and summing: . import numpy as np %timeit np.sum([i**2 for i in unoptimized_list]) . 320 ms ± 2.81 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) . . Note: Even though we&#8217;re using NumPy&#8217;s sum() method, since the input we&#8217;re passing to it is a regular Python list, NumPy optimizations are not applied. . As we can see the whole thing took about 314 ms. . Using a NumPy Array . Now let&#39;s do the same with a NumPy array, which also gives us the opportunity to intruduce the syntax for defining one using a range. . optimized_array = np.arange(1000000) . Let&#39;s check the type of optimized_array to convince ourselves that it is, indeed, a NumPy ndarray. . type(optimized_array) . numpy.ndarray . Now, finally, let&#39;s square each element and sum the results: . %timeit np.sum(optimized_array**2) . 1.61 ms ± 19.7 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each) . Remarkably, the run-time was cut from 314 ms to only around 1.61ms! . NumPy Basics . Let&#39;s exlpore some of the ways in which we can represent arrays and matrices in NumPy. . Creating Arrays . We&#39;ve already seen how we can create a 1-dimensional NumPy array of consecutive integers $0,1,...,n-1$ using the arrange() method. . The standard way of creating a NumPy array is passing a Python list to the constructor array() like so: . a = np.array([1,2,3]) a . array([1, 2, 3]) . We can also create some sommon arrays, such as an array of consecutive integers, with some special methods such as arange(), which takes an integer $n$ as input and creates a sequential array from $0,...,n-1$. . np.arange(5) # array([0, 1, 2, 3, 4]) . array([0, 1, 2, 3, 4]) . Representing Matrices . Let&#39;s represent a $2 times 3$ matrix $ A = begin{bmatrix} 1 &amp; 2 &amp; 3 4 &amp; 5 &amp; 6 end{bmatrix} $ using NumPy: . A = np.array([[1,2,3], [4,5,6]]) A . array([[1, 2, 3], [4, 5, 6]]) . There are also ways to quickly create some common matrices using special methods. . For example, ones() accepts a shape pair, and creates a matrix of $1$s with of the given shape. . np.ones((2,3)) . array([[1., 1., 1.], [1., 1., 1.]]) . The method, zeros() works the same way as ones(): . np.zeros((2,3)) . array([[0., 0., 0.], [0., 0., 0.]]) . Meanwhile, identity() accepts an integer $n$ as input and creates a square $n times n$ identity matrix. . np.identity(3) . array([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]]) . Creating a matrix with identical elements in general uses the full() method which takes a shape attribute, a value attribute and on optional dtype attribute as follows: . np.full((2,3), 7, dtype = int) . array([[7, 7, 7], [7, 7, 7]]) . Indexing . Indexing a 1-dimensional NumPy array is done as expected, through the use of the trusty brackets []. Indexing an n-dimensional matrix in NumPy still uses [] but it introduces a new, improved, syntax. . Suppose we&#39;d like to access the element in the first row, and last column of A. The standard way would be: . A[0][2] . 3 . As we can see, that still works. But the recommended and, subjectively speaking, prettier way is: . A[0,2] . 3 . Of course, slicing still works as expected. . For example, let&#39;s print the entire first row of A: . A[0,:] . array([1, 2, 3]) . The entire first column: . A[:,0] . array([1, 4]) . Finally, let&#39;s print the submatrix $ begin{bmatrix} 2 &amp; 3 end{bmatrix} $: . A[0,1:] . array([2, 3]) . Properties and Methods of NumPy Arrays . A few of the useful properties and methods of ndarray are highlighted in this section. . shape - returns the shape of the matrix as an $(m,n)$ pair . A.shape # (2,3) . | . ndim - returns the dimension of a matrix as a single digit . A.ndim # 2 . . Note: The output of the ndim property should not be understood in a linear algebraic sense as the dimension of either the domain or range of the corresponding transformation, nor the dimension of either of its four fundamental subspaces. It is to only be understood in the data structure sense as the level of nestedness of the array. . | . size - returns the total number of elements in the matrix . A.size # 6 . | . dtype - returns the data type of the elements in the matrix. . A.dtype # dtype(&#39;int32&#39;) . . Note: If the ndarray does not represent a matrix, such as B = np.array([[1,2,3],[4,5]]) then dtype outputs O signifying that the entries are general Python objects. In such a case, the array loses its optimizations. . | . Statistical and Mathematical Methods . There is also a vast selection of statstical, and more generally, mathematical methods that ndarrays come with. Here are a few of the common ones: . sum() - returns the sum of all the entries . A.sum() # 21 . It also accepts an axis attribute where axis = 0 refers to the sum along the columns, and axis = 1 refers to the sum along the rows. . A.sum(axis = 0) # [5,7,9] . A.sum(axis = 1) # [6,15] . | mean() - returns the empirical mean of all the entries . A.mean() # 3.5 . | var() - returns the variance of the entries . A.var() # 2.9166666666666665 . | std() - returns the standard deviation of the entries . A.std() # 1.707825127659933 . | . Multi-Indexing, Filtering, and Broadcasted Operations . Recall from the Pandas article the ways in which we were able to multi-index and filter, and how we eliminated the need for using Python loops and list comprehensions using broadcasted operators instead. Since both a Pandas Series and a DataFrame are extensions of NumPy&#39;s ndarray, all of these apply here as well. . As a refresher on broadcasted operations, here are a few filtering examples. . Let&#39;s obtain those elements of A that are greater than 3: . A[A &gt; 3] . array([4, 5, 6]) . Now let&#39;s obtain those elements of A that are greater than the empirical mean: . A[A &gt; A.mean()] . array([4, 5, 6]) . What about those elements of A that are less than or equal to the empirical mean? . A[~(A &gt; A.mean())] . array([1, 2, 3]) . Which is equivalent to: . A[A &lt;= A.mean()] . array([1, 2, 3]) . Matrix Operations . One of NumPy&#39;s key selling points is that it makes matrix operations in Python easy. It offers simple syntax to add, multiply, transpose, invert, flatten, etc. matrices. . Addition . Addition of matrices is, by defualt, per-element (as are all NumPy opertions). There&#39;s no special syntax, it&#39;s done through the + operator. . For example: . A = np.ones((2,3)) B = np.ones((2,3)) A + B . array([[2., 2., 2.], [2., 2., 2.]]) . Multiplication . The operator * performs per-element multiplication. . A = np.full((2,3), 2, dtype = int) B = np.full((2,3), 3, dtype = int) A * B . array([[6, 6, 6], [6, 6, 6]]) . But this, as we know, isn&#39;t matrix multiplication as it&#39;s commonly defined in mathematics — the dot product of corresponding rows and columns. For instance, if we try to multiply an $m times n$ matrix with an $n times p$ matrix, NumPy will throw the following error: . A = np.full((2,3), 2, dtype = int) B = np.full((3,4), 3, dtype = int) A * B . ValueError: operands could not be broadcast together with shapes (2,3) (3,4) . This is because per-element operations require the shapes of the operands to be the same or compatible up to broadcasting. Here, since the shapes are different, NumPy attempts to broadcast one operand to match the shape of the other. But broadcasting is impossible between matrices of shapes $2 times 3$ and $3 times 4$ per the broadcasting rules. . There&#39;s a workaround that lets us use *. Since NumPy overloads the * operator, it works as it should for numpy.matrix types. . A = np.matrix([[2,2,2], [2,2,2]]) B = np.matrix([[3,3,3,3], [3,3,3,3], [3,3,3,3]]) A * B . matrix([[18, 18, 18, 18], [18, 18, 18, 18]]) . However, this is not the recommended way to do matrix multiplication in NumPy. Overloaded operators can produce convoluted code. For instance, we may have many different matrix and ndarray data structures and be unable to anticipate the result of a given * operation. . Instead, the recommended way to do matrix multiplication is through the @ operator. . When we use @ NumPy internally uses its matmul() method. So, the following are equivalent and both produce the matrix product of A and B. . A = np.full((2,3), 2, dtype = int) B = np.full((3,4), 3, dtype = int) A @ B . array([[18, 18, 18, 18], [18, 18, 18, 18]]) . A = np.full((2,3), 2, dtype = int) B = np.full((3,4), 3, dtype = int) np.matmul(A,B) . array([[18, 18, 18, 18], [18, 18, 18, 18]]) . NumPy also offers, dot() which is equivalent to matmul() for 1D and 2D matrices. So, the following is yet another way we can multiply matrices: . A = np.full((2,3), 2, dtype = int) B = np.full((3,4), 3, dtype = int) A.dot(B) . array([[18, 18, 18, 18], [18, 18, 18, 18]]) . However, matmul() is preferred over dot() because of the clarity of its name, and because the dot product has a distinct mathematical meaning separate from matrix multiplication. . Transposing .",
            "url": "https://v-poghosyan.github.io/blog/python%20for%20ml/numpy/machine%20learning/2022/01/27/Python-for-ML-NumPy.html",
            "relUrl": "/python%20for%20ml/numpy/machine%20learning/2022/01/27/Python-for-ML-NumPy.html",
            "date": " • Jan 27, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Optimization - Review of Linear Algebra and Geometry",
            "content": "Introduction . The study of optimization can be summed up as the attempt to find those parameter(s) that optimize some objective function, if such exist. The objective function can be almost anything — cost, profit, nodes in a wireless network, distance to a destination, similarity to a target image, etc. If the objective function describes cost we may wish to minimize it. If, on the other hand, it describes profit then a natural goal would be to maximize it. . The problems of minimization and maximization, summed up as optimization in one word, are the same problem up to a reflection with respect to the domain of the parameter(s). . Formally, let the objective function be $f: mathbb{R^n} to mathbb{R}$, and let it have minimizer $x^* in mathbb{R^n}$. Then, by definition of minimizer, $f(x^*) leq f(x) forall x in mathbb{R^n}$. It follows that $-f(x^*) geq -f(x) forall x in mathbb{R^n}$, so $x^*$ is the maximizer for $-f$. . Model of a Convex Optimization Problem . This series of posts will cover the ways in which we can solve an optimization problem of the form . $ textrm{minimize}: f(x) textrm{subject to}: x in mathcal{X} $ . where the objective function $f$ is a convex function, and the constraint set $ mathcal{X}$ is a convex set. Importantly, we will not cover the ways in which we can model a real-world problem as a convex optimization problem of the above form. . Why Convex Optimization? . First, let&#39;s define the size of an optimization problem as the dimensionality of the parameter $x$ added to the number of the problem constraints. . Convex optimization problems are a class of easy optimization problems — problems whose time and/or space complexity grows slowly with respect to problem size. . These problems are general enough to capture many scenarios of interest, even some that do not fall strictly into the convex category, but specific enough to be solvable through generic algorithms and numerical methods. . Review of Linear Algebra and Geometry . We start our exploration of convex optimization with a refresher on convexity and the linear algebra that&#39;s in common use in the subject. . Convexity . Set convexity is defined as follows: . Definition:&nbsp; A set $C subseteq mathbb{R^d}$ is convex if, for all points $x_1,x_2 in C$ and any $ theta in [0,1]$, the point $ theta x_1 + (1- theta) x_2$ (i.e. the parametrized line segment between $x_1$ and $x_2$) is also in $C$. . Some Operations that Preserve Convexity . Shifting, scaling, and rotation (i.e. affine transformations) preserve convexity. Let the matrix $A$ define such a transformation, and $b$ be a shift vector. Then $C&#39; = {Ax + b | x in C }$ is convex provided that $C$ was convex. . An intersection of convex sets is also convex. That is, $C&#39; = { x | x in C_1 cap x in C_2 }$ is convex provided that $C_1$ and $C_2$ were convex to begin with. The proof follows directly from the definition of intersection... . However, unions of convex sets need not be convex... . Examples of Convex Sets . The following are some common convex sets we will come across in practice. . Convex Hull of $n$ Points . A convex combination of points $x_1, ..., x_n$ is a point of the form $x = theta_1 x_1 + ... + theta_n x_n$ where $ sum_{i = 1}^{n} theta_i = 1$ and $ theta_i geq 0 forall i$. . Let $x_1,x_2,...,x_n$ be $n$ points in space. Their convex hull is the set of all points which can be written as some convex combination of them. Equivalently, by varying the $theta_i$&#39;s we generate the convex hull as the set of all convex combinations of these points. . The convex hull can be visualized as the closed polygon formed when a rubber band is stretched around the $n$ points. The convex hull of two points is the line segment joining them. That of three points is the polygon (complete with its inner region). In general, for $n$ points, the concept generalizes to an $n$-dimensional polygon. . Formally, the convex hull is the set $ { theta_1 x_1 + ... + theta_n x_n | theta_1 + ... + theta_n = 1 textrm{and} theta_i geq 0 forall i }$ . Note: A handy interactive visualization, along with an efficient algorithm that generates a convex hull of $n$ points on a 2D plane can be found in the following blog post by Joel Gibson. . Convex Hull of a Set . The convex hull of a set can be similarly defined as all the convex combinations of the elements in the set. However, since the set may contain infinite elements, there&#39;s a more helpful, equivalent definition... . Let $C$ be a non-convex set. The convex hull of $C$ is the intersection of all convex supersets of $C$. That is, it&#39;s the intersection of all convex sets containing $C$. The result of such an intersection will be the smallest convex superset of $C$. . In fact, this minimal convex superset is unique 1 and can therefore be taken as yet another, equivalent, definition for the convex hull of a set. . Visualizing the convex hull of a non-convex set is similar to visualizing that of $n$ points — simply imagine the shape enclosed by a rubber band stretched around the non-convex set. . Affine Combination of $n$ Points . An affine combination of points $x_1,...,x_n$ is a point of the form $x = theta_1 x_1 + ... + theta_n x_n$ with $ sum_{i=1}^{n} theta_i = 1$ but where the $ theta_i$&#39;s need not be non-negative. . For two points, the set of all affine combinations is the line that passes through them, whereas for three points it&#39;s the plane. in general, it is the plane in $(n-1)$-dimensions passing through the $n$ points. . Linear Combinations - Hyperplanes and Halfspaces . A linear combination of $n$ points is all points of the form $x = theta_1 x_1 + ... + theta_n x_n$ with the $ theta_i$&#39;s totally unrestricted. . The set of all linear combinations of a point, called its span, is simply the line passing through it. Formally, the span of $n$ points is $ { theta_1 x_1 + ... + theta_n x_n | forall theta_1,..., theta_n }$. For two points the span is the plane passing through them and, in general, the span of $n$ points is a plane in $n$-dimensions that contains these points. . Hyperplanes . For fixed weights $ theta_i = a_i forall i$, a hyperplane is the set of all points $x in mathbb{R^n}$ whose linear combination equals a fixed constant $b in mathbb{R}$. . Formally, a hyperplane is the set $ { x | a_1 x_1 + ... a_n x_n = b } = { x | a^T x = b }$ . There&#39;s a geometric interpretation of the parameters $a in mathbb{R^n}$ and $b in mathbb{R}$. Since the dot-product between perpendicular vectors is $0$, $ { x | a^T x = 0 }$ is simply the set of all vectors perpendicuar to $a$ (whose tail, as with all vectors in linear algebra, is considered to be fixed at the origin), making $a$ the normal vector to the hyperplane passing through the origin. To allow for parallel hyperplanes that are translated from the origin, the offset $b in mathbb{R}$ is introduced in the generalization $ { x | a^T x = b }$. This is now the set of all vectors whose dot-product with $a$ is constant. These vectors are not quite perpendicular to $a$, but they form a parallel hyperplane that&#39;s been shifted from the origin by a distance of $ frac{|b|}{ |a |_2}$. . Since the sum $a_1 x_1 + ... a_n x_n = b$ is fixed, the last coordinate, which we&#39;ll call $x_k$ for some $k in [1,...,n]$, is fixed by the choice of the other $n-1$ coordinates. Therefore, a hyperplane in $ mathbb{R^n}$ spans $n-1$ dimensions instead of $n$. . Halfspaces . A halfspace is either of the two sub-spaces a hyperplane partitions the whole space into. Since the dot-product between vectors which are roughly in the same direction is positive, and vice versa, the two halfspaces associated to a hyperplane $ { x | a^T x = b }$ are $ { x | a^T x geq b }$ and $ { x | a^T x leq b } $. . Conic Combinations of $n$ Points . A conic combination of $x_1,...x_n$ is a point $x = sum_{i=1}^{n} theta_i x_i$ where $ theta_i geq 0 forall i$. Note that the absence of the restriction that $ sum_{i=1}^{n} theta_i = 1$ is what distinguishes a conic combination from a convex combination. . A visual example: . . Ellipses . Recall from Euclidean geometry that ellipses are conic sections. In general we define ellipses in $n$-dimensions as the sub-level sets of quadratic forms. That is $ { x | (x-c)^T M (x-c) leq 1 }$ where $M succeq 0$ defines the strech along each principal axis, and $c in mathbb{R^n}$ is the center. . An equivalent definition of an ellipse using the L2-norm is $ { x | |Ax - b |_2 leq 1 }$. That is, for a given $A$ and $b$ in the L2-norm definition, we can find an $M$ and $c$ in the sub-level set definition and vice versa. . Note: More generally, the ellipse is $ { x | (x-c)^T M (x-c) leq r }$. However, since the scaling factor $r$ is positive, it can simply be absorbed into $Q$ without affecting $Q$&#8217;s positive semidefiniteness. . To quickly convince ourselves in the equivalence of these definitions, we take the simple case where $b = 0$. . $$ begin{aligned} |Ax |_2 &amp;= ((Ax)^T(Ax))^{1/2} &amp;= (x^TA^TAx)^{1/2} &amp;= (x^TU D U^Tx)^{1/2} &amp;= x^TU D^{1/2} U^Tx end{aligned} $$Where the third equality is by the spectral decomposition of the real symmetric matrix $A^TA$, in which $D = diag( lambda_1,..., lambda_n)$ is the diagnonal matrix of eigenvalues and the columns of $U$ are the corresponding eigenvectors. Taking $M= UD^{1/2}U^T$, where $D^{1/2}$ is simply $D^{1/2} = diag( sqrt lambda_1,..., sqrt lambda_n)$, we have the equivalent sub-level set definiton of the ellipse. . Norm Balls . Related to ellipses are Euclidean balls, which are norm balls for the choice of the L2-norm. A Euclidean ball has the form $ { x | |x |_2 leq r }$, and is clearly convex as it&#39;s a generalizations of the sphere in $n$-dimensions. . But also, a Euclidean ball is the special ellipse for the choice of $M = rI$, and $c = 0$. . In general, norm balls $ { x | |x |_p leq r }$ where $ |x |_p = (x_1^p + ... + x_n^p)^{1/p}$ are convex for any choice of $p geq 1$. . Polyhedra . Where a halfspace is a set with one linear inequality constraint, a polyhedron is a set with many, but finite, such linear inequality constraints. These constraints can be packed into a matrix $A in mathbb{R^{m times n}}$ by vector $b in mathbb{R^m}$ multiplication form, making the polyhedron into the set $ {x | Ax leq b }$. . Since polyhedra are simply intersections of halfspaces and hyperplanes, and the latter are both convex, polyhedra are also convex sets. . The Set of All Positive Semidefinite Matrices . The set of all PSD matrices $ { Q | x^TQx geq 0 forall x in mathbb{R^m} }$ is convex. We can, of course, use the definition of convexity to show this. But, a more elucidative approach would be the following remark. . Note that $Q mapsto x^TQx$ is a linear functional that maps the space of all PSD matrices to its field of scalars. This is analogous to how $a mapsto x^Ta$ is a linear functional so, just as $ { a | x^Ta geq 0 }$ is a halfspace in the space of vectors, $H_x = { Q | x^TQx geq 0 }$ for a given choice of $x in mathbb{R^m}$ is a halfspace in the space of PSD matrices. Halfspaces, as we already know, are convex and $ { Q | x^TQx geq 0 forall x in mathbb{R^m} }$ is nothing but an intersection of halfspaces for each choice of $x$. That is, $ { Q | x^TQx geq 0 forall x in mathbb{R^m} } = bigcap_x H_x$, concluding the proof of its convexity. . 1. Proof of uniqueness of the minimal, convex superset: Suppose $C_1$ and $C_2$ are both minimal, convex supersets of $C$. Any convex set $D$ that contains $C$ must clearly contain the minimal, convex superset. Hence, $C_1 subseteq C_2$ and $C_2 subseteq C_1$, which implies $C_1 = C_2$.↩ .",
            "url": "https://v-poghosyan.github.io/blog/optimization/applied%20mathematics/proofs/2022/01/23/Optimization-Review-of-Linear-Algebra-and-Geometry.html",
            "relUrl": "/optimization/applied%20mathematics/proofs/2022/01/23/Optimization-Review-of-Linear-Algebra-and-Geometry.html",
            "date": " • Jan 23, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Machine Learning - Decision Trees",
            "content": "Introduction . Classification vs. Regression . We start our discussion of decision trees with a definition of classification and classifier. . Definition:&nbsp; Classification is the process of grouping data into discrete categories (i.e. class labels). . We may contrast this definition with regression which is the process of predicting a continous (i.e. real or complex-valued) output. . A common example of a classification problem is the sorting of emails into the binary categories of &#39;spam&#39; and &#39;not spam&#39;. However, the labels in a classification problem need not be binary — they may be any discrete set. Whereas a common example of regression is learning a linear (or a non-linear) function that best fits a given dataset. . Note: The line between classification and regression is sometimes blurred. For instance, logistic regression is a regression algorithm which outputs a prediction in the continous probability range $[0,1]$. It&#8217;s commonly used with a decision rule which casts its output into discrete classes. Thus, even though it&#8217;s a regression algorithm, it can easily be converted into a classification algorithm and is often used for classification problems in practice. . This leads us to the expected definition of a classifier, which is: . Definition:&nbsp; A classifier is any algorithm that performs classification. . Decision Trees . Decision trees are one type of powerful classifier among many. . The nodes of a decision tree correspond to the features of the dataset and its leaves correspond to the class labels. The paths in a decision tree correspond to the conjunction of features that lead to the class labels at its leaves. . To understand this, let&#39;s look at an example of a decision tree that&#39;s very easy to understand because of the historical context of the data it&#39;s attempting to learn. . Example: . # Imports import pandas as pd import numpy as np from sklearn.datasets import fetch_openml import pydotplus from sklearn.tree import DecisionTreeClassifier, export_graphviz from IPython.display import Image # Setting random state for replicability of results RS = 2022 # Fetching data as features and labels from OpenML as a Pandas dataframe X, y = fetch_openml(&quot;titanic&quot;, version = 1, as_frame = True, return_X_y = True) # Cleaning and engineering data: # Dropping unneeded columns and those with too many missing values... X.drop( columns = [ &#39;boat&#39;, &#39;body&#39;, &#39;home.dest&#39;, &#39;cabin&#39;, &#39;ticket&#39;, &#39;name&#39;, &#39;embarked&#39;, &#39;fare&#39; ], inplace = True ) # Dropping data points with any missing values and subseting labels accordingly X[&#39;labels&#39;] = y # Temporarily adding labels to data as a column X.dropna(inplace = True) # Dropping the rows with any null values y = X[&#39;labels&#39;] # Subsetting the labels X.drop(columns = &#39;labels&#39;, inplace = True) # Dropping the column of labels # Converting &#39;male&#39; and &#39;female&#39; feature values into the Boolean values 0 and 1 respectively X[&#39;sex&#39;] = X[&#39;sex&#39;].apply(lambda x: 0 if x == &#39;male&#39; else 1) # Defining a decision tree classifier clf = DecisionTreeClassifier( random_state = RS, max_depth = 3 ) # Training the decision tree classifier clf = clf.fit(X, y) # Visualizing the decision tree feature_cols = [] png = export_graphviz( clf, feature_names = X.columns, class_names = [&#39;died&#39;,&#39;survived&#39;], impurity = False, ) graph = pydotplus.graph_from_dot_data(png) Image(graph.create_png()) . . Bad pipe message: %s [b&#39;a/5.0 (Macintosh; Intel Mac OS X 10.14; rv:96.0) Gecko/20100101 Firefox/96.0 r nAccept: */* r nAccept-Language:&#39;] . The above decision tree has identified the isolated features, as well as the conjunction of features, that best predict the chances of a given passanger of the Titanic to survive. These features, ordered loosely in terms of importance, are sex, age, sibsp (number of siblings or spouses), and pclass (passanger class). . As we can infer from the tree, were you a male passanger (sex &lt;= 0.5) on the Titanic over the age of 9.5 you probably did not survive the crash. If, however, you were either a female passanger or a male child below the age of 9.5 with fewer than 2.5 siblings (a conjunction of features), it&#39;s likely that you survived. . Setup . Simplifying Assumptions . In the rest of this article, for simplicity, we will assume binary input and binary output for decision trees. That is, the training set is ${S = {(x^1,y^1), ... ,(x^k, y^k) }}$ with ${x^i in {0,1 }^n}$ and ${y^i in {0,1 } forall i}$. This means that the decision tree itself is simply a binary function which also receives binary input. . The task is to learn this function. . Potential Function .",
            "url": "https://v-poghosyan.github.io/blog/machine%20learning/decision%20trees/random%20forests/2021/12/29/Machine-Learning-Decision-Trees.html",
            "relUrl": "/machine%20learning/decision%20trees/random%20forests/2021/12/29/Machine-Learning-Decision-Trees.html",
            "date": " • Dec 29, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Python for Machine Learning - Pandas",
            "content": "Importing Pandas . Pandas is a library that contains pre-written code to help wrangle with data. We can think of it as Python&#39;s equivalent of Excel. . We import Pandas into our development environment as we import any other library — using the import command. . import pandas . It&#39;s standard to import Pandas with the shorthand pd in order to avoid typing pandas all the time. . import pandas as pd . This gives us access to a vast array of pre-built objects, functions, and methods which are detailed in the API reference. . Two Underlying Data Types . Series . The basic unit of Pandas is the pandas.Series object which, in keeping with the Excel analogy, can be thought of as a column in an Excel table. It&#39;s a one-dimensional data structure that&#39;s derived from a NumPy array. However, unlike a NumPy array, the indices of a Series object aren&#39;t limited to the integer values $0,1,...n$ — they can also be descriptive labels. . Let&#39;s create a Series object representing the populations of the G-7 countries in units of millions. . import pandas as pd g7_pop = pd.Series([35,63,80,60,127,64,318]) . As we can see, creating a series is a matter of passing a Python list (or a Numpy array) into the Series constructor. . Indexing . Indexing a Series object is similar to indexing a Python list. For instance, let&#39;s print the first element in the above series. . g7_pop[0] . 35 . Let&#39;s now swap out the integer-based indices with descriptive labels. Each Series object has an index argument that can be overwritten. . g7_pop.index = [ &#39;Canada&#39;, &#39;France&#39;, &#39;Germany&#39;, &#39;Italy&#39;, &#39;Japan&#39;, &#39;UK&#39;, &#39;US&#39; ] . Now, we can print the first element of the series using its descriptive label. . g7_pop[&#39;Canada&#39;] . 35 . We may notice a similarity between a standard Python dictionary and a labeled Series object. Namely, indexing a series with a label and keying into a Python dictionary have similar syntax. In fact, it&#39;s possible to create a labeled Series object directly from a Python dictionary. . g7_pop = pd.Series({ &#39;Canada&#39; : 35, &#39;France&#39; : 63, &#39;Germany&#39; : 80, &#39;Italy&#39; : 60, &#39;Japan&#39; : 127, &#39;UK&#39; : 64, &#39;US&#39; : 318 }) g7_pop . Canada 35 France 63 Germany 80 Italy 60 Japan 127 UK 64 US 318 dtype: int64 . In the event of overwritting the integer-based indices, it&#39;s still possible to access the elements of a Series sequentially using the iloc method (short for &quot;integer location&quot;). To retrieve the population of Canada, we can do as follows: . g7_pop.iloc[0] . 35 . It&#39;s also possible to use a range when indexing. For instance, suppose we&#39;d like to retrieve the populations of the first three countries from g7_pop. We can simply write: . g7_pop[&#39;Canada&#39;:&#39;Germany&#39;] . Canada 35 France 63 Germany 80 dtype: int64 . Or equivalently: . g7_pop.iloc[0:3] . Canada 35 France 63 Germany 80 dtype: int64 . Since the Series object is based on a Numpy array, it also supports multi-indexing through passing a list of indices or a Boolean mask. . For instance, to print the populations of Canada and Germany at the same time, we can pass in the list [&#39;Canada&#39;,&#39;Germany&#39;] or the Boolean mask [True, False, True, False, False, False, False]. . g7_pop[[&#39;Canada&#39;,&#39;Germany&#39;]] . Canada 35 Germany 80 dtype: int64 . g7_pop[[True, False, True, False, False, False, False]] . Canada 35 Germany 80 dtype: int64 . Broadcasted and Vectorized Operations . Since it&#39;s based on a NumPy array, a Series object also supports vectorization and broadcasted operations. . As a quick reminder, vectorization is the process by which NumPy optimizes looping in Python. It stores the array internally in a contiguous block of memory and restricts its contents to only one data type. Letting Python know this data type in advance, NumPy can then skip the per-iteration type checking that Python normally does in order to speed up our code. In fact, NumPy delegates most of the operations on such optimized arrays to pre-written C code under the hood. . Broadcasting, on the other hand, is the optimized process by which NumPy performs arithmetic and Boolean operations on arrays of unequal dimensions. . For instance, suppose the projected population growth of each G-7 country is 10 mln by the year 2030. Instead of looping through the Series object and adding 10 to each row or using a list comprehension, we can simply use broadcasted addition. . g7_2030_pop = g7_pop + 10 g7_2030_pop . Canada 45 France 73 Germany 90 Italy 70 Japan 137 UK 74 US 328 dtype: int64 . Filtering . Thanks to broadcasted Boolean operations and multi-indexing with a Boolean mask, it&#39;s possible to write concise and readable filtering expressions on Series. . For instance, let&#39;s return the list of countries with a population over 70 mln. . g7_pop[g7_pop &gt;= 70] . Germany 80 Japan 127 US 318 dtype: int64 . The expression g7_pop &gt;= 70 is a broadcasted Boolean operation on the Series object g7_pop which returns a Boolean array [False, False, True, False, True, False, True]. Then g7_pop is multi-indexed using this Boolean mask. . As another example of readable filtering expressions, we can return the list of countries whose populations exceed the mean population. . g7_pop[g7_pop &gt;= g7_pop.mean()] . Japan 127 US 318 dtype: int64 . DataFrame . Each DataFrame is composed of one or more Series. Whereas a Series is analogous to a column of an Excel table, a DataFrame is analogous to the table itself. . The DataFrame constructor accepts a variety of input types, among them an ndarray and a dictionary. . If we&#39;re passing an ndarray, it becomes necessary to specify the column labels separately. Additionally, we may overwrite the integer-based indexing as we did with the Series object. . data = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) df = pd.DataFrame(data, index = [&#39;R1&#39;,&#39;R2&#39;,&#39;R3&#39;], columns = [&#39;C1&#39;, &#39;C2&#39;, &#39;C3&#39;]) df . C1 C2 C3 . R1 1 | 2 | 3 | . R2 4 | 5 | 6 | . R3 7 | 8 | 9 | . We may bypass specifying columns manually by passing in a dictionary instead: . data = { &#39;C1&#39; : [1, 2, 3], &#39;C2&#39; : [4, 5, 6], &#39;C3&#39; : [7, 8, 9] } df = pd.DataFrame(data, index = [&#39;R1&#39;,&#39;R2&#39;,&#39;R3&#39;]) df . C1 C2 C3 . R1 1 | 4 | 7 | . R2 2 | 5 | 8 | . R3 3 | 6 | 9 | . . Note: Whereas in a Series the keys of the input dictionary were the row labels, in a DataFrame they&#8217;re the column labels. . In practice we often create a DataFrame from a CSV file using the pandas.read_csv() method like so: . csv_path = &#39;file.csv&#39; #Stores the path to a CSV df = pd.read_csv(csv_path) . . Tip: We may optionally pass header = None as an argument to read_csv() after csv_path if the first row of the CSV file itself is a data point, and not a header. . Tip: Pandas also supports reading an Excel file into a DataFrame using the pandas.read_excel() method. . Common Methods . Here are the common DataFrame methods that we should keep in our toolbox. These methods give us an overview of our data, and help us clean it up. . df.head() shows, by default, the first 5 rows of the dataset. Accepts an integer argument for the number of rows to display. | df.tail() shows the last 5 rows, and also accepts an integer argument. | df.info() gives a bird&#39;s eye overview of the dataset by showing the total rows/columns, the number of non-null datapoints, and the data types. | df.describe() returns statistically significant values for each column such as, the mean, standard deviation, minimum, and maximum values. | df.shape - returns the dimension of the dataset as an $(m,n)$ tuple. | . Indexing . Let&#39;s add to the dataset of G-7 countries the columns &#39;GDP&#39; and &#39;Surface Area&#39;. . g7_df = pd.DataFrame({ &#39;Population&#39; : g7_pop, &#39;GDP&#39; : [1.7, 2.8, 3.8, 2.1, 4.6, 2.9, 1.7], &#39;Surface Area&#39; : [9.0, 0.6, 0.3, 0.3, 0.3, 0.2, 9.0] }) g7_df . Population GDP Surface Area . Canada 35 | 1.7 | 9.0 | . France 63 | 2.8 | 0.6 | . Germany 80 | 3.8 | 0.3 | . Italy 60 | 2.1 | 0.3 | . Japan 127 | 4.6 | 0.3 | . UK 64 | 2.9 | 0.2 | . US 318 | 1.7 | 9.0 | . Whereas in a Series, the primary axis of indexing are the rows, in a DataFrame the primary axis are the columns. Thus, indexing a column works as expected. . g7_df[&#39;GDP&#39;] . Canada 1.7 France 2.8 Germany 3.8 Italy 2.1 Japan 4.6 UK 2.9 US 1.7 Name: GDP, dtype: float64 . Multi-indexing also works in the familiar way: . g7_df[[&#39;Population&#39;,&#39;GDP&#39;]] . Population GDP . Canada 35 | 1.7 | . France 63 | 2.8 | . Germany 80 | 3.8 | . Italy 60 | 2.1 | . Japan 127 | 4.6 | . UK 64 | 2.9 | . US 318 | 1.7 | . If we want to index rows, however, we must use the loc or iloc methods. . For instance, say we are interested in the population, GDP, and surface area of only the first three countries. We could query the dataset like so: . g7_df.loc[&#39;Canada&#39;:&#39;Germany&#39;] . Population GDP Surface Area . Canada 35 | 1.7 | 9.0 | . France 63 | 2.8 | 0.6 | . Germany 80 | 3.8 | 0.3 | . If we&#39;re only interested in the population and GDP of the first three countries, then we could instead do the following: . g7_df[[&#39;Population&#39;, &#39;GDP&#39;]].loc[&#39;Canada&#39;:&#39;Germany&#39;] . Population GDP . Canada 35 | 1.7 | . France 63 | 2.8 | . Germany 80 | 3.8 | . Noting that loc accepts two inputs, one for the selection of rows and one for columns, we can achieve the above more concisely as follows: . g7_df.loc[&#39;Canada&#39;:&#39;Germany&#39;, [&#39;Population&#39;, &#39;GDP&#39;]] . Population GDP . Canada 35 | 1.7 | . France 63 | 2.8 | . Germany 80 | 3.8 | . Filtering . Since the loc method also accepts a Boolean mask as input, we use it to filter the DataFrame by row. For instance, suppose we want the GDP of countries with a population over 70 mln. We query the dataset as follows: . g7_df.loc[g7_df[&#39;Population&#39;] &gt;= 70, &#39;GDP&#39;] . Germany 3.8 Japan 4.6 US 1.7 Name: GDP, dtype: float64 . As additional exercise, suppose we are only interested in the population and GDP of countries that are smaller than 1.0 mln square kilometers. . g7_df.loc[g7_df[&#39;Surface Area&#39;] &lt;= 1.0, [&#39;Population&#39;,&#39;GDP&#39;]] . Population GDP . France 63 | 2.8 | . Germany 80 | 3.8 | . Italy 60 | 2.1 | . Japan 127 | 4.6 | . UK 64 | 2.9 | . Adding, Dropping, and Renaming Columns and Rows . Let&#39;s add a &#39;Languages&#39; column to g7_df. We simply follow the same syntax as adding a key to a dictionary... . languages = pd.Series({ &#39;Canada&#39; : &#39;French, English&#39;, &#39;France&#39; : &#39;French&#39;, &#39;Germany&#39; : &#39;German&#39;, &#39;Italy&#39; : &#39;Italian&#39;, &#39;Japan&#39; : &#39;Japanese&#39;, &#39;UK&#39; : &#39;English&#39;, &#39;US&#39; : &#39;English&#39; }) # Next, we add the series as a column to the DataFrame g7_df[&#39;Languages&#39;] = languages g7_df . Population GDP Surface Area Languages . Canada 35 | 1.7 | 9.0 | French, English | . France 63 | 2.8 | 0.6 | French | . Germany 80 | 3.8 | 0.3 | German | . Italy 60 | 2.1 | 0.3 | Italian | . Japan 127 | 4.6 | 0.3 | Japanese | . UK 64 | 2.9 | 0.2 | English | . US 318 | 1.7 | 9.0 | English | . We can also drop a column or a row using the drop() method. . Let&#39;s drop the newly created &#39;Languages&#39; column. To drop a coulumn, we specify a columns argument in the drop() method like so: . g7_df.drop(columns = &#39;Languages&#39;) . Population GDP Surface Area . Canada 35 | 1.7 | 9.0 | . France 63 | 2.8 | 0.6 | . Germany 80 | 3.8 | 0.3 | . Italy 60 | 2.1 | 0.3 | . Japan 127 | 4.6 | 0.3 | . UK 64 | 2.9 | 0.2 | . US 318 | 1.7 | 9.0 | . The method drop() returns a new DataFrame which does not contain the unneeded column, but the original g7_df still contains this column. To prove this, let&#39;s print it: . g7_df . Population GDP Surface Area Languages . Canada 35 | 1.7 | 9.0 | French, English | . France 63 | 2.8 | 0.6 | French | . Germany 80 | 3.8 | 0.3 | German | . Italy 60 | 2.1 | 0.3 | Italian | . Japan 127 | 4.6 | 0.3 | Japanese | . UK 64 | 2.9 | 0.2 | English | . US 318 | 1.7 | 9.0 | English | . As we can see the &#39;Languages&#39; column is still there. The solution is to specify an inplace = True argument so that the column is dropped in-place. . g7_df.drop(columns = &#39;Languages&#39;, inplace = True) g7_df . Population GDP Surface Area . Canada 35 | 1.7 | 9.0 | . France 63 | 2.8 | 0.6 | . Germany 80 | 3.8 | 0.3 | . Italy 60 | 2.1 | 0.3 | . Japan 127 | 4.6 | 0.3 | . UK 64 | 2.9 | 0.2 | . US 318 | 1.7 | 9.0 | . In order to drop rows, we specify the index argument instead. Suppose we want to remove Canada and Italy from the dataset. . g7_df.drop(index = [&#39;Canada&#39;, &#39;Italy&#39;]) . Population GDP Surface Area . France 63 | 2.8 | 0.6 | . Germany 80 | 3.8 | 0.3 | . Japan 127 | 4.6 | 0.3 | . UK 64 | 2.9 | 0.2 | . US 318 | 1.7 | 9.0 | . It is also possible to rename a column or a row using the rename() method. . Suppose we&#39;d like to rename the columns to include the units of measurement, and suppose we&#39;d also like to expand the UK and US to their full names. . g7_df.rename( columns = { &#39;Population&#39; : &#39;Population (mln)&#39;, &#39;GDP&#39; : &#39;GDP (USD)&#39;, &#39;Surface Area&#39; : &#39;Surface Area (mln sq. km)&#39; }, index = { &#39;UK&#39; : &#39;United Kingdom&#39;, &#39;US&#39; : &#39;United States&#39; } ) . Population (mln) GDP (USD) Surface Area (mln sq. km) . Canada 35 | 1.7 | 9.0 | . France 63 | 2.8 | 0.6 | . Germany 80 | 3.8 | 0.3 | . Italy 60 | 2.1 | 0.3 | . Japan 127 | 4.6 | 0.3 | . United Kingdom 64 | 2.9 | 0.2 | . United States 318 | 1.7 | 9.0 | . Manipulating Columns . Those of us familiar with Excel know the functions feature which allows users to select specific cells or combinations of cells, perform algebraic or logical operations with their contents, and store the results in new cells. The way to do that in Pandas is, once again, through broadcasted operations. . For instance, suppose we&#39;d like to add a new &#39;GDP Per Capita&#39; column to the g7_df dataset. This is simply a matter of dividing the GDP of each country by its population. . Using broadcasted division, the code is simply: . g7_df[&#39;GDP Per Capita&#39;] = g7_df[&#39;GDP&#39;] / g7_df[&#39;Population&#39;] g7_df . Population GDP Surface Area GDP Per Capita . Canada 35 | 1.7 | 9.0 | 0.048571 | . France 63 | 2.8 | 0.6 | 0.044444 | . Germany 80 | 3.8 | 0.3 | 0.047500 | . Italy 60 | 2.1 | 0.3 | 0.035000 | . Japan 127 | 4.6 | 0.3 | 0.036220 | . UK 64 | 2.9 | 0.2 | 0.045312 | . US 318 | 1.7 | 9.0 | 0.005346 | . Worked Example - Bitcoin Price Timeseries: Cleaning and Reindexing . Sometimes we may wish to use a certain column to index a DataFrame. For instance, if we&#39;re working with a dataset of Bitcoin prices, it would be wise to use the &#39;Time&#39; column as the index so that we can easily access the value of Bitcoin at a given time. . Note: Data that&#8217;s indexed by time is called a timeseries... . For this example, we will retrieve the actual daily Bitcoin price history from CoinCap API 2.0. Feel free to check out the code that fetches the data as JSON and converts it into a Pandas DataFrame in the collapsable code below. . import requests import json # Specifying request URL, payload, and headers url = &#39;https://api.coincap.io/v2/assets/bitcoin/history?interval=d1&#39; payload = {} headers = {} # Making the request and parsing it as JSON response = requests.request(&#39;GET&#39;, url, headers = headers, data = payload) json_data = json.loads(response.text)[&#39;data&#39;] # Converting the result into a DataFrame bitcoin_df = pd.json_normalize(json_data) # Cleanup bitcoin_df.rename( columns = { &#39;priceUsd&#39; : &#39;priceInUSD&#39;, &#39;time&#39; : &#39;Time&#39;, &#39;date&#39; : &#39;Date&#39; }, inplace = True ) . . The result of this is the following DataFrame: . bitcoin_df.head() . priceInUSD Time Date . 0 37480.8939504110899664 | 1610668800000 | 2021-01-15T00:00:00.000Z | . 1 36853.8623471143090244 | 1610755200000 | 2021-01-16T00:00:00.000Z | . 2 35670.6623897365179078 | 1610841600000 | 2021-01-17T00:00:00.000Z | . 3 36061.4760792230247237 | 1610928000000 | 2021-01-18T00:00:00.000Z | . 4 36868.3293610208260669 | 1611014400000 | 2021-01-19T00:00:00.000Z | . Now that we have the dataset as a cleaned-up Pandas DataFrame called bitcoin_df, we can get to work. . First order of business is to reindex the dataset based on the &#39;Time&#39; column. We can set a column as index using the set_index() method in the following way: . bitcoin_df.set_index(&#39;Time&#39;, inplace=True) bitcoin_df.index.name = None # The index column shouldn&#39;t have a name — this removes the name &#39;Time&#39; bitcoin_df.head() . priceInUSD Date . 1610668800000 37480.8939504110899664 | 2021-01-15T00:00:00.000Z | . 1610755200000 36853.8623471143090244 | 2021-01-16T00:00:00.000Z | . 1610841600000 35670.6623897365179078 | 2021-01-17T00:00:00.000Z | . 1610928000000 36061.4760792230247237 | 2021-01-18T00:00:00.000Z | . 1611014400000 36868.3293610208260669 | 2021-01-19T00:00:00.000Z | . As we can see the column that was previously named &#39;Time&#39; now acts as index. . Next, we should convert the entries of the index from a Unix timestamp into a Python datetime for more clarity. While doing this, let&#39;s also convert the entries of the &#39;Date&#39; column which are currently in string format. . bitcoin_df.index = pd.to_datetime(bitcoin_df.index, unit=&#39;us&#39;) # Converting index to datetime from Unix seconds bitcoin_df[&#39;Date&#39;] = pd.to_datetime(bitcoin_df[&#39;Date&#39;]) # Converting &#39;Date&#39; to datetime from string bitcoin_df.head() . priceInUSD Date . 2021-01-15 37480.8939504110899664 | 2021-01-15 00:00:00+00:00 | . 2021-01-16 36853.8623471143090244 | 2021-01-16 00:00:00+00:00 | . 2021-01-17 35670.6623897365179078 | 2021-01-17 00:00:00+00:00 | . 2021-01-18 36061.4760792230247237 | 2021-01-18 00:00:00+00:00 | . 2021-01-19 36868.3293610208260669 | 2021-01-19 00:00:00+00:00 | . Now we can comfortably access the price of Bitcoin on any given day. Suppose we&#39;d like to know its price on 2021-12-28, the day of writing this post... We can simply do: . bitcoin_df.loc[&#39;2021-12-28&#39;, &#39;priceInUSD&#39;] . &#39;48995.0145281203441155&#39; . Common Workflows . Finding the Unique Elements in a Column . Pandas comes with the method unique() which can be applied to a Series object. . Let&#39;s fetch some data about the planets in our solar system from the devstronomy repository. . planets_df = pd.read_csv(&#39;https://raw.githubusercontent.com/devstronomy/nasa-data-scraper/master/data/csv/planets.csv&#39;) planets_df . planet mass diameter density gravity escape_velocity rotation_period length_of_day distance_from_sun perihelion ... orbital_period orbital_velocity orbital_inclination orbital_eccentricity obliquity_to_orbit mean_temperature surface_pressure number_of_moons has_ring_system has_global_magnetic_field . 0 Mercury | 0.3300 | 4879 | 5427 | 3.7 | 4.3 | 1407.6 | 4222.6 | 57.9 | 46.0 | ... | 88.0 | 47.4 | 7.0 | 0.205 | 0.034 | 167 | 0 | 0 | No | Yes | . 1 Venus | 4.8700 | 12104 | 5243 | 8.9 | 10.4 | -5832.5 | 2802.0 | 108.2 | 107.5 | ... | 224.7 | 35.0 | 3.4 | 0.007 | 177.400 | 464 | 92 | 0 | No | No | . 2 Earth | 5.9700 | 12756 | 5514 | 9.8 | 11.2 | 23.9 | 24.0 | 149.6 | 147.1 | ... | 365.2 | 29.8 | 0.0 | 0.017 | 23.400 | 15 | 1 | 1 | No | Yes | . 3 Mars | 0.6420 | 6792 | 3933 | 3.7 | 5.0 | 24.6 | 24.7 | 227.9 | 206.6 | ... | 687.0 | 24.1 | 1.9 | 0.094 | 25.200 | -65 | 0.01 | 2 | No | No | . 4 Jupiter | 1898.0000 | 142984 | 1326 | 23.1 | 59.5 | 9.9 | 9.9 | 778.6 | 740.5 | ... | 4331.0 | 13.1 | 1.3 | 0.049 | 3.100 | -110 | Unknown* | 79 | Yes | Yes | . 5 Saturn | 568.0000 | 120536 | 687 | 9.0 | 35.5 | 10.7 | 10.7 | 1433.5 | 1352.6 | ... | 10747.0 | 9.7 | 2.5 | 0.057 | 26.700 | -140 | Unknown* | 62 | Yes | Yes | . 6 Uranus | 86.8000 | 51118 | 1271 | 8.7 | 21.3 | -17.2 | 17.2 | 2872.5 | 2741.3 | ... | 30589.0 | 6.8 | 0.8 | 0.046 | 97.800 | -195 | Unknown* | 27 | Yes | Yes | . 7 Neptune | 102.0000 | 49528 | 1638 | 11.0 | 23.5 | 16.1 | 16.1 | 4495.1 | 4444.5 | ... | 59800.0 | 5.4 | 1.8 | 0.011 | 28.300 | -200 | Unknown* | 14 | Yes | Yes | . 8 Pluto | 0.0146 | 2370 | 2095 | 0.7 | 1.3 | -153.3 | 153.3 | 5906.4 | 4436.8 | ... | 90560.0 | 4.7 | 17.2 | 0.244 | 122.500 | -225 | 0.00001 | 5 | No | Unknown | . 9 rows × 21 columns . If we want to find out the unique number of moons each planet has, we can simply do: . planets_df[&#39;number_of_moons&#39;].unique() . array([ 0, 1, 2, 79, 62, 27, 14, 5], dtype=int64) . As we can see, the 9 planets in our solar system (counting Pluto) have 8 unique number of moons. This is because, as we can see from the dataset, Mercury and Venus both have 0 moons. . Saving Data . After all the data manipulation, it would be useful to save the resulting dataset locally on our machine. Pandas offers us a way to do that using the DataFrame.to_csv() method. . Working with the planets_df defined above, we can narrow the dataset down to the planets which have a gravitational force that&#39;s close to that of the Earth ($9.8 m/s^2$). . earthlike_planets_df = planets_df[(planets_df[&#39;gravity&#39;] &gt;= 9.8 - 1) &amp; (planets_df[&#39;gravity&#39;] &lt;= 9.8 + 1)] earthlike_planets_df . planet mass diameter density gravity escape_velocity rotation_period length_of_day distance_from_sun perihelion ... orbital_period orbital_velocity orbital_inclination orbital_eccentricity obliquity_to_orbit mean_temperature surface_pressure number_of_moons has_ring_system has_global_magnetic_field . 1 Venus | 4.87 | 12104 | 5243 | 8.9 | 10.4 | -5832.5 | 2802.0 | 108.2 | 107.5 | ... | 224.7 | 35.0 | 3.4 | 0.007 | 177.4 | 464 | 92 | 0 | No | No | . 2 Earth | 5.97 | 12756 | 5514 | 9.8 | 11.2 | 23.9 | 24.0 | 149.6 | 147.1 | ... | 365.2 | 29.8 | 0.0 | 0.017 | 23.4 | 15 | 1 | 1 | No | Yes | . 5 Saturn | 568.00 | 120536 | 687 | 9.0 | 35.5 | 10.7 | 10.7 | 1433.5 | 1352.6 | ... | 10747.0 | 9.7 | 2.5 | 0.057 | 26.7 | -140 | Unknown* | 62 | Yes | Yes | . 3 rows × 21 columns . . Tip: Pandas prefers the use of bitwise Boolean operators &amp; and |, instead of the Python&#8217;s default and and or. This is because Pandas relies on NumPy, which in turn relies on the capacity of the bitwise operators to be overloaded. . We can now save this new dataset to our desktop as follows: . earthlike_planets_df.to_csv(&#39;C:/Users/Vahram/Desktop/earthlike_planets.csv&#39;) .",
            "url": "https://v-poghosyan.github.io/blog/python%20for%20ml/pandas/machine%20learning/2021/12/28/Python-for-ML-Pandas.html",
            "relUrl": "/python%20for%20ml/pandas/machine%20learning/2021/12/28/Python-for-ML-Pandas.html",
            "date": " • Dec 28, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "LeetCode 11 - Container with Most Water",
            "content": "Introduction . Problem Statement . You are given an integer array height of length n. There are n vertical lines drawn such that the two endpoints of the ith line are (i, 0) and (i, height[i]). . Find two lines that together with the x-axis form a container, such that the container contains the most water. . Return the maximum amount of water a container can store. . Example: . . Input: height = [1,8,6,2,5,4,8,3,7] Output: 49 Explanation: The above vertical lines are represented by array [1,8,6,2,5,4,8,3,7]. In this case, the max area of water (blue section) the container can contain is 49. . Foreword . The brute force solution to this problem consists of checking each pair of vertical lines. Since order of any given pair does not matter, this solution has time complexity ${O({n choose 2}) = O(n^2)}$ where $n$ is the length of the height array. . The non-brute-force solution to this problem (i.e. the &#39;two pointer solution&#39;) is pretty intuitive - the difficulty lies in its proof of correctness. Therefore, I will give away the procedure in this foreword and then proceed to prove its correctness. . The Procedure . The following is the overall procedure in words: . Initialize left and right pointers at 1 and n respectively (assuming indices start at 1). | While the pointers do not intersect: . Fix the pointer whose corresponding vertical line is longer. | Advance the pointer whose corresponding vertical line is shorter towards the fixed one. | . | . The Code . #hide-output class Solution: def maxArea(self, height: List[int]) -&gt; int: i, j = 0, len(height) - 1 water = 0 while i &lt; j: water = max(water, (j - i) * min(height[i], height[j])) if height[i] &lt; height[j]: i += 1 else: j -= 1 return water . . Proof of Correctness . Optimal Substructure . The procedure is inspired by the following recurisve optimal substructure of the problem: . Let $h(i)$ denote the height of the $i$-th vertical line. . | Let $a(i,j)$ denote the area of the container formed by the pair of vertical lines $(i,j)$. . | Let $maxArea([i:j])$ denote the maximum area formed by the lines ${i,...,j}$ – that is the output of the procedure on the subarray ${[i:j]}$. . | . Claim . The problem has optimal substructure: $$maxArea([1:n]) = max {a(1,n), maxArea([2:n]) }$$ . Proof of Claim . For the initial pair $(1,n)$ where, WLOG, ${h(1) &lt; h(n)}$ we have ${a(1,n) &gt; a(1,k) forall k}$. This is because we&#39;re starting out from the widest container formed by ${(1,n)}$ and considering containers of decreasing width formed by the pairs ${(1, n-1), (1, n-2), ..., (1,2)}$. . There are two cases: . In case ${h(k) &gt; h(1)}$ for some ${1 &lt; k leq n}$ the area of the container formed by ${(1,k)}$ is still determined by ${h(1)}$, except now it&#39;s less wide. Whereas if ${h(k) &lt; h(1)}$ the area of the container decreases not only in width but also in height. . In both cases we have ${a(1,n) &gt; a(1,k)}$ which means in general ${a(1,n) &gt; a(1, k) forall k}$. . Therefore, we may omit the first vertical line from consideration and consider the subproblem on the indices ${2,...,n}$. The overall optimal solution will then be $$maxArea([1:n]) = max {a(1,n), maxArea([2:n]) }$$ as was the claim. . Inductive Proof . As with all problems that have a recursive optimal substructure, an inductive proof of correctness is immediately what springs to mind. . Base Case . For the case when $n = 2$, ${maxArea([1:2]) = max {a(1,2), maxArea([2:2]) } = a(1,2)}$ since ${maxArea[2:2] = 0}$. This is obviously correct. . Inductive Step . Suppose for an array of length $m$, the procedure $maxArea$ is correct. We would like to show that for an array of length $m+1$ it is still correct. . Assume, WLOG, ${h(1) &lt; h(m+1)}$. Note that by the optimal substructure shown above, ${maxArea([1:m+1]) = max {a(1,m+1), maxArea([2:m+1]) }}$. In omiting the first element of the input array, the only pairs we remove from consideration are ${(1,m), (1,m-1),..., (1,2)}$ which we have already shown to be suboptimal to ${(1,m+1)}$ in the proof of the optimal substructure. And since by assumption the procedure $maxArea$ on the $m$-element subarray ${[2:m+1]}$ is correct, we are done! . Conclusion . With some problems, it is the case that figuring out why the solution works gives more insight into the problem than simply solving it based on raw intuition... .",
            "url": "https://v-poghosyan.github.io/blog/leetcode/toy%20problems/proofs/2021/12/23/LeetCode-11.html",
            "relUrl": "/leetcode/toy%20problems/proofs/2021/12/23/LeetCode-11.html",
            "date": " • Dec 23, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Thanks for dropping by! I’m Vahram, a software developer and graduate student of Computer Science @ UT Austin. . 🎓 Education . After graduating from community college in 2018, I transfered to UCLA where I pursued my undergraduate studies in Applied Mathematics. In 2020, having graduated from UCLA with honors, I enrolled in the M.S. in Computer Science program @ UT Austin. . 💻 Work . While still at community college I got my first job as a Software Engineer Intern @ Omron Automation. There, I was part of the team that developed embedded systems for the motion controllers and control panels, as well as the team that built and maintained the company’s proprietary Visual Studio based IDE. . In the Summer of 2022 I will be joining the team over @ Capital One as a Software Engineer Intern. . 🙋‍♂️ Social Media . For a detailed rundown of my work history, check out my LinkedIn. . Feel free to also email me at: vahram@utexas.edu .",
          "url": "https://v-poghosyan.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://v-poghosyan.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}