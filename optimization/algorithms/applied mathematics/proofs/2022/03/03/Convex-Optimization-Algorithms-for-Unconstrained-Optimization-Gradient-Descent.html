<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Optimization - Algorithms for Unconstrained Optimization, Gradient-Descent | Vahram Poghosyan</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Optimization - Algorithms for Unconstrained Optimization, Gradient-Descent" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="GD, Smoothness, Strict Convexity, Line Search" />
<meta property="og:description" content="GD, Smoothness, Strict Convexity, Line Search" />
<link rel="canonical" href="https://v-poghosyan.github.io/blog/optimization/algorithms/applied%20mathematics/proofs/2022/03/03/Convex-Optimization-Algorithms-for-Unconstrained-Optimization-Gradient-Descent.html" />
<meta property="og:url" content="https://v-poghosyan.github.io/blog/optimization/algorithms/applied%20mathematics/proofs/2022/03/03/Convex-Optimization-Algorithms-for-Unconstrained-Optimization-Gradient-Descent.html" />
<meta property="og:site_name" content="Vahram Poghosyan" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-03-03T00:00:00-06:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Optimization - Algorithms for Unconstrained Optimization, Gradient-Descent" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-03-03T00:00:00-06:00","datePublished":"2022-03-03T00:00:00-06:00","description":"GD, Smoothness, Strict Convexity, Line Search","headline":"Optimization - Algorithms for Unconstrained Optimization, Gradient-Descent","mainEntityOfPage":{"@type":"WebPage","@id":"https://v-poghosyan.github.io/blog/optimization/algorithms/applied%20mathematics/proofs/2022/03/03/Convex-Optimization-Algorithms-for-Unconstrained-Optimization-Gradient-Descent.html"},"url":"https://v-poghosyan.github.io/blog/optimization/algorithms/applied%20mathematics/proofs/2022/03/03/Convex-Optimization-Algorithms-for-Unconstrained-Optimization-Gradient-Descent.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://v-poghosyan.github.io/blog/feed.xml" title="Vahram Poghosyan" /><!-- the google_analytics_id gets auto inserted from the config file -->



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-QB9Q6T3YNL"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-QB9Q6T3YNL');
</script>


<link rel="shortcut icon" type="image/x-icon" href="/blog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">Vahram Poghosyan</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">About Me</a><a class="page-link" href="/blog/search/">Search</a><a class="page-link" href="/blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Optimization - Algorithms for Unconstrained Optimization, Gradient-Descent</h1><p class="page-description">GD, Smoothness, Strict Convexity, Line Search</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2022-03-03T00:00:00-06:00" itemprop="datePublished">
        Mar 3, 2022
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      25 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/blog/categories/#Optimization">Optimization</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#Algorithms">Algorithms</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#Applied Mathematics">Applied Mathematics</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#Proofs">Proofs</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-justify-center">
          <div class="px-2">

    <a href="https://github.com/v-poghosyan/blog/tree/master/_notebooks/2022-03-03-Convex Optimization-Algorithms-for-Unconstrained-Optimization-Gradient-Descent.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/blog/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/v-poghosyan/blog/master?filepath=_notebooks%2F2022-03-03-Convex+Optimization-Algorithms-for-Unconstrained-Optimization-Gradient-Descent.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/blog/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/v-poghosyan/blog/blob/master/_notebooks/2022-03-03-Convex Optimization-Algorithms-for-Unconstrained-Optimization-Gradient-Descent.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/blog/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul id="toc" class="section-nav">
<li class="toc-entry toc-h1"><a href="#PLAN">PLAN </a></li>
<li class="toc-entry toc-h1"><a href="#Introduction">Introduction </a></li>
<li class="toc-entry toc-h1"><a href="#The-Gradient-Descent-Algorithm">The Gradient Descent Algorithm </a>
<ul>
<li class="toc-entry toc-h2"><a href="#Idea-1---Greedy-Choice-of-Direction">Idea 1 - Greedy Choice of Direction </a></li>
<li class="toc-entry toc-h2"><a href="#Idea-2---Greedy-Choice-of-Next-Iterate">Idea 2 - Greedy Choice of Next Iterate </a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#Important-Questions-in-Analysis">Important Questions in Analysis </a>
<ul>
<li class="toc-entry toc-h2"><a href="#Initialization">Initialization </a></li>
<li class="toc-entry toc-h2"><a href="#Fixed-Step-Size-GD">Fixed Step-Size GD </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Simple-Analysis-of-Fixed-Step-Size-GD">Simple Analysis of Fixed Step-Size GD </a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#Smoothness-and-Strong-Convexity">Smoothness and Strong Convexity </a>
<ul>
<li class="toc-entry toc-h2"><a href="#Self-Tuning-Property">Self-Tuning Property </a></li>
<li class="toc-entry toc-h2"><a href="#Quadratic-Bounds">Quadratic Bounds </a></li>
<li class="toc-entry toc-h2"><a href="#The-Optimal-Fixed-Step-Size">The Optimal Fixed Step-Size </a></li>
<li class="toc-entry toc-h2"><a href="#Convergence-Rate">Convergence Rate </a>
<ul>
<li class="toc-entry toc-h3"><a href="#M-Smooth-Objectives">M-Smooth Objectives </a></li>
<li class="toc-entry toc-h3"><a href="#M-Smooth-and-m-Strongly-Convex-Objective">M-Smooth and m-Strongly-Convex Objective </a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#Variable-Step-Size-GD">Variable Step-Size GD </a></li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2022-03-03-Convex Optimization-Algorithms-for-Unconstrained-Optimization-Gradient-Descent.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="PLAN">
<a class="anchor" href="#PLAN" aria-hidden="true"><span class="octicon octicon-link"></span></a>PLAN<a class="anchor-link" href="#PLAN"> </a>
</h1>
<ol>
<li>Unconstrained algorithms 
<del>2. Oracle Access Model of order 1</del>
<del>3. Develop GD from two perspectives - linear and quadratic</del>
</li>
<li>Run GD on model problems x^2 and |x|</li>
<li>Develop the notion of M-smooth and m-strongly convex based on step 4</li>
<li>
<p>Analyze the performance of GD</p>
</li>
<li>
<p>Develop Accelerated GD</p>
</li>
<li>Develop Subgradient Descent</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Introduction">
<a class="anchor" href="#Introduction" aria-hidden="true"><span class="octicon octicon-link"></span></a>Introduction<a class="anchor-link" href="#Introduction"> </a>
</h1>
<p><strong><em>Gradient descent (GD)</em></strong> is a powerful, yet incredibly simple, iterative optimization algorithm. We can think of it as a <strong><em>greedy algorithm</em></strong> in the setting of continuous optimization. That is, one step of GD is our best, local attempt at optimization given only limited information about the objective $f(x)$, and having limited computational power at our disposal.</p>
<p>We can further qualify what we mean by <em>'limited information'</em> about the objective by introducing a categorization on iterative optimization algorithms – the <strong><em>Oracle Access Model (OAM)</em></strong>. In this model, the objective is abstracted into a black box. For each input $x$, the black box gives access to the objective value $f(x)$ and, optionally, more global information in the form of higher order behavior such as $\nabla f(x)$, $\nabla^2 f(x)$, etc. GD is what's known as a <strong><em>first-order oracle</em></strong> because it's only allowed access to the objective $f(x)$ and its first-order information $\nabla f(x)$. It's important to note that the OAM is not all-inclusive, there are a number of optimization algorithms, falling under the blanket of <strong><em>composite optimization</em></strong>, that utilize structural information about the objective that goes beyond $n$-th order information. An example of such an algorithm is <strong><em>proximal gradient descent (PGD)</em></strong>.</p>
<p>For now, we focus on the simpler case of unconstrained optimization with OAM algorithms in order to develop the key algorithmic ideas. Later on, perhaps in a different post, we will explore modifications to gradient descent, such as PGD, that make it suitable for constrained optimization.</p>
<h1 id="The-Gradient-Descent-Algorithm">
<a class="anchor" href="#The-Gradient-Descent-Algorithm" aria-hidden="true"><span class="octicon octicon-link"></span></a>The Gradient Descent Algorithm<a class="anchor-link" href="#The-Gradient-Descent-Algorithm"> </a>
</h1>
<p>As all iterative algorithms, gradient descent relies on <strong><em>initialization</em></strong> and an <strong><em>update step</em></strong>. In this section, we explore two ideas that play a key role in developing the GD algorithm.</p>
<h2 id="Idea-1---Greedy-Choice-of-Direction">
<a class="anchor" href="#Idea-1---Greedy-Choice-of-Direction" aria-hidden="true"><span class="octicon octicon-link"></span></a>Idea 1 - Greedy Choice of Direction<a class="anchor-link" href="#Idea-1---Greedy-Choice-of-Direction"> </a>
</h2>
<p>Let $x$ be the initial iterate, and let the update be given by:
<br>
$$x^+ = x + \eta d$$ 
<br></p>
<p>for some directional unit-vector $d$ and <strong><em>step-size</em></strong> parameter $\eta &gt; 0$.</p>
<p>We base the algorithm on the assumption that the linear approximation of the objective at a the next iterate $x^+$ is a good-enough estimate of its true value at $x^+$.</p>
<p>That is:</p>
<p><br>

$$f(x^+) = f(x + \eta d) \approx f(x) + \eta \nabla f(x)^T d \ \ \forall d \tag{1.1}$$

<br></p>
<p>Immediately, a locally optimal choice presents itself to us. Since we wish to minimize $f(x)$, it would be wise to insist that the objective at $x^+$ improves or, at least, does not worsen.</p>
<p>That is, we insist:</p>
<p><br>

$$f(x^+) \approx f(x) + \eta \nabla f(x)^T d \leq f(x) \tag{1.3}$$

<br></p>
<p>And, since we are greedy in our approach, we wish to make $f(x^+)$ as small as possible. Since, on the RHS, $f(x)$ is fixed and $\eta &gt; 0$, this amounts to minimizing the scaled inner-product $\nabla f(x)^Td$. To that end, we choose $d$ opposite and parallel to the gradient, i.e. $d = - \frac{\nabla f(x)}{||\nabla f(x)||_2}$.</p>
<p>The update step becomes:</p>
<p><br>

$$x^+ = x - \eta \frac{\nabla f(x)}{||\nabla f(x)||_2}$$

<br></p>
<p>By re-labeling, $\eta$ can absorb the normalization constant. This obtains the gradient descent update step as it's often introduced in the textbooks – a step in the negative gradient direction:</p>
<p><br>

$$x^+ = x - \eta \nabla f(x) \tag{1.4}$$

<br></p>
<p>This makes intuitive sense because the negative gradient direction is the direction in which the objective decreases most. So, it's only natural that the update should take us in this most enticing direction.</p>
<h2 id="Idea-2---Greedy-Choice-of-Next-Iterate">
<a class="anchor" href="#Idea-2---Greedy-Choice-of-Next-Iterate" aria-hidden="true"><span class="octicon octicon-link"></span></a>Idea 2 - Greedy Choice of Next Iterate<a class="anchor-link" href="#Idea-2---Greedy-Choice-of-Next-Iterate"> </a>
</h2>
<p>Instead of defining the update step $x^+ = x + \eta d$ and then choosing the locally optimal direction $d$ greedily, we can choose the update step and the direction, both, in one fell swoop.</p>
<p>Starting from the linear approximation:</p>
<p><br>
$$
f(y) \approx f(x) + \nabla f(x)^T(y - x) \ \ \forall y \tag{2.1}
$$
<br></p>
<p>We can now insist, in a greedy fashion, that the next iterate $x^+$ be the minimizer of the linear approximation. That is, we insist:</p>
<p><br>
$$
x^+ = \arg \min_y f(x) + \nabla f(x)^T(y - x) \tag{2.2}
$$
<br></p>
<p>But the linear approximation is only local, so it would be wise to distrust it for points far away from the current iterate. In this case, since the linear approximation is, in fact, unbounded below, $(2.2)$ would obtain $x^+ = \pm \infty$. To avoid this problem, we introduce a parametrized penalty term that prevents $x^+$ from venturing too far from the current iterate $x$. That is:</p>
<p><br>
$$
x^+ = \arg \min_y f(x) + \nabla f(x)^T(y - x) + \eta ||y - x||_2^2 \tag{2.3}
$$
<br></p>
<p>Now, since the RHS is a a simple quadratic in $y$, it has a unique minimizer which can be found by using the <strong><em>unconstrained optimality condition</em></strong>. This just means taking the gradient of the RHS w.r.t. the optimization variable $y$, setting it to zero, and then solving for the unique root. This obtains:</p>
<p><br>

$$x^+ = x - \frac{1}{2 \eta} \nabla f(x)$$

<br></p>
<p>By re-labeling, we, once again, get the canonical form of the GD update step as in $(1.3)$ – a step in the negative gradient direction.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Important-Questions-in-Analysis">
<a class="anchor" href="#Important-Questions-in-Analysis" aria-hidden="true"><span class="octicon octicon-link"></span></a>Important Questions in Analysis<a class="anchor-link" href="#Important-Questions-in-Analysis"> </a>
</h1>
<p>Given the ease with which we came up with the algorithm, we should ask ourselves the following questions:</p>
<ol>
<li>Is GD sensitive to initialization?</li>
<li>Is GD guaranteed to converge for all step-sizes?</li>
<li>How should we choose a step-size that guarantees convergence? </li>
<li>What's the rate of convergence of GD? Does the rate depend on step-size? Does it depend on properties of the objective function?</li>
<li>How should we choose a step-size that maximizes convergence rate?</li>
</ol>
<p>We will shortly explore each of these questions and more. However, before doing so, it's worth taking a bird's eye look at the problem of convex optimization itself.</p>
<p>Perhaps the most important question to ask ourselves is this: does gradient descent's convergence rate, for an optimally chosen step-size, give a taxonomy of easier-to-harder problems within the scope of convex optimization? The answer, as it turns out, is <em>yes</em>.</p>
<h2 id="Initialization">
<a class="anchor" href="#Initialization" aria-hidden="true"><span class="octicon octicon-link"></span></a>Initialization<a class="anchor-link" href="#Initialization"> </a>
</h2>
<p>From this point on, we will limit our discussion to convex objectives in order to eliminate the possibility of strictly <strong><em>local optimizers</em></strong> and <strong><em>inflection points</em></strong>, both of which GD, by construction, can get stuck at given a badly chosen initial point. This ensures the only <strong><em>stationary points</em></strong>, points at which $\nabla f(x) = 0$ and the GD update makes no further progress, are global minimizers. On convex functions GD, as we will soon discover, has a convergence guarantee for all step-sizes independently of initialization.</p>
<h2 id="Fixed-Step-Size-GD">
<a class="anchor" href="#Fixed-Step-Size-GD" aria-hidden="true"><span class="octicon octicon-link"></span></a>Fixed Step-Size GD<a class="anchor-link" href="#Fixed-Step-Size-GD"> </a>
</h2>
<p>To kickstart our analysis of GD, we consider the fixed step-size algorithm first. Let's take two quintessential convex problems in $\mathbb{R}$, $f(x) = x^2$ and $h(x) = |x|$, and analyze GD's performance on these objectives.</p>
<h3 id="Simple-Analysis-of-Fixed-Step-Size-GD">
<a class="anchor" href="#Simple-Analysis-of-Fixed-Step-Size-GD" aria-hidden="true"><span class="octicon octicon-link"></span></a>Simple Analysis of Fixed Step-Size GD<a class="anchor-link" href="#Simple-Analysis-of-Fixed-Step-Size-GD"> </a>
</h3>
<p>First, let's run the algorithm on $h(x) = |x|$ for $x \in \mathbb{R}$.</p>
<p>Since $|x|$ is non-differentiable at $x = 0$, the gradient has a discontinuity at $x = 0$. Non-differentiability, such as this, will eventually lead us to introduce the notion of <strong><em>sub-gradients</em></strong>, but for now we can get away with using the discontinuous gradient:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
$$
h'(x) = 
\begin{cases} 
\begin{aligned} 
-1 \ &amp;\textrm{if $x &lt; 0$} \\ 
1 \ &amp;\textrm{if $x &gt; 0$} 
\end{aligned}
\end{cases}
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Then, for a fixed $\eta &gt; 0$, the update step is:</p>
<p><br>

$$x^+ = x \pm \eta$$

<br></p>
<p>where the sign of $\eta$ depends on where the previous iterate, $x$, falls inside the domain $(-\infty, 0) \cup (0, \infty)$.</p>
<p>Now, switching our attention to $f(x) = x^2$, we compute its GD update as follows. We compute the gradient as $f'(x) = 2x$ which leads to the fixed step-size update:</p>
<p><br>

$$x^+ = x - 2\eta x$$

<br></p>
<p>Note that $x^* = 0$ is the unique optimizer of both $f(x)$ and $h(x)$. With this in mind, there are two key observations to make.</p>
<p>The first is that, for $x$ far away from $x^* = 0$, the update, $2\eta x$, is large in magnitude. So, if the iterate is far from the optimizer, GD makes fast progress towards it.</p>
<p>The second observation is that, as $x \rightarrow x^*$, the update becomes small in magnitude. So, as the iterate comes close to the optimizer, GD takes smaller and smaller steps which converge to $0$ in a summable way. This means, we can get the sub-optimality $|f(x) - f(x^*)|$ to be $\epsilon$-arbitrarily small for any fixed step-size $\eta$.</p>
<p>Neither of these observations hold for GD on $h(x) = |x|$ since the update $\eta$ is fixed regardless of the Euclidean distance between $x$ and $x^* = 0$. In particular, this means GD is not fast for $x$ far away from $x^*$ and does not slow down as $x$ nears $x^*$. Arbitrary accuracy is, also, impossible in the setting of a fixed step-size $\eta$. The iterates eventually cycle between $x^T - \eta$ and $x^T + \eta$ where $x^T$ is the last iterate before entering the cycle, that is $x^T \in (-\eta, \eta)$. The sub-optimality, consequently, also cycles between two values which depend on the choice of $\eta$. This is to say that the sub-optimality cannot be $\epsilon$-arbitrary small for a fixed choice of $\eta$. To be clear, there is still convergence but it's slow and not arbitrarily accurate. Arbitrary accuracy for such problems as this can only be achieved by choosing a sequence of diminishing step-sizes $\{ \eta_t \}_{t=1}^T$ which help diminish the update since the gradient itself is non-diminishing. Of course, the sequence must be chosen with care since it's possible to <em>'run out of steam'</em> before reaching the optimizer.</p>
<p>We say GD on $f(x) = x^2$ enjoys the <strong><em>self-tuning property</em></strong>, whereas GD on $h(x) = |x|$ does not. This speaks to the fact that the self-tuning is a property of the objective functions, rather than GD itself.</p>
<p>As an overview of the theory we will soon develop, functions <em>like</em> $x^2$ (or, more generally, any quadratic) will all have the self-tuning property while functions <em>like</em> $|x|$ will not. This is what ends up introducing a taxonomy of easier-to-harder convex optimization problems. What it means, precisely, to be <em>like</em> $x^2$ or $|x|$ will be made rigorous in the next few sections.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Smoothness-and-Strong-Convexity">
<a class="anchor" href="#Smoothness-and-Strong-Convexity" aria-hidden="true"><span class="octicon octicon-link"></span></a>Smoothness and Strong Convexity<a class="anchor-link" href="#Smoothness-and-Strong-Convexity"> </a>
</h1>
<p>As we saw above, gradient descent with a fixed step-size behaved much better on $f(x) = x^2$ than on $h(x) = |x|$. Since both of these problems are convex, $x^2$ must have additional properties not shared by $|x|$ that make it more amenable to optimization by GD. These properties turn out to be <strong><em>smoothness</em></strong> and <strong><em>strong convexity</em></strong>. We will see that these properties provide insight into choosing the best fixed-step size which guarantees faster convergence of GD.</p>
<p>We can start by asking ourselves what makes the two quintessential functions $f(x) = x^2$ and $h(x) = |x|$ different from one another. Since the GD update step relies on the gradient, it helps thinking in terms of the differences of the gradients instead of the objective functions themselves.</p>
<p>The first difference of note is that $|x|$ has a discontinuity at $x = 0$ that's not present in $x^2$. At a point of discontinuity the gradient experiences an abrupt jump. So, in general, jumps in the gradient must pose a problem for GD.</p>
<p>The second thing to note is that $|x|$ is flat compared to $x^2$. In flat regions, the gradient is constant. So, in general, constant regions in the gradient must pose a problem for GD.</p>
<p>Both of these scenarios can be ruled out with a <a href="https://en.wikipedia.org/wiki/Lipschitz_continuity"><strong><em>Lipschitz condition</em></strong></a> on the gradient. Lipschitz conditions are both regularity conditions, as well as, growth conditions. So, they can rule out abrupt jumps and ensure there's some change in the gradient.</p>
<p>We are ready to define the two properties mentioned in the beginning of this section.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote>
<p><strong>Smoothness:</strong>   We say a function $f(x)$ is <strong><em>M-smooth</em></strong> if its gradient is <strong><em>M-Lipschitz</em></strong>. That is, if:<br>

$$\exists M &gt; 0 \ \ s.t. \ \ ||\nabla f(x) - \nabla f(y)||_2 \leq M||x-y||_2 \ \ \forall x,y$$

<br></p>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This is a universal upper-bound on the change in gradient which rules out jumps.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote>
<p><strong>Strong Convexity:</strong>   We say a function $f(x)$ is <strong><em>m-strongly-convex</em></strong> if:<br>

$$\exists m &gt; 0 \ \ s.t. \ \ ||\nabla f(x) - \nabla f(y)||_2 \geq m||x-y||_2 \ \ \forall x,y$$

<br></p>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This is a universal lower-bound on the change in gradient which rules out the possibility of a constant gradient.</p>
<p>In particular, an $M$-smooth, and $m$-strongly-convex function $f(x)$ has the property that:</p>
<p><br>

$$m||x-y||_2 \leq ||\nabla f(x) - \nabla f(y)||_2 \leq M||x-y||_2 \ \ \forall x,y \tag{3.1}$$

<br></p>
<p>For a twice-differentiable function, there's a more compact way to express these properties using the hessian. It makes use of an ordering on matrices introduced by matrix <a href="https://en.wikipedia.org/wiki/Definite_matrix"><strong><em>definiteness</em></strong></a>.</p>
<p><br>

$$||\nabla f(x) - \nabla f(y)||_2 \leq M||x-y||_2 \ \ \forall x,y$$


$$\iff$$


$$||\nabla^2 f(x)||_2 \leq M \ \ \forall x$$


$$\iff$$


$$\nabla^2 f(x) \preceq  MI \ \ \forall x \tag{4.1}$$

<br></p>
<p>The first equivalence is by the <a href="https://en.wikipedia.org/wiki/Mean_value_theorem"><strong><em>Mean Value Theorem</em></strong></a> and the second follows from the definition of <strong><em>matrix norm</em></strong>.</p>
<p>Line $(3.1)$ should be read as <em>'the maximum eigenvalue of the hessian $\nabla^2 f(x)$ is $M$'</em>.</p>
<p>By a symmetric argument, we also have:</p>
<p><br>

$$\nabla^2 f(x) \succeq mI \ \ \forall x \tag{4.2}$$

<br></p>
<p>Which should be read as <em>'the minimum eigenvalue of the hessian $\nabla^2 f(x)$ is $m$'</em>.</p>
<p>Together, $(4.1)$ and $(4.2)$ give the analog of $(3.1)$ for twice-differentiable $M$-smooth and $m$-strongly-convex functions:</p>
<p><br>

$$mI \preceq \nabla^2 f(x) \preceq MI \ \ \forall x \tag{3.2}$$

<br></p>
<p>Since the hessian represents the curvature of the function, $(3.2)$ is a two-sided bound on the curvature of $f(x)$. So, we see that smoothness and strong convexity also regulate function shape itself. The lower-bound rules out flatness, while the upper-bound rules out discontinuities like corners and cusps.</p>
<h2 id="Self-Tuning-Property">
<a class="anchor" href="#Self-Tuning-Property" aria-hidden="true"><span class="octicon octicon-link"></span></a>Self-Tuning Property<a class="anchor-link" href="#Self-Tuning-Property"> </a>
</h2>
<p>For a convex function that's $M$-smooth and $m$-strongly-convex we have $(3.1)$ which, as a reminder, is:</p>
<p><br>

$$m||x-y||_2 \leq ||\nabla f(x) - \nabla f(y)||_2 \leq M||x-y||_2 \ \ \forall x,y$$

<br></p>
<p>Fixing iterate $x$, and replacing $y$ with the optimizer $x^*$ we have:</p>
<p><br>

$$m||x-x^*||_2 \leq ||\nabla f(x) - \nabla f(x^*)||_2 \leq M||x-x^*||_2$$

<br></p>
<p>But, since $x^*$ is an optimizer, $\nabla f(x^*) = 0$, so the above becomes:</p>
<p><br>

$$m||x-x^*||_2 \leq ||\nabla f(x)||_2 \leq M||x-x^*||_2$$

<br></p>
<p>The first inequality says that the magnitude of the gradient is <em>at least</em> a constant multiple of the distance from the optimizer. The second inequality says that the magnitude of the gradient is <em>at most</em> a constant multiple of the distance from the optimizer. So, the gradient is large for $x$ far from $x^*$ and gets smaller as $x \rightarrow x^*$. We recognize this as none other than the self-tuning property. So, smoothness and strong-convexity were, indeed, what was needed to encapsulate this property.</p>
<h2 id="Quadratic-Bounds">
<a class="anchor" href="#Quadratic-Bounds" aria-hidden="true"><span class="octicon octicon-link"></span></a>Quadratic Bounds<a class="anchor-link" href="#Quadratic-Bounds"> </a>
</h2>
<p>Smoothness and strong-convexity, should they hold for a given convex function, give a universal quadratic point-wise upper and lower-bound, respectively, for this function. This is what it means to say that the function is <em>like</em> a quadratic. In a sense, all we're saying is that a function is asymptotically bounded by a quadratic from above and below at every point. That is, at any point, it should neither grow slower nor faster than quadratically.</p>
<p>To construct the upper and lower-bounds, we may use the following  two lemmas:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote>
<p><strong>Lemma 1:</strong> If $f$ is convex and $L$-Lipschitz then $g(x) = \frac{L}{2}x^Tx - f(x)$ is convex.</p>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote>
<p><strong>Lemma 2:</strong> If $f$ is $m$-strongly-convex then $g(x) = f(x) - \frac{m}{2}x^Tx$ is convex.</p>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Both lemmas are statements of comparative convexity in disguise. <strong><em>Lemma 1</em></strong> says that $\frac{L}{2}x^Tx$ is more convex than $f$, whereas <strong><em>Lemma 2</em></strong> says that $f$ is more convex than $\frac{m}{2}x^Tx$.</p>
<p>It's not difficult to see how these lemmas can assist in sandwiching $f$ between an upper-bound that's more convex and a lower-bound that's less convex.</p>
<p>The bounds themselves come from the quadratic approximation of $f$:</p>
<p><br>

$$f(y) \approx f(x) + \nabla f(x)^T(y-x) + \frac{1}{2}(y-x)^T\nabla^2 f(x)(y-x)\ \ \forall y$$

<br></p>
<p>By replacing the hessian with its bounds $mI$ and $MI$ and using the above lemmas we obtain:</p>
<p><br>
$$f(y) \geq f(x) + \nabla f(x)^T(y-x) + \frac{m}{2}||y-x||_2^2 \ \ \forall y$$ 

$$\textrm{and} \tag {5.1}$$


$$f(y) \leq f(x) + \nabla f(x)^T(y-x) + \frac{M}{2}||y-x||_2^2 \ \ \forall y$$

<br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="The-Optimal-Fixed-Step-Size">
<a class="anchor" href="#The-Optimal-Fixed-Step-Size" aria-hidden="true"><span class="octicon octicon-link"></span></a>The Optimal Fixed Step-Size<a class="anchor-link" href="#The-Optimal-Fixed-Step-Size"> </a>
</h2>
<p>We already showed that an $M$-smooth and $m$-strongly-convex function enjoys the advantage of self-tuning. But, without knowing how to choose the step-size, we can still cause GD to make slow progress or even diverge.</p>
<p>After all, gradient descent is based on a local linear approximation of the objective. If we take big steps, we are counting on the linear approximation to be a good-enough estimate far from the current iterate. This may be too optimistic, in which case GD will diverge. Being too pessimistic, however, is also not good. While taking small steps won't cause GD to diverge, it will make GD painfully slow.</p>
<p>Luckily, the quadratic upper-bound in $(5.1)$ informs our choice of step-size both in terms of a convergence guarantee and in terms of optimality. First, let's guarantee convergence...</p>
<p>By plugging the update step $x^+ = x - \eta \nabla f(x)$ as $y$ into the upper-bound in $(5.1)$ we obtain:</p>
<p><br>

$$f(x^+) \leq f(x) + \eta(1-\frac{M}{2}\eta)||\nabla f(x)||_2^2 \tag{5.2}$$

<br></p>
<p>As before, it would be wise to insist $f(x^+) \leq f(x)$, which gives the convergence interval as $0 &lt; \eta &lt; \frac{2}{M}$.</p>
<p>It helps considering the simple example of quadratics.</p>
<hr>
<p><strong>Example 1:</strong></p>
<p>Consider the quadratic form in $\mathbb{R}$ given by $f(x) = \frac{1}{2}M x^2$ where $x, M \in \mathbb{R}$. Here we can think of $M$ as the only, and therefore the largest, eigenvalue of the $1 \times 1$ matrix $[M]$.</p>
<p>Its GD update step looks like:</p>
<p><br>

$$x^+ = x - \eta M x = (1 - \eta M)x$$

<br></p>
<p>Which, by recursion from iteration $T$ down to the initial iteration, gives:</p>
<p><br>

$$x^T = (1- \eta M)^Tx_0$$

<br></p>
<p>Then, since $x^* = 0$, convergence is guaranteed by ensuring $|1 - \eta M| &lt; 1$ or, equivalently, $0 &lt; \eta &lt; \frac{2}{M}$ as desired.</p>
<p>This generalizes to higher dimensions as we shall see in the following example.</p>
<p><strong>Example 2:</strong></p>
<p>Consider the quadratic form in $\mathbb{R}^n$ given by $f(x) = \frac{1}{2}x^TQx$ where $x \in \mathbb{R}^n$ and $Q \succeq 0$.</p>
<p>Its GD update step looks like:</p>
<p><br>
$$x^+ = x - \eta Qx = (I - \eta Q)x$$ 
<br></p>
<p>Which, by recursion from iteration $T$ down to the initial iteration, gives:</p>
<p><br>

$$x^T = \underbrace{(I - \eta Q)\ldots(I - \eta Q)}_{\text{$T$ times}}x_0$$

<br></p>
<p>The eigenvalues $\tilde \lambda_i$ of the matrix $I - \eta Q$ are related to the eigenvalues $\lambda_i$ of $Q$ according to:</p>
<p><br>

$$\tilde \lambda_i = 1 - \eta \lambda_i$$

<br></p>
<p>Hence, if $\lambda_{min} = m$ and $\lambda_{max} = M$, then $\tilde \lambda_{min} = 1 - \eta M$ and $\tilde \lambda_{max} =1 - \eta m$.</p>
<p>The eigenvalues of $I - \eta Q$ act on the current iterate by scaling it. So, in order to ensure convergence to $0$, we need $\tilde \lambda_i \in (-1,1) \ \ \forall i$. Or, equivalently:</p>
<p><br>

$$\tilde \lambda_{min} &gt; -1$$


$$\textrm{and}$$


$$\tilde \lambda_{max} &lt; 1$$

<br></p>
<p>Both of these together give us $0 \leq \eta \leq \frac{2}{M}$ as desired.</p>
<hr>
<p>But $0 \leq \eta \leq \frac{2}{M}$ is an interval, not a greedy choice. It's just a condition that guarantees convergence. By making the greedy choice we can find the optimal step-size within this interval.</p>
<p>Since the RHS of $(5.2)$ is strongly convex, the quadratic upper-bound is guaranteed to have a unique minimizer.</p>
<p>The idea is similar to other instances of making a greedy choice we've seen so far. Since the function value is upper-bounded by this quadratic, minimizing this upper-bound gives the best guarantee of smallness for the function value available to us without access to higher order information about the objective. That is:</p>
<p><br>

$$f(y) \leq \min_y f(x) + \nabla f(x)^T(y-x) + \frac{M}{2}||y-x||_2^2$$

<br></p>
<p>Then, the greedy choice for the next iterate is:</p>
<p><br>
$$
x^+ = \arg \min_y f(x) + \nabla f(x)^T(y-x) + \frac{M}{2}||y-x||_2^2
$$
<br></p>
<p>As always, minimizing a quadratic is easy. After going through the motions, we obtain:</p>
<p><br>

$$x^+ = x - \frac{1}{M}\nabla f(x)$$

<br></p>
<p>So, the optimal fixed step-size is $\eta = \frac{1}{M}$.</p>
<p>We will see this idea of minimizing a quadratic approximation of the objective, instead of the objective itself, repeat itself when we get to <strong><em>Newton's Method (NM)</em></strong>. However, NM is a second-oder oracle which has access to $\nabla^2 f(x)$, and hence the quadratic approximation, at all points in the domain of the objective. The remarkable thing about $M$-smoothness and $m$-strong-convexity is that they still give gradient descent, a first-order oracle, access to universal quadratic bounds without the need to know $\nabla^2 f(x)$. These upper-bounds are, as we saw, what inform GD's choice of step-size. This step-size, as we shall soon see, give the GD its convergence guarantees.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Convergence-Rate">
<a class="anchor" href="#Convergence-Rate" aria-hidden="true"><span class="octicon octicon-link"></span></a>Convergence Rate<a class="anchor-link" href="#Convergence-Rate"> </a>
</h2>
<p>As shown above, the theoretical best fixed step-size for an $M$-smooth objective $f(x)$ is $\eta = \frac{1}{M}$. With this choice of step-size, we can derive convergence guarantees for GD as well as its convergence rate.</p>
<h3 id="M-Smooth-Objectives">
<a class="anchor" href="#M-Smooth-Objectives" aria-hidden="true"><span class="octicon octicon-link"></span></a>M-Smooth Objectives<a class="anchor-link" href="#M-Smooth-Objectives"> </a>
</h3>
<p>Fixing the current iterate as $x$, and plugging in $x^+ = x - \frac{1}{M} \nabla f(x)$ into the upper-bound in $(5.1)$, we obtain the quadratic upper-bound on the next iterate in terms of the magnitude of the gradient:</p>
<p><br>

$$f(x^+) \leq f(x) - \frac{1}{2M}||\nabla f(x)||_2^2 \tag{5.3}$$

<br></p>
<p>Furthermore, since the underlying assumption throughout this post is that the objective is convex, we have a linear lower-bound $\forall y$, and particularly at the optimizer $y = x^*$, as:</p>
<p><br>
$$f(x^*) \geq f(x) + \nabla f(x)^T(x^* - x) \tag{5.4}$$ 
<br></p>
<p>By reversing $(5.4)$ and adding it to $(5.3)$ we get:</p>
<p><br>

$$f(x^+) \leq f(x^*) + \nabla f(x)^T(x - x^*) - \frac{1}{2M}||\nabla f(x)||_2^2$$

<br></p>
<p>With a bit of algebraic finessing, we can bring the above to the form:</p>
<p><br>

$$f(x^+) \leq f(x^*) + \frac{M}{2}\left[ ||x-x^*||_2^2 - ||x - \frac{1}{M}\nabla f(x) - x^*||_2^2\right]$$

<br></p>
<p>But $x - \frac{1}{M}\nabla f(x) = x^+$, so we have:</p>
<p><br>

$$f(x^+) - f(x^*) \leq \frac{M}{2}\left[ ||x-x^*||_2^2 - ||x^+ - x^*||_2^2\right]$$

<br></p>
<p>We recognize, $||x^+ - x^*||_2^2$ as the sub-optimality of the next iterate, and $||x - x^*||_2^2$ as the sub-optimality of the previous iterate. When both sides are summed over $T$ iterations, the RHS sum telescopes and we're left with:</p>
<p><br>

$$\sum_{t = 0}^{T-1} (f(x_{t+1}) - f(x_t)) \leq \frac{M}{2}||x_0 - x^*||_2^2$$

<br></p>
<p>The LHS is the sum of sub-optimalities across all $T$ iterations, and the RHS is a quantity that's proportional to the initial condition. Taking the average error across all iterations, we get:</p>
<p><br>

$$\frac{1}{T} \sum_{t = 0}^{T-1} (f(x_{t+1}) - f(x_t)) \leq \frac{M}{2T}||x_0 - x^*||_2^2$$

<br></p>
<p>But, we know that the algorithm with fixed step-size $\eta = \frac{1}{M}$ has the descent property since $0 &lt; \frac{1}{M} &lt; \frac{2}{M}$. So, the last sub-optimality $f(x_{T}) - f(x^*)$ must be the smallest. In particular, it must be smaller than the average. So, we have:</p>
<p><br>

$$f(x_{T}) - f(x^*) \leq \frac{M}{2T}||x_0 - x^*||_2^2 \tag{5.5}$$

<br></p>
<p>So, the error gets better with more iterations and, conversely, gets worse the larger $M$, a measure of how abruptly the gradient changes anywhere on its domain, is.</p>
<p>Note that $M$, as well as $||x_0 - x^*||_2^2$ are fixed in $(5.5)$. So, the convergence rate of GD on an $M$-smooth objective is $\mathcal{O}(\frac{1}{T})$.</p>
<h3 id="M-Smooth-and-m-Strongly-Convex-Objective">
<a class="anchor" href="#M-Smooth-and-m-Strongly-Convex-Objective" aria-hidden="true"><span class="octicon octicon-link"></span></a>M-Smooth and m-Strongly-Convex Objective<a class="anchor-link" href="#M-Smooth-and-m-Strongly-Convex-Objective"> </a>
</h3>
<p>The situation is drastically better if, in addition to $M$-smoothness, we also have $m$-strong-convexity. Not only do we get a much faster convergence rate, we also guarantee convergence in the iterates themselves. Note that, so far, we've discussed sub-optimality in objective values only. That is, the only convergence guarantee we've seen so far is $f(x^T) \rightarrow f(x^*)$ as $T \rightarrow \infty$. But sometimes more is needed, we may want convergence of the iterates themselves. That is, we may want $x^T \rightarrow x^*$ as $T \rightarrow \infty$? Since strong convexity guarantees the existence of a unique optimizer, we can discuss this type of sub-optimality for $m$-strongly-convex objectives.</p>
<p>In this scenario, we have the analog of $(5.3)$ for the quadratic lower-bound.</p>
<p>Just as $x - \frac{1}{M} \nabla f(x)$ minimized the quadratic upper-bound, $x - \frac{1}{m}$ minimizes the quadratic lower-bound. Plugging it into the lower-bound, we get $(5.3)$'s analog as:</p>
<p><br>

$$f(y) \geq f(x) - \frac{1}{2m}||\nabla f(x)||_2^2 \ \ \forall y \tag{5.6}$$

<br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p></p>
<div class="flash">
    <svg class="octicon octicon-info" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z"></path></svg>
    <strong>Note: </strong>Note, this result is stronger than its analog (i.e. it holds ’$\forall y$’) as opposed to $(5.3)$, which is only guaranteed to hold at its minimizer $x^+ = x - \frac{1}{M}\nabla f(x)$, because it’s a minimization of a lower-bound as opposed to a minimization of an upper-bound. 
</div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Since $(5.6)$ holds $\forall y$, it holds, in particular, at the optimizer $y=x^*$:</p>
<p><br>

$$f(x^*) \geq f(x) - \frac{1}{2m}||\nabla f(x)||_2^2 \tag{5.7}$$

<br></p>
<p>We can now solve for $||\nabla f(x)||_2^2$ in $(5.7)$ and plug the result into $(5.3)$. From $(5.7)$, we get:</p>
<p><br>

$$||\nabla f(x)||_2^2 \geq 2m(f(x)-f(x^*))$$

<br></p>
<p>Which, when plugged into $(5.3)$, gives:</p>
<p><br>

$$f(x^+) - f(x^*) \leq \left(1-\frac{m}{M}\right)(f(x)-f(x^*))$$

<br></p>
<p>The LHS is the sub-optimality in the next iteration, whereas the RHS is that in the current iteration.  Recursion from iteration $T$ down to the initial iteration, gives:</p>
<p><br>

$$f(x_T) - f(x^*) \leq  \left(1-\frac{m}{M}\right)^T (f(x_0) - f(x^*)) \tag{5.8}$$

<br></p>
<p>And, since $m \leq M$ and both strictly positive, $0 &lt; \frac{m}{M} \leq 1$ which guarantees convergence.</p>
<p>Note that $m$, $M$, and the initial sub-optimality $f(x_0) - f(x^*)$ are fixed quantities in $(5.8)$. So, the convergence rate of GD on a smooth and strongly-convex objective is $\mathcal{O}(c^{-T})$ for some constant $c &gt; 0$. That is, the error decreases exponentially in the number of iterations. However, historically, mathematicians were concerned with the logarithm of the error, rather than the error itself, and hence this type of convergence is known as <strong><em>linear convergence</em></strong>.</p>
<p>Also note that $(5.8)$ predicts the performance of GD on objectives with more-or-less circular vs elongated level sets. These are called <strong><em>badly</em></strong> and <strong><em>well conditioned</em></strong> objectives respectively. To say that an objective is $M$-smooth and $m$-strongly convex is to say $(3.2)$ which implies that all the eigenvalues of the hessian are bounded between $m$ and $M$. The eigenvalues of the hessian represent the stretch of the level sets in the principal directions. So, to say $\frac{m}{M} \approx 1$ is to say $m \approx M$ which means the level sets are not more-or-less stretched in any particular directions which, in turn, forces the level sets to be roughly circular. As we can see from $(5.8)$, GD converges quite fast in such cases since $\left( 1 - \frac{m}{M} \right)$ is small. The opposite is true in the case when the level sets are elongated.</p>
<p>As promised, we also have convergence of the iterates themselves. From the quadratic lower-bound in $(5.1)$ we can derive an upper-bound on $||x - x^*||_2$ as follows. As before, letting $y = x^*$ in the quadratic lower-bound gives us:</p>
<p><br>

$$f(x^*) \geq f(x) + \nabla f(x)^T(x^* - x) + \frac{m}{2}||x^* - x||_2^2$$

<br></p>
<p>But, by the <a href="https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality"><strong><em>Cauchy-Schwarz Inequality</em></strong></a>, we further have:</p>
<p><br>

$$f(x^*) \geq f(x) - ||\nabla f(x)||_2||(x^* - x)||_2 + \frac{m}{2}||x^* - x||_2^2$$

<br></p>
<p>But, since $f(x^*) \leq f(x)$ by the optimality of $x^*$, we must have:</p>
<p><br>

$$- ||\nabla f(x)||_2||(x^* - x)||_2 + \frac{m}{2}||x^* - x||_2^2 \leq 0$$

<br></p>
<p>From which it follows that:</p>
<p><br>

$$||x - x^*||_2 \leq \frac{2}{m}||\nabla f(x)||_2 \tag{5.9}$$

<br></p>
<p>As GD converges to $f(x^*)$, $\nabla f(x) \rightarrow \nabla f(x^*) = 0$ where the last equality is by optimality of $x^*$. So, $x \rightarrow x^*$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Variable-Step-Size-GD">
<a class="anchor" href="#Variable-Step-Size-GD" aria-hidden="true"><span class="octicon octicon-link"></span></a>Variable Step-Size GD<a class="anchor-link" href="#Variable-Step-Size-GD"> </a>
</h1>
<p>While smoothness gives the theoretical best step size, for most problems it's difficult to come up with the smoothness parameter $M$. So, while informative, the discussion we've had so far has been largely theoretical. In practice, there are subroutines such as <strong><em>Exact Line Search (ELS)</em></strong> or <strong><em>Backtracking Line Search (BTLS)</em></strong> which choose an appropriate step-size $\eta_t$ at each iteration. These subroutines can also be modified and used with other optimization algorithms. For example, BTLS is the state of the art way of choosing a step-size in Newton's and <strong><em>Quasi-Newton's Methods</em></strong>.</p>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="v-poghosyan/blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-dark"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/blog/optimization/algorithms/applied%20mathematics/proofs/2022/03/03/Convex-Optimization-Algorithms-for-Unconstrained-Optimization-Gradient-Descent.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Personal blog about Machine Learning, Data Science, Mathematics, Software Engineering, and various other topics. Created by Vahram Poghosyan, © 2021 - Present.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/v-poghosyan" target="_blank" title="v-poghosyan"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/Vahram31680593" target="_blank" title="Vahram31680593"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
